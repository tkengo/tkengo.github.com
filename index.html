<!DOCTYPE html>
<html>
  <head>
	<meta name="generator" content="Hugo 0.26" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width">
  <meta name="author" content="@tkengo">
  <meta http-equiv="X-UA-Compatible" CONTENT="IE=EmulateIE7" />
  
    <title>けんごのお屋敷</title>
  
  <link rel="stylesheet" href="http://tkengo.github.io/css/common.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="shortcut icon" href="http://tkengo.github.io/images/favicon.ico">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.7/styles/atelier-cave.light.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.7/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  
  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
  <style type="text/css">
    @font-face {
      font-family: mplus-1p-regular;
      src: url('http://mplus-fonts.sourceforge.jp/webfonts/mplus-1p-regular.ttf') format("truetype");
      font-family: mplus-1p-bold;
      src: url('http://mplus-fonts.sourceforge.jp/webfonts/mplus-1p-bold.ttf') format("truetype");
    }
  </style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]] },
      TeX: { extensions: ["color.js"] }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

  <body>
    <div class="container-bg">
      <div class="header">
  <header>
    <h1 class="logo"><a href="http://tkengo.github.io/">けんごのお屋敷</a></h1>
    <ul class="menu">
      <li><a href="/post">Archives</a></li>
    </ul>
  </header>
</div>

      
      <div class="posts">
        
        <div class="container">
  <main>
    <header>
      <p class="day">2017-09-06</p>
      <h1><a href="http://tkengo.github.io/blog/2017/09/06/ml-math/">「やさしく学ぶ 機械学習を理解するための数学のきほん」を執筆しました</a></h1>
    </header>
    <article>
      

<p>私が執筆した書籍 <a href="https://www.amazon.co.jp/dp/4839963525/" target="_blank">やさしく学ぶ 機械学習を理解するための数学のきほん</a> がマイナビ出版から Amazon で 2017/9/20 より発売されます。Amazon 上では既に予約可能になっていますので、興味のある方は是非とも手に取ってみてください。</p>

<div style="width:100%;">
<a href="https://www.amazon.co.jp/dp/4839963525/" target="_blank">
<img src="/assets/img/ml-math/cover-math.png" style="border-width:0;width:50%;float:right;">
</a>
</div>

<p>本書は、以前よりこのブログ内で公開していた「やる夫で学ぶ機械学習シリーズ」というシリーズ物の記事をベースとして、加筆・修正を加えたものになります。ブログの記事がベースになってはいますが、追加で書いた分の方が多く、お金を出して買ってもらえるクオリティにするために、より丁寧な説明を心がけて書きました。元記事は「やる夫」と「やらない夫」というキャラクターを登場人物として、機械学習の基礎を面白おかしく丁寧に解説していくものでしたが、書籍化するに当たって「やる夫」と「やらない夫」をそのまま使うわけにもいかなかったので、プログラマの「アヤノ」とその友達で機械学習に詳しい「ミオ」というキャラクターを新しく作って、彼女らの会話を通して一緒に勉強していく形を取っています。この本は以下のような構成になっていて、Chapter 1 〜 5 が本編で、Appendix で必要な数学知識の補足をしています。</p>

<ul>
<li>Chapter 1 - ふたりの旅の始まり</li>
<li>Chapter 2 - 回帰について学ぼう</li>
<li>Chapter 3 - 分類について学ぼう</li>
<li>Chapter 4 - 評価してみよう</li>
<li>Chapter 5 - 実装してみよう</li>
<li>Appendix (本編に入り切らなかった数学知識の解説)</li>
</ul>

<p>会話形式なので、全体的な文量もそこまで多くなくスラスラ読んでいけると思います。ただし、本書は機械学習の数学に関する入門書という位置付けなので、プログラミング自体の入門解説だったり、いろいろなアルゴリズムの紹介だったりは、内容にありません。Chapter 2 及び Chapter 3 あたりが、いま公開されているブログ記事ベースになっていますので、いきなり本を買うのが嫌な方や試し読みをしてみたい方などは、まず <a href="/blog/2016/06/06/yaruo-machine-learning0/">やる夫で学ぶ機械学習シリーズ</a> を読んでみると、感覚をつかんでいただけるのではないかなと思います。また、マイナビ出版の Facebook には <a href="https://www.facebook.com/pg/Mynavibooks.Creative/photos/?tab=album&amp;album_id=1755672201120222">本書のサンプルページ</a> が掲載されていますので、そちらを見ても雰囲気はつかんでいただけるかなと思います。</p>

<h1 id="執筆にかけた想い">執筆にかけた想い</h1>

<p>書籍の宣伝はこの辺にして、ここからは個人的な想いを書いたポエムですので、ご了承ください。</p>

<hr />

<p>- <em>数学や機械学習を通じて、気軽に話したりお互いに刺激し合える仲間が欲しい</em> -</p>

<p>始まりは、ただ、それだけでした。</p>

<p>私がいわゆる機械学習と言われる領域に足を踏み入れた頃は、いまほど界隈は賑わっていませんでした。元々は大学時代からコンピュータービジョンや 3D に興味があり、それらに関するアルゴリズムや実装を探しているうちに機械学習に興味をもって勉強し始めましたが、もちろん当時は参考にできる Web サイトも書籍も、今よりも少ない状態です。仕事として機械学習をやっているわけでもなく、手取り足取り教えてくれるような詳しい人や、機械学習に興味を持っていそうな人は周りにはおらず、毎日ひとりでアテもなく先の見えない道を進むばかりだったのを覚えています。一人きりで何かを続けていくことほど、モチベーションが長続きしないことはありません。</p>

<p>そんな状況でしたのでちょっとだけ勉強には苦労していたのですが、学生時代から数学が好きだったこともあり (得意かどうかは置いといて) 数学に対する抵抗はまったくありませんでした。むしろ数学をやっている時間は一人でも楽しくて、没頭することができました。そうして勉強しながらしばらく時が経ち、世の中が機械学習やディープラーニングで盛り上がりを見せ始めたのをきっかけに、自分のブログでもぽつぽつと機械学習に関する情報を発信するようになっていきます。また、数学好きが高じて <a href="https://maths4pg-fuk.connpass.com/">プログラマのための数学勉強会@福岡</a> というイベントも主催していました。そうやってアウトプットを続けていれば、きっと仲間が集って、いつかみんなでワイワイできるようになるんだ、と信じて。</p>

<p>そういったアウトプットの一環として <a href="/blog/2016/06/06/yaruo-machine-learning0/">やる夫で学ぶ機械学習シリーズ</a> は産まれました。このシリーズは「わかるって楽しい」をコンセプトに、なるべく読者を置いてけぼりにしないように気をつけながら書いていました。シリーズ目次には「まとめることで知識を体系化できて自分の為になる」などと書いてますが、先にも仲間集めをしたいと言っていたように、本当は <strong>もっと身近に機械学習に興味がある人や仲間を増やしたい</strong> という理由がありました。自分なんかが書いた記事で機械学習に興味を持ってくれる人が増えると考えるなんておこがましい、と割りとネガティブに考えていて、無理やり別の理由をくっつけていたのですが、幸運なことに公開後に非常に大きな反響をいただきました。そのおかげで「あのブログ読みました」と声をかけていただくことも多くなりましたし、結果的にこのシリーズをベースに書籍の執筆の機会をいただくことが出来ました。このシリーズ記事を通じて、機械学習の基礎を理解して興味を持ってくれた人がいたとすると非常に嬉しいですし、そういう人が増えてくれると記事を書いた甲斐があったというものです。</p>

<p>そして、私のその想いは書籍になっても変わりません。いまは時代も移り変わり、初心者向けの解説書や Web の記事はたくさんあり、導入の敷居は確実に当時よりも下がっています。そんな中、あえて本書を選んで手にとってくれた方には是非とも、一緒に話したり勉強したりできる仲間を作って、そして私のようにひとりぼっちで悩むことがないように情報を発信し続けてくれればいいな、と思っています。</p>

<p>願わくば一人でも多くの機械学習へ興味を持つ方へ本書が届き、そして役に立ってくれますように。</p>

    </article>
    <footer class="tag">
      <i class="fa fa-tag"></i>
      
        <a class="tag-link" href="tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a>
      
    </footer>
    <ul class="snsb">
      <li>
        <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://tkengo.github.io/blog/2017/09/06/ml-math/" data-lang="ja">ツイート</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li>
        <div class="fb-like" data-href="http://tkengo.github.io/blog/2017/09/06/ml-math/" data-layout="button_count" data-action="like" data-show-faces="false" data-share="false"></div>
      </li>
      <li>
        <a href="http://b.hatena.ne.jp/entry/http://tkengo.github.io/blog/2017/09/06/ml-math/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
      </li>
    </ul>
  </main>
</div>

        
        <div class="container">
  <main>
    <header>
      <p class="day">2016-08-22</p>
      <h1><a href="http://tkengo.github.io/blog/2016/08/22/maximum-likelyhood-estimation-by-machine-learning/">機械学習でパラメータ推定 - 最尤推定法 -</a></h1>
    </header>
    <article>
      

<p>最尤推定法 (<em>Maximum Likelyhood や Maximum Likelyhood Estimation と言われ、それぞれ頭文字を取って ML や MLE などとも言われる</em>) は機械学習やコンピュータービジョンなどの分野で良く使われる推定法で、次のような条件付き同時確率を最大化することでパラメータの推定を行います。</p>

<p>$$
\hat{\theta} = \mathop{\mathrm{argmax}}\limits_{\theta} \mathrm{P}(x_1, x_2, \cdots, x_N|\theta)
$$</p>

<p>これだけ見て「うん、アレね」と理解できる人はこの記事の対象読者ではなさそうですので、逆にいろいろ教えて下さい。この記事では理論の面から最尤推定法にアタックしてみます。数式成分が多めで、うっとなることもあるかもしれませんが、ゆっくり読んでいきましょう。</p>

<p><em>※この記事を読むにあたっては、確率論と微分の基礎知識程度は必要です。</em></p>

<h1 id="尤度">尤度</h1>

<p>いきなり応用問題から始めると必ず挫折するので、まずは一番簡単な問題設定から始めます。Wikipedia に <a href="https://ja.wikipedia.org/wiki/%E6%9C%80%E5%B0%A4%E6%8E%A8%E5%AE%9A">最尤推定</a> のページがありますので、この中で使われている例を参考に話を進めていきましょう。</p>

<hr />

<p><em>見た目がまったく同じ 3 枚のコイン A, B, C があります。これらのコインはイカサマで、投げた時に表の出る確率がそれぞれ違います。</em></p>

<ul>
<li><em>コイン A は <sup>1</sup>&frasl;<sub>3</sub> の確率で表が出る</em></li>
<li><em>コイン B は <sup>1</sup>&frasl;<sub>2</sub> の確率で表が出る</em></li>
<li><em>コイン C は <sup>2</sup>&frasl;<sub>3</sub> の確率で表が出る</em></li>
</ul>

<p><em>この 3 枚のコインを箱の中に入れてシャッフルした後に 1 枚引きます。引いたコインを 10 回投げたら、表が 3 回、裏が 7 回出ました。あなたは A, B, C のどのコインを引いたでしょうか？</em></p>

<hr />

<p>この問題設定は極めて単純です。単純すぎて最尤推定法を使わなくても解けるくらい簡単ですが、ここでは敢えて最尤推定法を使って解いてみます。最尤推定法は次の条件付き同時確率を最大化するパラメータ $\hat{\theta}$ を求めることでした。</p>

<p>$$
\hat{\theta} = \mathop{\mathrm{argmax}}\limits_{\theta} \mathrm{P}(x_1, x_2, \cdots, x_N|\ \theta)
$$</p>

<p>$x_1$ は 1 回目に投げた時の結果、$x_2$ は 2 回目に投げた時の結果、$x_N$ は N 回目に投げた時の結果になります。コインは全部で 10 回投げることになっているので $N=10$ なのは自明ですよね？これらは、統計学や機械学習の文脈では観測データや学習データとも言われますが、先の問題設定では表が 3 回、裏が 7 回出たということなので、例えばですがこんな風に表せます。<em>(表、裏、みたいな単語だと扱いにくいので、今後は表 = 1、裏 = 0 という風に数値に変換して扱います)</em></p>

<ul>
<li>$x_1 = 0$ <em>(裏)</em></li>
<li>$x_2 = 1$ <em><span style="color:red;">(表)</span></em></li>
<li>$x_3 = 0$ <em>(裏)</em></li>
<li>$x_4 = 0$ <em>(裏)</em></li>
<li>$x_5 = 0$ <em>(裏)</em></li>
<li>$x_6 = 0$ <em>(裏)</em></li>
<li>$x_7 = 1$ <em><span style="color:red;">(表)</span></em></li>
<li>$x_8 = 1$ <em><span style="color:red;">(表)</span></em></li>
<li>$x_9 = 0$ <em>(裏)</em></li>
<li>$x_{10} = 0$ <em>(裏)</em></li>
</ul>

<p>ここでちょっとコイントスについて考えます。問題ではコインを 10 回投げていますが、それぞれのコイントスは独立だと考えます。要するに 1 回目に表か裏かどちらが出ようが、2 回目のトスには何の影響も与えないと考えます <em>(3 回目以降も同様)</em> 。まあ普通そうですよね。各試行が独立だと考えると、同時確率は各試行の確率の積に分解できるので、こう書き換えることができます。</p>

<p>$$
\mathrm{P}(x_1, x_2, \cdots, x_{10}|\ \theta) = \prod_i^{10} \mathrm{P}(x_i|\ \theta)
$$</p>

<p>じゃあ $\mathrm{P}$ と $\theta$ は何かというと、確率分布とそのパラメータです。確率分布はそれぞれ固有のパラメータを持っていて、そのパラメータが決まれば分布の形が決まります。たとえば&hellip;</p>

<ul>
<li><strong>正規分布:</strong> 平均 $\mu$ と分散 $\sigma^2$ という 2 つのパラメータを持つ</li>
<li><strong>t分布:</strong> 自由度 $\nu$ というパラメータを持つ</li>
<li><strong>ベルヌーイ分布:</strong> $\lambda$ というパラメータを持つ</li>
</ul>

<p>式中ではパラメータのことを $\theta$ と書いてはいますが、これを見てわかるようにパラメータは確率分布によって全然違うのでそれを何か 1 つの文字にまとめているというだけで、確率分布によって様々に変わります。</p>

<p>ここでは箱から引いたコインを投げた時にどれくらいの確率で表が出るのか、ということを知りたいので、確率 $\lambda$ で 1 が出て、確率 $1 - \lambda$ で 0 が出る、というベルヌーイ分布を考えます。たとえば $\lambda = 0.4$ だとすると、40% の確率で 1 が、60% の確率で 0 が出ることになります。ベルヌーイ分布はこのように数式で表すことができます。</p>

<p>$$
\mathrm{P}(x\ |\ \lambda) = \lambda^x (1-\lambda)^{1-x} \ \ \ \ (0 \le \lambda \le 1,\ \ x \in \{0,1\})
$$</p>

<p>これを最尤推定法の式に代入すると最終的にはこう書くことができます。</p>

<p>$$
\begin{eqnarray}
\mathrm{P}(x_1, x_2, \cdots, x_{10}|\ \theta) &amp; = &amp; \prod_i^{10} \mathrm{P}(x_i|\ \theta) \<br />
                                                  &amp; = &amp; \prod_i^{10} \lambda^{x_i} (1-\lambda)^{1-x_i}
\end{eqnarray}
$$</p>

<p>この式がどういうことを意味しているのかをちょっと考えてみます。この式は <strong>尤度</strong> と呼ばれており、パラメータ $\lambda$ が与えられた時に、その $\lambda$ がどれくらい学習データを尤もらしく説明できているかという指標になります。尤度という概念はとてもわかりづらく、言葉だけでは少し理解しにくいので、実際に値を計算してみましょう。たとえば $\lambda$ を適当に 3 つ決めて、それぞれの $\lambda$ で式の値を求めてみます。</p>

<ul>
<li>$\lambda = 0.05$ (5% の確率で表が出るコイン) の場合</li>
</ul>

<p>$$
\begin{eqnarray}
\prod_i^{10} 0.05^{x_i} (1-0.05)^{1-x_i} &amp; = &amp; 0.95 \cdot 0.05 \cdot 0.95 \cdot 0.95 \cdot 0.95 \cdot 0.95 \cdot 0.05 \cdot 0.05 \cdot 0.95 \cdot 0.95 \<br />
                                            &amp; = &amp; 0.00008729216
\end{eqnarray}
$$</p>

<ul>
<li>$\lambda = 0.33$ (33% の確率で表が出るコイン) の場合</li>
</ul>

<p>$$
\begin{eqnarray}
\prod_i^{10} 0.33^{x_i} (1-0.33)^{1-x_i} &amp; = &amp; 0.67 \cdot 0.33 \cdot 0.67 \cdot 0.67 \cdot 0.67 \cdot 0.67 \cdot 0.33 \cdot 0.33 \cdot 0.67 \cdot 0.67 \<br />
                                            &amp; = &amp; 0.00217803792
\end{eqnarray}
$$</p>

<ul>
<li>$\lambda = 0.91$ (91% の確率で表が出るコイン) の場合</li>
</ul>

<p>$$
\begin{eqnarray}
\prod_i^{10} 0.91^{x_i} (1-0.91)^{1-x_i} &amp; = &amp; 0.09 \cdot 0.91 \cdot 0.09 \cdot 0.09 \cdot 0.09 \cdot 0.09 \cdot 0.91 \cdot 0.91 \cdot 0.09 \cdot 0.09 \<br />
                                            &amp; = &amp; 0.00000003604
\end{eqnarray}
$$</p>

<p>これは、最初に学習データとして得られた $x_1, x_2, \cdots, x_{10}$ が <strong>どういうコインから生成されたものか</strong> という指標をしめしています。10 回中 3 回だけ表が出るという結果が、$\lambda = 0.91$ のコインから生成されることは稀でしょう。$\lambda = 0.91$ ということは表が出る確率が 91% もあるので、3 回とは言わずもっと表が出るはずです。逆に $\lambda = 0.05$ の場合は、表が出る確率は 5% ということなので、10 回中 3 回も表が出ているという結果はあまり納得のいく結果ではありません。結局は $\lambda = 0.33$ というコインが、一番うまく結果を説明しています。</p>

<p>これは、尤度の値を見てみれば一目瞭然です。3 つの尤度を比べてみると $\lambda = 0.33$ の尤度が最も大きく、続いて $\lambda = 0.05$、そして $\lambda = 0.91$ が最も小さくなっています。要するに尤度の値が大きければ大きいほど、うまくデータを説明してくれる尤もらしいパラメータであると言えます。</p>

<p>この辺りまで理解できるとゴールが見えてきます。最初に示した最尤推定法の式を覚えていますか？これは今考えているコインの問題に対しては、最終的にこのように変形できます。</p>

<p>$$
\begin{eqnarray}
\hat{\theta} &amp; = &amp; \mathop{\mathrm{argmax}}\limits_{\theta} \mathrm{P}(x_1, x_2, \cdots, x_{10}|\ \theta) \<br />
             &amp; = &amp; \mathop{\mathrm{argmax}}\limits_{\theta} \prod_i^{10} \mathrm{P}(x_i|\ \theta) \<br />
             &amp; = &amp; \mathop{\mathrm{argmax}}\limits_{\lambda} \prod_i^{10} \lambda^{x_i} (1-\lambda)^{1-x_i}
\end{eqnarray}
$$</p>

<p>これは尤度を最大にする $\lambda$ を求める、という意味の式です。尤度が最大になる $\lambda$ がわかれば、観測された学習データ $x_1, x_2, \cdots, x_{10}$ がどういうコインを使って生成されたものなのか、言い換えればどういうコインを投げると $x_1, x_2, \cdots, x_{10}$ という結果が出てくるのか、ということがわかります。</p>

<p>尤度を最大化するパラメータを推定する、これが最尤推定です。</p>

<h1 id="最適化問題">最適化問題</h1>

<p>尤度を最大化するという最適化問題なので、あとは最適化問題を解くためのアルゴリズムに従って解いていくだけです。ここでは最大化したい尤度を $L$ として、このように表します。</p>

<p>$$
L(\lambda) = \prod_i^{10} \lambda^{x_i} (1-\lambda)^{1-x_i}
$$</p>

<p>ただし、一般的にはこの $L$ をそのまま最大化するのではなく尤度の対数を取った対数尤度 $\log L$ を最大化します。</p>

<p>$$
\begin{eqnarray}
\log L(\lambda) &amp; = &amp; \log \prod_i^{10} \lambda^{x_i} (1-\lambda)^{1-x_i} \<br />
                &amp; = &amp; \sum_i^{10} \log \lambda^{x_i} (1-\lambda)^{1-x_i} \<br />
                &amp; = &amp; \sum_i^{10} \left( x_i \log \lambda + (1 - x_i) \log (1 - \lambda) \right)
\end{eqnarray}
$$</p>

<p>対数を取る理由は主には、計算を簡単にするためとアンダーフローを防ぐためです。今計算しようとしている尤度は要するに同時確率なので確率の積になっていますが、対数を取ると積を和に変換できます。また、さっき具体的にパラメータを代入して尤度を計算してみてわかったと思いますが、同時確率の計算は値が急激に小さくなっていきます。これも対数を取ることで積が和に変換されるので、値が小さくなることを防げます。</p>

<p>さて、この $\log L(\lambda)$ が最大になるような $\lambda$ を探すことが目的でした。関数の最大値を探す時は微分した結果を = 0 と置いて、微分した変数について解けばよいですよね <em>(関数が単純な凸関数である場合に限られますが)</em> 。</p>

<p>$$
\begin{eqnarray}
\frac{d \log L(\lambda)}{d \lambda} &amp; = &amp; \sum_i^{10} ( \frac{x_i}{\lambda} - \frac{1 - x_i}{1 - \lambda}) \<br />
                                    &amp; = &amp; \frac{1}{\lambda(1 - \lambda)} \sum_i^{10} (x_i - \lambda)
\end{eqnarray}
$$</p>

<p>微分結果が求まったので、これを = 0 と置いて $\lambda$ について解きます。</p>

<p>$$
\begin{eqnarray}
\frac{1}{\lambda(1 - \lambda)} \sum_i^{10} (x_i - \lambda) &amp; = &amp; 0 \<br />
\sum_i^{10} x_i - 10 \lambda &amp; = &amp; 0 \<br />
\lambda &amp; = &amp; \frac{1}{10} \sum_i^{10} x_i
\end{eqnarray}
$$</p>

<p>これで答えがでました。このような式で求めた $\lambda$ が尤度を最大にします。実際に $x_1, x_2, \cdots, x_{10}$ を代入して計算してみると</p>

<p>$$
\begin{eqnarray}
\lambda &amp; = &amp; \frac{1}{10} \sum_i^{10} x_i \<br />
        &amp; = &amp; \frac{1}{10} (0 + 1 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0) \<br />
        &amp; = &amp; \frac{3}{10}
\end{eqnarray}
$$</p>

<p>これは <sup>3</sup>&frasl;<sub>10</sub> の確率で表が出るコインであれば尤度が最大になるということで、これに一番近いコインは A になり (A は <sup>1</sup>&frasl;<sub>3</sub> の確率で表が出るコイン)、観測された学習データから考えるとあなたが引いたコインは A だろうということが言えます。</p>

<p>気付いた人もいるかもしれませんが、10 回中 3 回表が出るんだったら、そりゃあ <sup>3</sup>&frasl;<sub>10</sub> になるのは直感的に当然だろうって思いますよね。これは問題設定が単純ってこともあるかもしれませんが、$x_1, x_2, \cdots, x_{10}$ というデータだけから判断した結果なのでそうなるのも分かる気がしますね。</p>

<h2 id="正規分布のパラメータ推定">正規分布のパラメータ推定</h2>

<p>最後に正規分布についても例を見て終わりたいと思います。</p>

<p>ベルヌーイ分布のパラメータは $\lambda$ ひとつだけでしたが、正規分布は平均 $\mu$ と分散 $\sigma^2$ の 2 つのパラメータを持っていますので、尤度を最大化するためにそれぞれ $\mu$ と $\sigma^2$ をそれぞれ求めます。今回は確率分布 $\mathrm{P}$ に正規分布を使うので尤度はこうなります。</p>

<p>$$
\begin{eqnarray}
L &amp; = &amp; \mathrm{P}(x_1, x_2, \cdots, x_N|\ \theta) \<br />
  &amp; = &amp; \prod_i^N \mathrm{P}(x_i|\ \theta) \<br />
  &amp; = &amp; \prod_i^N \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2} \right)
\end{eqnarray}
$$</p>

<p>同じようにこの尤度を $L$ として対数を取ると、このように変形できます。</p>

<p>$$
\begin{eqnarray}
\log L &amp; = &amp; \log \prod_i^N \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2} \right) \<br />
       &amp; = &amp; \sum_i^N \log \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2} \right) \<br />
       &amp; = &amp; \sum_i^N \left( -\log \sqrt{2 \pi} -\frac{1}{2}\log \sigma^2 - \frac{(x_i - \mu)^2}{2\sigma^2} \right)
\end{eqnarray}
$$</p>

<p>ベルヌーイ分布を考えた時と同様に、この対数尤度関数を $\mu$ と $\sigma^2$ について微分して、それぞれ = 0 と置いて解いていきます。正規分布ではパラメータが 2 つある多変数関数なので偏微分になります。</p>

<p>まずは $\mu$ で偏微分をするところから。</p>

<p>$$
\begin{eqnarray}
\frac{\partial \log L}{\partial \mu} &amp; = &amp; \sum_i^N \left(0 + 0 + \frac{(x_i - \mu)}{\sigma^2} \right) \<br />
                                     &amp; = &amp; \frac{1}{\sigma^2} \sum_i^N (x_i - \mu)
\end{eqnarray}
$$</p>

<p>これを = 0 と置いて $\mu$ について解いていきます。</p>

<p>$$
\begin{eqnarray}
\frac{1}{\sigma^2} \sum_i^N (x_i - \mu) &amp; = &amp; 0 \<br />
\sum_i^N x_i - N\mu &amp; = &amp; 0 \<br />
\mu &amp; = &amp; \frac{1}{N} \sum_i^N x_i
\end{eqnarray}
$$</p>

<p>これでまずは尤度を最大にする $\mu$ が見つかりました。次は $\sigma^2$ について解いていきます。まずは偏微分から。</p>

<p>$$
\begin{eqnarray}
\frac{\partial \log L}{\partial \sigma^2} &amp; = &amp; \sum_i^N \left(0 - \frac{1}{2\sigma^2} + \frac{(x_i - \mu)^2}{2\sigma^4} \right) \<br />
                                          &amp; = &amp; -\frac{N}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_i^{N}(x_i-\mu)^2
\end{eqnarray}
$$</p>

<p>同じようにこれを = 0 と置いて $\sigma^2$ について解きます。</p>

<p>$$
\begin{eqnarray}
-\frac{N}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_i^{N}(x_i-\mu)^2 &amp; = &amp; 0 \<br />
-N\sigma^2 + \sum_i^{N}(x_i-\mu)^2 &amp; = &amp; 0 \<br />
\sigma^2 &amp; = &amp; \frac{1}{N}\sum_i^{N}(x_i-\mu)^2
\end{eqnarray}
$$</p>

<p>これで尤度を最大にする正規分布のパラメータ $\mu$ と $\sigma^2$ がわかりました。</p>

<p>$$
\begin{eqnarray}
\mu &amp; = &amp; \frac{1}{N} \sum_i^N x_i \<br />
\sigma^2 &amp; = &amp; \frac{1}{N}\sum_i^{N}(x_i-\mu)^2
\end{eqnarray}
$$</p>

<p>これは純粋なデータの平均と分散になっています。</p>

<h1 id="まとめ">まとめ</h1>

<p>尤度を最大化してパラメータを推定するための最尤推定法を紹介してきました。学習データからモデルのパラメータ $\theta$ を訓練することが目的ではなく、最終的にやりたいことは新しい未知データがどれだけモデルに適合しているのか推論することです。最尤推定は比較的簡単な手法だと思うので、試しに実装してみると面白いと思います。</p>

    </article>
    <footer class="tag">
      <i class="fa fa-tag"></i>
      
        <a class="tag-link" href="tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a>
      
        <a class="tag-link" href="tags/%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E6%8E%A8%E5%AE%9A">パラメータ推定</a>
      
    </footer>
    <ul class="snsb">
      <li>
        <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://tkengo.github.io/blog/2016/08/22/maximum-likelyhood-estimation-by-machine-learning/" data-lang="ja">ツイート</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li>
        <div class="fb-like" data-href="http://tkengo.github.io/blog/2016/08/22/maximum-likelyhood-estimation-by-machine-learning/" data-layout="button_count" data-action="like" data-show-faces="false" data-share="false"></div>
      </li>
      <li>
        <a href="http://b.hatena.ne.jp/entry/http://tkengo.github.io/blog/2016/08/22/maximum-likelyhood-estimation-by-machine-learning/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
      </li>
    </ul>
  </main>
</div>

        
        <div class="container">
  <main>
    <header>
      <p class="day">2016-06-16</p>
      <h1><a href="http://tkengo.github.io/blog/2016/06/16/yaruo-machine-learning6/">やる夫で学ぶ機械学習 - 対数尤度関数 -</a></h1>
    </header>
    <article>
      

<p>やる夫で学ぶ機械学習シリーズの第 6 回です。ロジスティック回帰の目的関数である尤度関数をもう少し詳しくみて、線形分離不可能な問題にどのように適用していくのかを見ていきます。</p>

<p>第 5 回はこちら。<a href="/blog/2016/06/04/yaruo-machine-learning5/">やる夫で学ぶ機械学習 - ロジスティック回帰 -</a></p>

<p>目次はこちら。<a href="/blog/2016/06/06/yaruo-machine-learning0/">やる夫で学ぶ機械学習シリーズ</a></p>

<h1 id="対数尤度関数">対数尤度関数</h1>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
尤度関数を微分してパラメータ $\boldsymbol{\theta}$ の更新式を求めてみようか。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
もう脳みそパンパンですお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
その前に、尤度関数はそのままだと扱いにくいから、少し変形しよう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
扱いにくい？どういうことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
まず同時確率という点だ。確率なので 1 以下の数の掛け算の連続になることはわかるな？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
確かに、確率の値としては 0 より大きくて 1 より小さいものだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
1 より小さい数を何度も掛け算すると、どんどん値が小さくなっていくだろう。コンピュータで計算する場合はそれは致命的な問題だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あー、オーバーフローの逆かお。アンダーフロー。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
次に掛け算という点だ。掛け算は足し算に比べて計算が面倒だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
小数点の計算とかあんまりやりたくないお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そこで一般的には尤度関数の対数をとったもの、対数尤度関数を使う。</p>

<p>$$
\log L(\boldsymbol{\theta}) = \log \prod_{i=1}^n P(y^{(i)}=1|\boldsymbol{x})^{y^{(i)}} P(y^{(i)}=0|\boldsymbol{x})^{1-y^{(i)}}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
目的関数に対して勝手に対数をとったりして、答え変わらないのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
問題ない。対数関数は単調増加な関数だからだ。対数関数のグラフの形を覚えているか？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
確かこんな感じのグラフだお。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/logistic-regression5.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それで正解だ。グラフがずっと右上がりになってることがわかるだろう。つまり単調増加な関数ってのは $x_1 &lt; x_2$ ならば $f(x_1) &lt; f(x_2)$ となるような関数 $f(x)$ だということだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なるほど、確かに $\log(x)$ はそういう風になっているお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
$\log$ は単調増加関数だから、今考えいてる対数尤度関数についても $L(\boldsymbol{\theta_1}) &lt; L(\boldsymbol{\theta_2})$ であれば $\log L(\boldsymbol{\theta_1}) &lt; \log L(\boldsymbol{\theta_2})$ ということが言えるだろう。要するに $L(\boldsymbol{\theta})$ を最大化することと $\log L(\boldsymbol{\theta})$ を最大化することは同じことなんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ふーん、よく考えられてるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では、対数尤度関数を少し変形して $f_{\boldsymbol{\theta}}$ を使って表してみよう。</p>

<p>$$
\begin{eqnarray}
\log L(\boldsymbol{\theta}) &amp; = &amp; \log \prod_{i=1}^n P(y^{(i)}=1|\boldsymbol{x})^{y^{(i)}} P(y^{(i)}=0|\boldsymbol{x})^{1-y^{(i)}} \<br />
&amp; = &amp; \sum_{i=1}^n \left( \log P(y^{(i)}=1|\boldsymbol{x})^{y^{(i)}} + \log P(y^{(i)}=0|\boldsymbol{x})^{1-y^{(i)}} \right) \<br />
&amp; = &amp; \sum_{i=1}^n \left( y^{(i)} \log P(y^{(i)}=1|\boldsymbol{x}) + (1-y^{(i)}) \log P(y^{(i)}=0|\boldsymbol{x}) \right) \<br />
&amp; = &amp; \sum_{i=1}^n \left( y^{(i)} \log f_{\boldsymbol{\theta}}(\boldsymbol{x}) + (1-y^{(i)}) \log (1-f_{\boldsymbol{\theta}}(\boldsymbol{x}) \right)
\end{eqnarray}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うっ、ちょっと式変形を追うのが大変だお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
実際には 2 行目は $\log(ab) = \log a + \log b$ という性質を、3 行目は $\log a^b = b \log a$ という性質を使っているだけだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
$\log$ の性質は覚えてるけど、最後の行はなんでそうなるんだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
確率の定義からだ。ここでは確率変数の取る値としては $y=1$ か $y=0$ かしかないから、$P(y^{(i)}=1|\boldsymbol{x}) = f_{\boldsymbol{\theta}}(\boldsymbol{x})$ ならば $P(y^{(i)}=0|\boldsymbol{x}) = 1 - f_{\boldsymbol{\theta}}(\boldsymbol{x})$ となる。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あー、そうか。全部の確率を足すと 1 になるんだったお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
ここまでで俺たちは目的関数として対数尤度関数 $\log L(\boldsymbol{\theta})$ を定義した。次はどうする？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
$\log L(\boldsymbol{\theta})$ を $\boldsymbol{\theta}$ の各要素 $\theta_j$ で微分&hellip;かお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
ではこれを微分していこう。</p>

<p>$$
\frac{\partial}{\partial \theta_j} \sum_{i=1}^n \left( y^{(i)} \log f_{\boldsymbol{\theta}}(\boldsymbol{x}) + (1-y^{(i)}) \log (1-f_{\boldsymbol{\theta}}(\boldsymbol{x}) \right)
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ちょっと何言ってるかよくわからん式だお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
回帰の時にやったように、合成関数の微分を使うんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
えーと、要するに&hellip;こうかお？</p>

<p>$$
\frac{\partial \log L(\boldsymbol{\theta})}{\partial f_{\boldsymbol{\theta}}} \cdot \frac{\partial f_{\boldsymbol{\theta}}}{\partial \theta_j}
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それでいい。ひとつずつ微分していくんだ。まず第一項はどうなるだろう？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
第一項は $\log L(\boldsymbol{\theta})$ を $f_{\boldsymbol{\theta}}$ で微分するんだから&hellip;えーっと、$\log(x)$ の微分は $\frac{1}{x}$ でよかったかお？</p>

<p>$$
\begin{eqnarray}
\frac{\partial \log L(\boldsymbol{\theta})}{\partial f_{\boldsymbol{\theta}}} &amp; = &amp; \frac{\partial}{\partial f_{\boldsymbol{\theta}}}\sum_{i=1}^n \left( y^{(i)} \log f_{\boldsymbol{\theta}}(\boldsymbol{x}) + (1-y^{(i)}) \log (1-f_{\boldsymbol{\theta}}(\boldsymbol{x}) \right) \<br />
&amp; = &amp; \sum_{i=1}^n \left( \frac{y^{(i)}}{f_{\boldsymbol{\theta}}(\boldsymbol{x})} - \frac{1-y^{(i)}}{1-f_{\boldsymbol{\theta}}(\boldsymbol{x})} \right) \<br />
\end{eqnarray}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
一気に第二項もやってしまうお。これも合成関数の微分を使うお。$z = 1 + \exp(-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x})$ と置いて $f_{\boldsymbol{\theta}}(\boldsymbol{x}) = z^{-1}$ とすると</p>

<p>$$
\begin{eqnarray}
\frac{\partial f_{\boldsymbol{\theta}}}{\partial \theta_j} &amp; = &amp; \frac{\partial f_{\boldsymbol{\theta}}}{\partial z} \cdot \frac{\partial z}{\partial \theta_j} \<br />
&amp; = &amp; -\frac{1}{z^2} \cdot -x_j \exp(-\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x}) \<br />
&amp; = &amp; \frac{x_j \exp(-\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x})}{(1 + \exp(-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x}))^2}
\end{eqnarray}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
できたお！これが微分結果だお。</p>

<p>$$
\frac{\partial \log L(\boldsymbol{\theta})}{\partial \theta_j} = \sum_{i=1}^n \left( \frac{y^{(i)}}{f_{\boldsymbol{\theta}}(\boldsymbol{x})} - \frac{1-y^{(i)}}{1-f_{\boldsymbol{\theta}}(\boldsymbol{x})} \right) \frac{x_j \exp(-\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x})}{(1 + \exp(-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x}))^2}
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだ、と言いたいところだが、ただ計算すればいいってもんじゃないだろ&hellip;やる夫はこれを見てどう思う？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
どう思うも何も、これでパラメータの更新式ができるお&hellip;まあ、二度と見たくもないような複雑な式ですお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そう、実はまだ複雑なんだ。もう少し式変形を進めて綺麗な形にできるんだが&hellip;</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やる夫の計算力が足りないってことかお！</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
いや、ここまでできただけでも十分だ。一緒に考えよう。まず、後ろの方の $\exp$ が含まれている式の方を見てみようか。やる夫は全部まとめているが、そこをあえてこんな風に分けてみよう。</p>

<p>$$
\frac{x_j \exp(\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x})}{(1 + \exp(-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x}))^2} = \frac{1}{1 + \exp(-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x})} \cdot \frac{\exp(-\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x})}{1 + \exp(-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x})} \cdot x_j
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
えっ、わざわざ分解するのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうするとこういう風に $f_{\boldsymbol{\theta}}(\boldsymbol{x})$ を使って書けるだろう。</p>

<p>$$
\frac{1}{1 + \exp(-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x})} \cdot \frac{\exp(-\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x})}{1 + \exp(-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x})} \cdot x_j = f_{\boldsymbol{\theta}}(\boldsymbol{x}) \cdot (1 - f_{\boldsymbol{\theta}}(\boldsymbol{x})) \cdot x_j
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あーなるほど。確かにやらない夫の言う通りに変形できそうだお。でも、第二項が $1 - f_{\boldsymbol{\theta}}(\boldsymbol{x})$ になるのはわかるけど、そんなの気付けないお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
式を変形する作業は慣れの問題も大きいだろうから、できなかったといってヘコむことはない。ちなみにシグモイド関数、ここでは $\sigma(x)$ と置こう、これの微分が $\sigma(x)(1-\sigma(x))$ になるのは有名だな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
確かに $f_{\boldsymbol{\theta}}$ はシグモイド関数で、それを微分した形はそうなってるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では、この式を微分結果に代入してみよう。約分した後に展開して整理すると最終的にはこんな形になる。</p>

<p>$$
\begin{eqnarray}
\frac{\partial \log L(\boldsymbol{\theta})}{\partial \theta_j} &amp; = &amp; \sum_{i=1}^n \left( \frac{y^{(i)}}{f_{\boldsymbol{\theta}}(\boldsymbol{x})} - \frac{1-y^{(i)}}{1-f_{\boldsymbol{\theta}}(\boldsymbol{x})} \right) f_{\boldsymbol{\theta}}(\boldsymbol{x}) (1 - f_{\boldsymbol{\theta}}(\boldsymbol{x})) x_j \<br />
&amp; = &amp; \sum_{i=1}^n \left( y^{(i)}(1 - f_{\boldsymbol{\theta}}(\boldsymbol{x})) - (1-y^{(i)})f_{\boldsymbol{\theta}}(\boldsymbol{x}) \right) x_j \<br />
&amp; = &amp; \sum_{i=1}^n \left(y^{(i)} - f_{\boldsymbol{\theta}}(\boldsymbol{x}) \right) x_j
\end{eqnarray}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
めちゃくちゃ単純な式になったお&hellip;やらない夫すごすぎるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
あとはいつものように、この式からパラメータ更新式を導出するんだ。ただし、回帰の時は最小化すればよかったが、今回は最大化なので注意が必要だ。そこが違うので、ただ代入すればいいというわけじゃない。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あ、確かにそうだお&hellip;もうすぐ終わりそうなのに、ここにきてまた壁がでてきたお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そう難しく考えるな。最大化問題を最小化問題に置き換える簡単な方法がある。符号を反転させればいいだけだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
まじかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
単純だろう。$f(x)$ を最大化させる問題と、その符号を反転させた $-f(x)$ を最小化させる問題はまったく同じ問題だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
希望が見えてきたお。要するにこれまでは $\log L(\boldsymbol{\theta})$ を最大化することを考えてきたけど、これからは $-\log L(\boldsymbol{\theta})$ を最小化することを考えればいいってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
その通りだ。符号が反転したんだから、微分結果の符号も反転する。$L^{-}(\boldsymbol{\theta}) = -\log L(\boldsymbol{\theta})$ と置くと、こうなるな。</p>

<p>$$
\frac{\partial L^{-}(\boldsymbol{\theta})}{\partial \theta_j} = \sum_{i=1}^n \left(f_{\boldsymbol{\theta}}(\boldsymbol{x}) - y^{(i)} \right) x_j
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
てことは、パラメータ更新式は、こうかお？</p>

<p>$$
\theta_j := \theta_j - \eta \sum_{i=1}^n \left(f_{\boldsymbol{\theta}}(\boldsymbol{x}) - y^{(i)} \right) x_j
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
よし、できたな。</p>

<h1 id="線形分離不可能問題">線形分離不可能問題</h1>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では、いよいよ線形分離不可能な問題にも挑戦していこうか。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
待ってましたお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
これが線形分離不可能だということはもういいな？</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron11.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
わかってるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
線形分離不可能な問題は、すなわち直線では分離できないということだ。であれば、曲線で分離すればいいという発想に自然といきつく。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/log-likelihood1.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
お、もしかして多項式回帰の時にやったように、次数を増やしてみるってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
勘がいいな。よし、これは 2 次元の話なので、素性としては $x_1$ と $x_2$ の 2 つがある。そこに 3 つ目の素性として $x_1^2$ を加える事を考えて見るんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
こういうことかお？</p>

<p>$$
\boldsymbol{\theta} = \left[
  \begin{array}{c}
    \theta_0 \<br />
    \theta_1 \<br />
    \theta_2 \<br />
    \theta_3
  \end{array}
\right] \ \ \
\boldsymbol{x} = \left[
  \begin{array}{c}
    1 \<br />
    x_1 \<br />
    x_2 \<br />
    x_1^2
  \end{array}
\right]
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。つまり $\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x}$ はこうだ。大丈夫か？</p>

<p>$$
\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
わかるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
また適当に $\boldsymbol{\theta}$ を決めてみよう。そうだな、たとえば $\boldsymbol{\theta}$ がこうなった時、それぞれ $y=1$ と $y=0$ と分類される領域はどこになる？</p>

<p>$$
\boldsymbol{\theta} = \left[
  \begin{array}{c}
    \theta_0 \<br />
    \theta_1 \<br />
    \theta_2 \<br />
    \theta_3
  \end{array}
\right] = \left[
  \begin{array}{c}
    0 \<br />
    0 \<br />
    1 \<br />
    -1
  \end{array}
\right]
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
えーっと、要するに $\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x} \ge 0$ を考えればいいんだから&hellip;前と同じように変形してみるお。</p>

<p>$$
\begin{eqnarray}
\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x} = x_2 - x_1^2 &amp; \ge &amp; 0 \<br />
x_2 &amp; \ge &amp; x_1^2
\end{eqnarray}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
こんな感じかお？</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/log-likelihood2.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
その通りだ。以前は決定境界が直線だったが、今は曲線になっているのが見て取れるだろう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
見た感じ、この決定境界だとまったく分類できてないように見えるけど、それはいつものようにやらない夫神が適当に $\boldsymbol{\theta}$ を決めたから、ってことでいいかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
もう慣れてきたようだな。やる夫の言う通りだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
じゃ、これをさっき求めたパラメータ更新式を使って、$\boldsymbol{\theta}$ を学習していけばいいってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
これで線形分離不可能な問題も解けるようになったな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
楽勝だお！</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
あとは、好きなように次数を増やせば複雑な形の決定境界にすることができる。たとえばさっきは素性として $x_1^2$ を増やしたが $x_2^2$ も増やすと、円状の決定境界ができる。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ロジスティック回帰のパラメータ更新って、最初に回帰で考えた時と同じように全データで学習してるけど、ここにも確率的勾配降下法って使えるのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
もちろん使えるぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ロジスティック回帰って万能だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
やる夫はいつも結論を出すのが早すぎだな。どんな問題に対しても万能なアルゴリズムなんて存在しないんだから適材適所で使っていかなければならない。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
今のやる夫はまだパーセプトロンとロジスティック回帰しか知らないんだから、ロジスティック回帰の方がやる夫にとっては有能な分類アルゴリズムだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それもそうだな。分類器はいろいろなアルゴリズムがあるから、勉強していくと楽しいぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
自習なんて面倒くさいお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
本当に面倒くさがりなんだな、お前&hellip;</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
天性だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
自慢するところかよ。しかしこれまでだいぶ理論を説明してきたが、理論だけじゃなくて実際になにか作ってみることも大事だ。理解度が飛躍的に上がるぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
じゃ、やっぱり前に言ってたアレ、作るお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
その熱意には負けるよ。</p>

<hr />

<p>やる夫で学ぶ機械学習 - 過学習 - へ続く。</p>

    </article>
    <footer class="tag">
      <i class="fa fa-tag"></i>
      
        <a class="tag-link" href="tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a>
      
        <a class="tag-link" href="tags/%E3%82%84%E3%82%8B%E5%A4%AB%E3%81%A7%E5%AD%A6%E3%81%B6">やる夫で学ぶ</a>
      
    </footer>
    <ul class="snsb">
      <li>
        <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://tkengo.github.io/blog/2016/06/16/yaruo-machine-learning6/" data-lang="ja">ツイート</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li>
        <div class="fb-like" data-href="http://tkengo.github.io/blog/2016/06/16/yaruo-machine-learning6/" data-layout="button_count" data-action="like" data-show-faces="false" data-share="false"></div>
      </li>
      <li>
        <a href="http://b.hatena.ne.jp/entry/http://tkengo.github.io/blog/2016/06/16/yaruo-machine-learning6/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
      </li>
    </ul>
  </main>
</div>

        
        <div class="container">
  <main>
    <header>
      <p class="day">2016-06-06</p>
      <h1><a href="http://tkengo.github.io/blog/2016/06/06/yaruo-machine-learning0/">やる夫で学ぶ機械学習シリーズ</a></h1>
    </header>
    <article>
      

<div style="font-size:18px;color:red;">
　本シリーズ記事が書籍になりました。詳しくはこちら。
<br/>
<br/>
<a href="/blog/2017/09/06/ml-math/">「やさしく学ぶ 機械学習を理解するための数学のきほん」を執筆しました</a>
</div>

<hr />

<p>これは、機械学習に関する基礎知識をまとめたシリーズ記事の目次となる記事です。まとめることで知識を体系化できて自分自身の為にもなるので、こういうアウトプットをすることは大事だと思っています。ただ、普通にブログ記事を書くのも面白くないので、ちょっといつもとは違う方法でやってみようというのが今回のシリーズ記事。</p>

<p>2 ちゃんねるのキャラクターが登場人物として出てきて、彼らが会話して話が進んでいく「やる夫で学ぶシリーズ」という講義調の形式のものがあります。個人的にはやる夫で学ぶシリーズや <a href="http://www.hyuki.com/girl/">数学ガール</a> のような会話形式で話が進んでいく読み物は読みやすいと思っています。さらに、先日みつけた <a href="http://www.ic.is.tohoku.ac.jp/~swk/lecture/yaruodsp/main.html">やる夫で学ぶディジタル信号処理</a> という資料がとてつもなくわかりやすく、これの真似をして書いてみようと思い至りました。記事中のやる夫とやらない夫のアイコンは <a href="http://matsucon.net/material/mona/">http://matsucon.net/material/mona/</a> こちらのサイトの素材を使わせていただきました。</p>

<p>第 2 回まで半年ほど前に公開していましたが、その後ブログ執筆の熱が冷めて放置されていました。が、実は下書きだけなら乱雑ながら第 5 回までは書いていて、周りにいる機械学習入門中の知り合い数名から下書きあるなら公開して欲しいと言われたので、お蔵入りさせるのも勿体ないし一気に清書して、この目次記事をつけて公開することにしました。今後、このシリーズ記事が増えるかどうかはわかりません。</p>

<h1 id="まえがき">まえがき</h1>

<p>このシリーズでは実践的な内容というよりかは、基礎的で理論的な部分をまとめていきます。機械学習をやり始めるにあたってまず最初にやったほうが良いのは、こういった座学のような記事を頑張って読むよりかは、やってみた系の記事を読みながら実際に手を動かしてコーディングしてみることです。いきなり機械学習の本や数式を眺めて理論から理解し始めるのは、数学に自信がある人や素養がある人以外は難しく、挫折してしまう原因となります。</p>

<p>今は良い時代になっていて、フレームワークを基盤として少しのコードを書いて、公開されている無料の学習用データを使えば、それらしいものが出来てしまいます。そういうもので感覚を掴んでから、理論を理解するのでも遅くはないと思っています。</p>

<p>とはいえ、プログラマであれば中身を知らないものをアレコレ触るのは怖さがあるというもの。理論の方に手をだしてみたくなったりもします。座学系の記事は、このブログ以外にもたくさん転がっているのでイロイロ読み比べて知識を自分のものにしていくのが大事です。一晩でなんとかなるものでもないですし、じっくりやっていきましょう。</p>

<h1 id="対象">対象</h1>

<ul>
<li>機械学習って最近よく聞くけど中身を良くしらない人</li>
<li>中身しらないけど機械学習って楽しそうだしなんか勉強してみたい人</li>
<li>数学が好きな人</li>
<li>ドヤ顔で機械学習のことを話したい人</li>
</ul>

<p>※上級者向けではありません。どちらかというとむしろ初学者向けです。</p>

<h1 id="目次">目次</h1>

<p>記事 1 つ 1 つが長く、分割していくうちに記事数が多くなってきたので、目次を作りました。</p>

<ul>
<li><a href="/blog/2016/01/03/yaruo-machine-learning1/">やる夫で学ぶ機械学習 - 序章 -</a></li>
<li><a href="/blog/2016/01/04/yaruo-machine-learning2/">やる夫で学ぶ機械学習 - 単回帰問題 -</a></li>
<li><a href="/blog/2016/06/02/yaruo-machine-learning3/">やる夫で学ぶ機械学習 - 多項式回帰と重回帰 -</a></li>
<li><a href="/blog/2016/06/03/yaruo-machine-learning4/">やる夫で学ぶ機械学習 - パーセプトロン -</a></li>
<li><a href="/blog/2016/06/04/yaruo-machine-learning5/">やる夫で学ぶ機械学習 - ロジスティック回帰 -</a></li>
<li><a href="/blog/2016/06/16/yaruo-machine-learning6/">やる夫で学ぶ機械学習 - 対数尤度関数 -</a></li>
</ul>

    </article>
    <footer class="tag">
      <i class="fa fa-tag"></i>
      
        <a class="tag-link" href="tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a>
      
        <a class="tag-link" href="tags/%E3%82%84%E3%82%8B%E5%A4%AB%E3%81%A7%E5%AD%A6%E3%81%B6">やる夫で学ぶ</a>
      
    </footer>
    <ul class="snsb">
      <li>
        <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://tkengo.github.io/blog/2016/06/06/yaruo-machine-learning0/" data-lang="ja">ツイート</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li>
        <div class="fb-like" data-href="http://tkengo.github.io/blog/2016/06/06/yaruo-machine-learning0/" data-layout="button_count" data-action="like" data-show-faces="false" data-share="false"></div>
      </li>
      <li>
        <a href="http://b.hatena.ne.jp/entry/http://tkengo.github.io/blog/2016/06/06/yaruo-machine-learning0/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
      </li>
    </ul>
  </main>
</div>

        
        <div class="container">
  <main>
    <header>
      <p class="day">2016-06-04</p>
      <h1><a href="http://tkengo.github.io/blog/2016/06/04/yaruo-machine-learning5/">やる夫で学ぶ機械学習 - ロジスティック回帰 -</a></h1>
    </header>
    <article>
      

<p>やる夫で学ぶ機械学習シリーズの第 5 回です。分類問題を解くためのロジスティック回帰を見ていきます。</p>

<p>第 4 回はこちら。<a href="/blog/2016/06/03/yaruo-machine-learning4/">やる夫で学ぶ機械学習 - パーセプトロン -</a></p>

<p>目次はこちら。<a href="/blog/2016/06/06/yaruo-machine-learning0/">やる夫で学ぶ機械学習シリーズ</a></p>

<h1 id="ロジスティック回帰">ロジスティック回帰</h1>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
分類問題を解くための素晴らしいアルゴリズムがあると聞きましたお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
今日は展開が早いな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
回りくどいのは終わりだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では、今日はロジスティック回帰の話をしていこう。ロジスティック回帰は、パーセプトロンのように基本的には二値分類の分類器を構築するためのものだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
パーセプトロンよりは使い物になるのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
もちろんさ。ロジスティック回帰はいろんなところで使われているし、線形分離不可能な問題も解ける。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
それはナイスだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
いつものように最初は簡単な具体例を示して概要を見ていこうか。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
よろしくお願いしますお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
問題設定はパーセプトロンの時と同じものを使おう。つまり、色を暖色か寒色に分類することを考えるんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
それ、線形分離可能な問題だお？線形分離不可能な問題やらないのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
物事には順序ってものがあるだろう。先に基礎からやるんだよ。ロジスティック回帰も、もちろん線形分離可能な問題は解ける。まずそこから入って、その応用で最後に線形分離不可能な問題を見ていこう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
基礎力つけるの面倒くさいけど、わかったお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
ロジスティック回帰は分類を確率として考えるんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
確率？暖色である確率が 80%、寒色である確率が 20%、みたいな話ってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだ、いつもとぼけた顔してる割には冴えてるじゃないか。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
顔は生まれつきだお。文句いうなお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
暖色、寒色のままでは扱いにくいのはパーセプトロンと同じだから、ここでは暖色を $1$、寒色を $0$ と置くとしよう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あれ、寒色は $-1$ じゃないのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
クラス毎の値が異なっていれば別になんでもいいんだが、パーセプトロンの時に暖色が $1$ で寒色が $-1$ にしたのは、そうした方が重みの更新式が簡潔に書けるからだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なるほど。ロジスティック回帰の場合は暖色を $1$ で寒色を $0$ にした方が、重みの更新式が簡潔に書き表せるってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだな。話を進めよう。回帰の時に、未知のデータ $\boldsymbol{x}$ に対応する値を求めるためにこういう関数を定義したのを覚えているか？</p>

<p>$$
f_{\boldsymbol{\theta}}(\boldsymbol{x}) = \boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
覚えてるお。学習用データに最も良くフィットするパラメータ $\boldsymbol{\theta}$ を求めた時だお。あの時は最急降下法か確率的勾配降下法を使ってパラメータ $\boldsymbol{\theta}$ の更新式を導出したんだったお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
ロジスティック回帰でも考え方は同じだ。未知のデータがどのクラスに分類されるかを求めたい時に、それを分類してくれるための関数が必要になるな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
パーセプトロンでやった時の識別関数 $f_{\boldsymbol{w}}$ みたいなもんかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。今回も回帰の時と同じように $\boldsymbol{\theta}$ を使うことにして、ロジスティック回帰の関数をこのように定義しよう。</p>

<p>$$
f_{\boldsymbol{\theta}}(\boldsymbol{x}) = \frac{1}{1 + \exp(-\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{x})}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
急に難易度が上がるのは、このシリーズのセオリーかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
ぱっと見て難しそうだと感じるのはわかるが、落ち着いて考えるんだ。第一印象で無理かどうかを決めるのは良くないぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
$\exp(x)$ ってのは $\mathrm{e}^x$ ってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだ。この関数は一般的にシグモイド関数と呼ばれるんだが、$\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x}$ を横軸、$f_{\boldsymbol{\theta}}$ を縦軸だとすると、グラフの形はこんな風になっている。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/logistic-regression1.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
お、ものすごくなめらかな形をしているお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
特徴としては、$\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} = 0$ の時に $f_{\boldsymbol{\theta}} = 0.5$ になっている。それから、グラフを見ればすぐわかると思うが $0 \le f_{\boldsymbol{\theta}} \le 1$ だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
グラフの形なんか示してどうするんだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
目に見えるように表現すると見えないモノが見えてくる。今は、分類を確率で考えようとしているんだ。覚えているか？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
覚えてるお。あ&hellip;そうか、シグモイド関数は $0 \le f_{\boldsymbol{\theta}} \le 1$ だから確率として扱える、ってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
その通りだ。ここからは、未知のデータ $\boldsymbol{x}$ が暖色、つまり $y=1$ である確率を $f_{\boldsymbol{\theta}}$ とするんだ。</p>

<p>$$
P(y=1|\boldsymbol{x}) = f_{\boldsymbol{\theta}}(\boldsymbol{x})
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
具体例を見てみようか。そうだな、$f_{\boldsymbol{\theta}}(\boldsymbol{x})$ を計算すると $0.7$ になったとしよう。これはどういう状態だ？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あ、えーっと&hellip;$f_{\boldsymbol{\theta}}(\boldsymbol{x}) = 0.7$ ってことは、暖色である確率が 70% ってことかお？逆に寒色である確率は 30%。普通に考えて $\boldsymbol{x}$ は暖色ってことになるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では今度は、$f_{\boldsymbol{\theta}}(\boldsymbol{x}) = 0.2$ の時はどうだろう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
暖色が 20% で、寒色が 80% だから $\boldsymbol{x}$ は寒色だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
やる夫は今おそらく、$f_{\boldsymbol{\theta}}(\boldsymbol{x})$ の閾値を $0.5$ としてクラスを振り分けているはずだ。</p>

<p>\begin{eqnarray}
y =
  \begin{cases}
    1 &amp; (f_{\boldsymbol{\theta}}(\boldsymbol{x}) \ge 0.5) \\[5pt]
    0 &amp; (f_{\boldsymbol{\theta}}(\boldsymbol{x}) &lt; 0.5)
  \end{cases}
\end{eqnarray}</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あぁ、別に意識はしてなかったけど、確かにそうだお。$f_{\boldsymbol{\theta}}(\boldsymbol{x}) \ge 0.5$ なら暖色だと思うし、逆は寒色だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
$f_{\boldsymbol{\theta}}(\boldsymbol{x}) \ge 0.5$ というのは、もっと詳しく見るとどういう状態だ？シグモイド関数のグラフを思い出して考えてみるんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
んー&hellip;？シグモイド関数は $\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} = 0$ の時に $f_{\boldsymbol{\theta}}(\boldsymbol{x}) = 0.5$ だったお&hellip;あっ、要するに $f_{\boldsymbol{\theta}}(\boldsymbol{x}) \ge 0.5$ ってことは $\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} \ge 0$ ってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
正解だ。さっきの場合分けの条件式の部分を書き直すとこうだな。</p>

<p>\begin{eqnarray}
y =
  \begin{cases}
    1 &amp; (\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} \ge 0) \\[5pt]
    0 &amp; (\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} &lt; 0)
  \end{cases}
\end{eqnarray}</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
シグモイド関数のグラフをみると、こんな風に分類される感じだな。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/logistic-regression2.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なるほど、わかりやすいお。でも、条件式の部分って $f_{\boldsymbol{\theta}}(\boldsymbol{x}) \ge 0.5$ でも $\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} \ge 0$ でも同じ意味ならなんでわざわざ書き直すんだお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では、今度はパーセプトロンの時に見たような、横軸が赤($x_1$)、縦軸が青($x_2$) のグラフを考えてみようか。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あぁ、色をプロットしていたアレかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
今回の問題、素性としては赤 ($x_1$) と青 ($x_2$) の 2 次元だが、回帰の時と同じように $\theta_0$ と $x_0$ も含めて、全体としては 3 次元ベクトルで考えるぞ。いいか？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
大丈夫だお。$x_0 = 1$ は固定だったお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
具体的に考えるために、適当にパラメータ $\boldsymbol{\theta}$ を決めよう。そうだな、こういう $\boldsymbol{\theta}$ があった時、暖色である場合の条件式 $\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} \ge 0$ をグラフに表すとどうなる？</p>

<p>$$
\boldsymbol{\theta} = \left[
  \begin{array}{c}
    \theta_0 \\ \theta_1 \\ \theta_2
  \end{array}
\right] = \left[
  \begin{array}{c}
    -100 \\ 2 \\ 1
  \end{array}
\right]
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、条件式をグラフに&hellip;とりあえず $\boldsymbol{\theta}$ を代入して、わかりやすいように変形してみるお&hellip;</p>

<p>$$
\begin{eqnarray}
\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} = -100 + 2 x_1 + x_2 &amp; \ge &amp; 0 \<br />
x_2 &amp; \ge &amp; - 2 x_1 + 100
\end{eqnarray}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
これをグラフに&hellip;こうかお？</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/logistic-regression3.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
ここまでくれば $\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} &lt; 0$ の場合がどうなるかも想像できるな？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
今度はさっきの反対側ってことかお。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/logistic-regression4.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
つまり $\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x} = 0$ という直線を境界線として、一方が暖色($y=1$)、もう一方が寒色($y=0$)、とクラス分けできるというわけだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
これは直感的でわかりやすいお！パーセプトロンの時にも $\boldsymbol{w} \cdot \boldsymbol{x} = 0$ というクラスを分類するための線が出てきたけど、それと同じものってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。このようなクラスを分割する線を決定境界、英語では Decision Boundary と呼ぶ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
この決定境界、暖色と寒色を分類する線としては全く正しくなさそうだけど、それはやらない夫が適当にパラメータの $\boldsymbol{\theta}$ を決めたから、ってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだ。ということは、これから何をやっていくのかは、想像つくだろう？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
正しいパラメータ $\boldsymbol{\theta}$ を求めるために、目的関数を定義して、微分して、パラメータの更新式を求める、であってるかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
よくわかってるじゃないか。</p>

<h1 id="尤度関数">尤度関数</h1>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あとは回帰と同じなら楽勝だお。今日はもう帰ってオンラインゲームでもやるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだったら良かったんだが、世の中そんなに甘くないぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
はっ&hellip;そういえば今日は週 1 のメンテナンス日だったお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
お前、そういうことじゃないだろ、常識的に考えて&hellip;。ロジスティック回帰は、二乗誤差の目的関数だとうまくいかないから別の目的関数を定義するんだ。最初にやった回帰と同じ手法ではやらない。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
そうなのかお&hellip;帰ってもやることないから、やらない夫の講義に付き合ってやるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
その上から目線、癪に障るが&hellip;ロジスティック回帰の目的関数を考えよう。パーセプトロンの時に準備した学習用データについて、さっき俺が適当に決めたパラメータ $\boldsymbol{\theta}$ を使って、実際に $f_{\boldsymbol{\theta}}(\boldsymbol{x})$ を計算してみよう。ここで示す $f_{\boldsymbol{\theta}}(\boldsymbol{x})$ の計算結果は全く厳密ではないが、イメージをつかむにはそれで十分だろう。</p>

<table>
<thead>
<tr>
<th>色</th>
<th>クラス</th>
<th>$y$</th>
<th>$f_{\boldsymbol{\theta}}(\boldsymbol{x})$</th>
</tr>
</thead>

<tbody>
<tr>
<td><span style="width:100px;height:20px;background-color:#d80055;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #d80055</td>
<td>暖色</td>
<td>1</td>
<td>0.00005</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#c80027;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #c80027</td>
<td>暖色</td>
<td>1</td>
<td>0.00004</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#9c0019;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #9c0019</td>
<td>暖色</td>
<td>1</td>
<td>0.00002</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#2c00c8;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #2c00c8</td>
<td>寒色</td>
<td>0</td>
<td>0.99991</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#120078;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #120078</td>
<td>寒色</td>
<td>0</td>
<td>0.99971</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#40009f;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #40009f</td>
<td>寒色</td>
<td>0</td>
<td>0.99953</td>
</tr>
</tbody>
</table>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
$f_{\boldsymbol{\theta}}(\boldsymbol{x})$ は $\boldsymbol{x}$ が暖色である確率だと定義した。これは覚えているな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
大丈夫だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では、それを踏まえた上で、$y$ と $f_{\boldsymbol{\theta}}(\boldsymbol{x})$ はどういう関係にあるのが理想的だと思う？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
えっ、えーっと&hellip; $f_{\boldsymbol{\theta}}(\boldsymbol{x})$ は $\boldsymbol{x}$ が暖色である確率なんだから&hellip;正しく分類されるためには、$y=1$ の時に $f_{\boldsymbol{\theta}}(\boldsymbol{x})$ が $1$ に近くて、$y=0$ の時に $f_{\boldsymbol{\theta}}(\boldsymbol{x})$ が $0$ に近い方がいい、ってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。理想の状態はやる夫の言った通りで正解だが、以下のようにも言い換えることができる。</p>

<ul>
<li>$y=1$ の時は $P(y=1|\boldsymbol{x})$ が最大になって欲しい</li>
<li>$y=0$ の時は $P(y=0|\boldsymbol{x})$ が最大になって欲しい</li>
</ul>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
$P(y=1|\boldsymbol{x})$ は $\boldsymbol{x}$ が暖色である確率、逆に $P(y=0|\boldsymbol{x})$ が $\boldsymbol{x}$ が寒色である確率、という理解であってるかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それでいい。これを全ての学習用データについて考えるんだ。すると目的関数は以下のような同時確率として考えることができる。これを最大化する $\boldsymbol{\theta}$ を見つけることが目的だ。</p>

<p>$$
L(\boldsymbol{\theta}) = \prod_{i=1}^n P(y^{(i)}=1|\boldsymbol{x})^{y^{(i)}} P(y^{(i)}=0|\boldsymbol{x})^{1-y^{(i)}}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
さて、帰るお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
期待通りの反応をありがとう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
どうやったらこんな意味不明な式が思いつくんだお&hellip;。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
確かに、式を簡潔にするために多少トリックを使ってはいるが、やる夫でも理解できるはずだ。1 つずつ考えよう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫の丁寧な説明をお待ちしておりますお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
$y^{(i)}$ が $1$ と $0$ の場合をそれぞれ考えてみよう。$y^{(i)}=1$ のデータの場合は後ろ側の $P(y^{(i)}=0|\boldsymbol{x})^{1-y^{(i)}}$ が $1$ になる。逆に $y^{(i)}=0$ のデータの場合は前の $P(y^{(i)}=1|\boldsymbol{x})^{y^{(i)}}$ が $1$ になる。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、なんでだお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
両方とも 0 乗になるだろう。$P(y^{(i)}=1|\boldsymbol{x})$ や $P(y^{(i)}=0|\boldsymbol{x})$ がどんな数だったとしても、べき乗のところが 0 になるからだ。$P(y^{(i)}=1|\boldsymbol{x})^0$ も $P(y^{(i)}=0|\boldsymbol{x})^0$ も両方とも 0 乗なんだから、1 になるのは明確だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あ、なるほど。確かに、言われてみればそうだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
まとめるとこうだ。</p>

<ul>
<li>$y^{(i)}=1$ の時は $P(y^{(i)}=1|\boldsymbol{x})$ の項が残る</li>
<li>$y^{(i)}=0$ の時は $P(y^{(i)}=0|\boldsymbol{x})$ の項が残る</li>
</ul>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
つまり全ての学習用データにおいて、正解ラベルと同じラベルに分類される確率が最大になるような同時確率を考えているということだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うーん、わかったようなわかってないような&hellip;前の回帰の時は目的関数の最小化だったけど、今回は最大化するのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。二乗誤差はその名の通り &ldquo;誤差&rdquo; なので小さいほうが理想的だ。だが、今回は同時確率だ。その同時確率が最も高くなるようなパラメータ $\boldsymbol{\theta}$ こそ、学習用データにフィットしていると言える。そういった、尤もらしいパラメータを求めたいんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
国語のお勉強も足りてないお&hellip;それなんて読むんだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
&ldquo;尤もらしい&rdquo; は &ldquo;もっともらしい&rdquo; と読む。さっき定義した関数も、尤度関数とも呼ばれる。これは &ldquo;ゆうどかんすう&rdquo; だな。目的関数に使った文字 $L$ も、尤度を英語で表した時の $Likelihood$ の頭文字から取った。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やっぱり、ここにきて難易度あがってるお。面倒くさいけど、あとで復習するお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。どうせ家に帰ってやることがないんだろう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
バカにするなお&hellip;</p>

<hr />

<p><a href="/blog/2016/06/16/yaruo-machine-learning6/">やる夫で学ぶ機械学習 - 対数尤度関数 -</a> へ続く。</p>

    </article>
    <footer class="tag">
      <i class="fa fa-tag"></i>
      
        <a class="tag-link" href="tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a>
      
        <a class="tag-link" href="tags/%E3%82%84%E3%82%8B%E5%A4%AB%E3%81%A7%E5%AD%A6%E3%81%B6">やる夫で学ぶ</a>
      
    </footer>
    <ul class="snsb">
      <li>
        <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://tkengo.github.io/blog/2016/06/04/yaruo-machine-learning5/" data-lang="ja">ツイート</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li>
        <div class="fb-like" data-href="http://tkengo.github.io/blog/2016/06/04/yaruo-machine-learning5/" data-layout="button_count" data-action="like" data-show-faces="false" data-share="false"></div>
      </li>
      <li>
        <a href="http://b.hatena.ne.jp/entry/http://tkengo.github.io/blog/2016/06/04/yaruo-machine-learning5/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
      </li>
    </ul>
  </main>
</div>

        
        <div class="container">
  <main>
    <header>
      <p class="day">2016-06-03</p>
      <h1><a href="http://tkengo.github.io/blog/2016/06/03/yaruo-machine-learning4/">やる夫で学ぶ機械学習 - パーセプトロン -</a></h1>
    </header>
    <article>
      

<p>やる夫で学ぶ機械学習シリーズの第 4 回です。分類問題を解くための基礎、パーセプトロンを図形的な側面から覗いてみます。</p>

<p>第 3 回はこちら。<a href="/blog/2016/06/02/yaruo-machine-learning3/">やる夫で学ぶ機械学習 - 多項式回帰と重回帰 -</a></p>

<p>目次はこちら。<a href="/blog/2016/06/06/yaruo-machine-learning0/">やる夫で学ぶ機械学習シリーズ</a></p>

<h1 id="問題設定">問題設定</h1>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
今日は分類問題について詳しく見ていく。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
分類問題かお。やる夫は、女の子が巨乳か貧乳かを分類して、想像を膨らませたいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そんなものは深夜に一人でニヤニヤしながらやるか、いつも引きこもってないで外に出て本物の女の子を見ることだな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
冗談きついお、やらない夫。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
分類の場合も、回帰の時と同じように具体例を示して、それを元に話を進めていったほうがわかりやすいな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
それが良いお。具体例は、やる夫の将来の嫁さんくらい大事だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
また意味のわからないたとえをありがとう。今回は分類の話なので、そうだな、色を分類することを考えてみよう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
おっぱいじゃなくて、色を分類、するのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
おっぱいは忘れろ。たとえば、適当に与えられた色が、暖色系なのか寒色系なのかに分類する、という問題はどうだ？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
二値分類の問題かお。分類は確か教師あり学習だったから、つまりラベル付きの学習用データが要るってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな、具体的には、色の情報と、その色が暖色なのか寒色なのか、というラベルを学習用データとして用意してやる必要がある。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なるほど。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
ところで、色と言えば RGB の三色を考えることができるが、最初は簡単な問題の方がいいから、緑のことは考えずに、赤と青だけに注目していこう。緑の要素は 0 に固定しようか。その方が図にもプロットできてわかりやすいしな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
簡単になるなら歓迎だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
たとえば、この色は暖色系、寒色系、どっちに見える？</p>

<div style="text-align:center;font-family:mplus-1p-regular;">
<span style="width:100px;height:20px;background-color:#d80055;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #d80055
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
んー、暖色系だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では、これはどうだ？</p>

<div style="text-align:center;font-family:mplus-1p-regular;">
<span style="width:100px;height:20px;background-color:#2c00c8;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #2c00c8
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
寒色系だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
ということは、今 2 つの学習用データができた。緑の要素を 0 に固定しているから、#RRGGBB の GG の部分が 00 になっていることに注目だ。</p>

<table>
<thead>
<tr>
<th>色</th>
<th>クラス</th>
</tr>
</thead>

<tbody>
<tr>
<td><span style="width:100px;height:20px;background-color:#d80055;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #d80055</td>
<td>暖色</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#2c00c8;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #2c00c8</td>
<td>寒色</td>
</tr>
</tbody>
</table>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うん、分類問題の場合の学習用データのイメージつかめたお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
横軸を赤、縦軸を青、とすると、今の学習用データはこんな風にプロットできる。大丈夫か？</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron1.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
楽勝だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さすがに、学習用データが 2 つしかないと足りないので、もう少し増やしておこう。</p>

<table>
<thead>
<tr>
<th>色</th>
<th>クラス</th>
</tr>
</thead>

<tbody>
<tr>
<td><span style="width:100px;height:20px;background-color:#d80055;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #d80055</td>
<td>暖色</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#c80027;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #c80027</td>
<td>暖色</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#9c0019;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #9c0019</td>
<td>暖色</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#2c00c8;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #2c00c8</td>
<td>寒色</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#120078;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #120078</td>
<td>寒色</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#40009f;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #40009f</td>
<td>寒色</td>
</tr>
</tbody>
</table>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
プロットしてみたお。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron2.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
よし、OK だ。この図を見て、このデータ点を分類するために 1 本だけ線を引くとしたら、やる夫はどんな風に線を引く？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、そんなの、誰でもこんな風に線を引くと思うお。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron3.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
よし。今回の目的はその線を見つけること、それが二値分類だ。</p>

<h1 id="内積">内積</h1>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
線をみつけるって、つまり回帰の時と同じように一次関数の傾きと切片を求めるのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
いや、今回はベクトルを見つけるのが目的だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ベクトルを見つける&hellip;。やらない夫、意味がわからんお&hellip;。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さっきの線は、重みベクトルを法線とする直線なんだ。重みベクトルを $\boldsymbol{w}$ とすると、直線の方程式はこのように表せる。</p>

<p>$$
\boldsymbol{w}\cdot\boldsymbol{x} = 0
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うーん、重みベクトルって一体なんなんだお&hellip;、その方程式の意味もよくわからんお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
重みベクトルは俺たちが知りたい未知のパラメータだ。回帰の時に $\boldsymbol{\theta}$ という未知のパラメータを求めるために色々と頑張ったが、それと同じものだな。$w$ という文字は weight(重み) の頭文字だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
$\boldsymbol{w}\cdot\boldsymbol{x}$ って、ベクトルの内積のことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そう、内積のことだ。実ベクトル空間の内積は、各要素の積を足し上げたものなので、こんな風にも書ける。$n$ は次元数だ。</p>

<p>$$
\boldsymbol{w}\cdot\boldsymbol{x} = \sum_{i=1}^n w_i x_i = 0
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ふむ。今回は赤と青の二次元を考えているので、$n = 2$ ってことでいいのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うーん、内積は覚えてるけど、でも、さっきの方程式&hellip;、なんなんだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
わからない時はいつでも、具体的な値を代入して計算してみよう。イメージがわくかもしれないぞ。やる夫が言ったように、今回の問題は $\mathbb{R}^2$ の実ベクトル空間上で考えているので、そうだな、重みベクトルを $\boldsymbol{w} = (1, 1)$ とした時、$\boldsymbol{x}$ はどうなる？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
とりあえず、代入してみるお。</p>

<p>\begin{eqnarray}
\boldsymbol{w}\cdot\boldsymbol{x} &amp; = &amp; w_1 x_1 + w_2 x_2 \<br />
&amp; = &amp; 1\cdot x_1 + 1\cdot x_2 \<br />
&amp; = &amp; x_1 + x_2 = 0
\end{eqnarray}</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、これって要するに $x_2 = -x_1$ だから、$\boldsymbol{x} = (1, -1)$ とか $\boldsymbol{x} = (2, -2)$ とか、解は無限にありそうだけど&hellip;、あー、つまり、傾き -1 のグラフってことかお？</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron4.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
よく閃いたな。その図に、重みベクトル $\boldsymbol{w} = (1, 1)$ を書き加えてみると、もっとわかりやすいかもしれないな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
えっと、$(1, 1)$ だから&hellip;</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron5.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あっ、重みベクトル $\boldsymbol{w}$ が、さっきやる夫が書いた直線と、直角になってるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
つまり重みベクトル $\boldsymbol{w}$ が、やる夫が書いた直線に対する法線になっているというわけだ。最初に俺が言ったように、$\boldsymbol{w}\cdot\boldsymbol{x} = 0$ という式は、重みベクトル $\boldsymbol{w}$ を法線とする直線の方程式になっている。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
そういうことかお。そういえば、内積の計算って $\cos$ が含まれてなかったかお？ベクトル同士の成す角 $\theta$ を使って..</p>

<p>$$
\boldsymbol{w}\cdot\boldsymbol{x} = |\boldsymbol{w}|\cdot|\boldsymbol{x}|\cdot\cos\theta
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それでも問題ない。さっき、やる夫はベクトル方程式から代数的に解いたが、幾何的にはベクトル同士が直角になっている時に内積が 0 になる。$\theta = 90^{\circ}, 270^{\circ}$ の時には $\cos\theta = 0$ になるからな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あーなるほど。そういう直角になってるベクトルがたくさんあって、それ全体が直線になってるのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
いろんな側面から覗いてみると、より理解も深まる。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
じゃ、話は戻るけど、さっきの色分類の問題に関するグラフで言うと、こんな風にやる夫が引いた直線に対する法線を見つける、ってことかお。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron6.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだ。まあ正確にはもちろん最初に直線があるわけではなく、最初に法線、つまり重みベクトルを見つけると、そのベクトルに対して直角な直線がわかり、その直線によってデータが分類される、というわけだな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
把握したお。</p>

<h1 id="パーセプトロン">パーセプトロン</h1>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
で、具体的にはどうやって、その重みベクトルとやらを求めるのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
回帰でやった時のように、重みベクトルをパラメータとして、パラメータを更新していくんだ。これから見ていくのは、パーセプトロンと呼ばれるモデルだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なんか突然かっこいい単語が出てきたお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
パーセプトロンは脳の機能を参考にモデル化したものだと言われることが多いな。ローゼンブラットという人物が考案したモデルなんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
今度は RPG の主人公キャラの名前かお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
パーセプトロンは複数の入力を受け取り、その入力に対する重みとの線形結合が一定の閾値を超えていた場合に活性化し、そうでない場合は非活性となるような単純な関数だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
線形結合とか活性化とか非活性とか、一体やらない夫は何を言ってるんだお。中二病なのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
いや、そういうわけではないが&hellip;。そうだな、つまらない形式的な話はよしておこう。形式的な話より直感的な話をしていこうか。やる夫もそっちの方が好きだろう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
それでいいんだお。ところで、パラメータの更新っていうと、また微分とか出てくるのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
あの時は目的関数があって、それを最小化したいがために微分したんだった。だが、今回は目的関数は不要なので、微分もしない。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なるほど。分類も回帰と同じ流れかと思ったけど、そうでもないのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
分類問題でも、もちろん目的関数を使うこともあって、その場合には目的関数を最小化する手順は同じようにある。ただ、パーセプトロンのように目的関数が必要ないような場合もある。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ふーん。じゃ、今回のパラメータの更新式は、どんなものになるんだお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
その前に、パラメータ更新に必要になるものを、いくつか準備をしておこう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うっ、前準備って、面倒くさくてあんまり好きじゃないお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
このまま先に進んでも、どうせやる夫はわからないだろうからな。まあ、そんなにたくさん準備がいるわけじゃないから安心しろ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫、いま、しれっとやる夫のこと馬鹿にしたお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
まず、最初に用意した学習用データの、暖色と寒色。このままだと扱いにくいから、暖色を 1、寒色を -1 として、$y$ で表すことにする。そして、横軸を赤、縦軸を青、としていたが、それぞれ $x_1$ と $x_2$ としよう。</p>

<table>
<thead>
<tr>
<th>色</th>
<th>クラス</th>
<th>$x_1$</th>
<th>$x_2$</th>
<th>$y$</th>
</tr>
</thead>

<tbody>
<tr>
<td><span style="width:100px;height:20px;background-color:#d80055;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #d80055</td>
<td>暖色</td>
<td>216</td>
<td>85</td>
<td>1</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#c80027;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #c80027</td>
<td>暖色</td>
<td>200</td>
<td>39</td>
<td>1</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#9c0019;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #9c0019</td>
<td>暖色</td>
<td>156</td>
<td>25</td>
<td>1</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#2c00c8;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #2c00c8</td>
<td>寒色</td>
<td>44</td>
<td>200</td>
<td>-1</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#120078;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #120078</td>
<td>寒色</td>
<td>18</td>
<td>120</td>
<td>-1</td>
</tr>

<tr>
<td><span style="width:100px;height:20px;background-color:#40009f;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> #40009f</td>
<td>寒色</td>
<td>64</td>
<td>159</td>
<td>-1</td>
</tr>
</tbody>
</table>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そして、ベクトル $\boldsymbol{x}$ を与えると、暖色か寒色を判定する関数、つまり 1 または -1 を返す関数 $f_{\boldsymbol{w}}$ をこのように定義する。このような関数を「識別関数」と呼ぶ。</p>

<p>\begin{eqnarray}
f_{\boldsymbol{w}}(\boldsymbol{x}) =
  \begin{cases}
     1 &amp; (\boldsymbol{w}\cdot\boldsymbol{x} \geq 0) \\[5pt]
    -1 &amp; (\boldsymbol{w}\cdot\boldsymbol{x} \lt 0)
  \end{cases}
\end{eqnarray}</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、要するに内積の符号によって返す値が違うってことかお。こんなんで、暖色か寒色が判定できるのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
内積の図形的なイメージを思い出すんだ。例えば、重みベクトル $\boldsymbol{w}$ に対して、内積が負になるベクトルってどんなベクトルだ？$\cos$ が含まれた方の式で考えてみるんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、$\cos$ を使った内積は $|\boldsymbol{w}|\cdot|\boldsymbol{x}|\cdot\cos\theta$ だから&hellip;、ベクトルの大きさは必ず正になるから、符号は $\cos\theta$ の値で決まることになるお&hellip;、これが負になるってことは、$90^{\circ} \lt \theta \lt 270^{\circ}$ の場合、ってことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
正解だ。そういうベクトル $\boldsymbol{x}$ は、図形的にはどんな位置にあると思う？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
重みベクトル $\boldsymbol{w}$ との成す角が $90^{\circ} \lt \theta \lt 270^{\circ}$ の範囲内にあるベクトルってことだから&hellip;、あー、なるほど、直線を挟んで重みベクトルとは反対側の部分、ってことかお。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron7.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
その通り。と、いうことは、内積が正のベクトルは、重みベクトルと同じ側の部分ってことだな。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron8.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なるほど！内積が正か負かで、直線を挟んで分割できてるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
内積は、ベクトル同士がどれだけ似ているかという指標にも使われる。符号が正だとある程度似ていて、0 で直角になり、そして負になると、だんだん元のベクトルから離れていく。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
内積にはそういう意味もあったのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さて、準備はこれで終わりだ。話を戻すと、パラメータの更新式はこのように定義する。$i$ は学習用データのインデックスだ。このパラメータ更新を全ての学習用データについて行っていく。</p>

<p>\begin{eqnarray}
\boldsymbol{w} :=
  \begin{cases}
    \boldsymbol{w} + y^{(i)}\boldsymbol{x}^{(i)} &amp; (f_{\boldsymbol{w}}(\boldsymbol{x}^{(i)}) \neq y^{(i)}) \\[5pt]
    \boldsymbol{w}                               &amp; (f_{\boldsymbol{w}}(\boldsymbol{x}^{(i)}) = y^{(i)})
  \end{cases}
\end{eqnarray}</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
んー、なんかまた、ごちゃごちゃした式だお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
意味がわからない数式を見た時は、いったん落ち着くんだ。難しそうな式がごちゃごちゃしているのは確かだが、その式の文字を 1 つずつ読み解いていき、ボトムアップで全体像を把握していけばいい。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
んっと、じゃ、まずは $f_{\boldsymbol{w}}(\boldsymbol{x}^{(i)}) \neq y^{(i)}$ の方から見ていくお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それがいい。その条件は、どういうことを表していると思う？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
$i$ 番目の学習用データを識別関数に通して分類した結果と、同じく $i$ 番目の学習用データのラベル $y$ が異なっている、ってことだお。あ、ということは、識別関数による分類に失敗した、ってことなのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
やるじゃないか、やる夫の言う通りだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
もう一つの $f_{\boldsymbol{w}}(\boldsymbol{x}^{(i)}) = y^{(i)}$ の方は、逆に分類に成功した、って意味だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだ。つまり、先のパラメータ更新式は、識別関数による分類が失敗した時だけ新しいパラメータに更新されるという意味だ。分類に成功した場合は、パラメータの重みベクトルは正しいはずだと捉えて更新をしない。そこ、大丈夫か？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
わかるお。分類が成功した時、つまり $f_{\boldsymbol{w}}(\boldsymbol{x}^{(i)}) = y^{(i)}$ の時は $\boldsymbol{w}$ がそのまま代入されて、元と何も変わってないから、そういうことだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では今度は、識別関数による分類が失敗した時の更新式を見てみようか。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
$\boldsymbol{w} + y^{(i)}\boldsymbol{x}^{(i)}$ の方かお。うーん&hellip;、さっきから考えてるけど、未だにまったく意味がわからんお&hellip;。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
式だけ眺めてても難しいかもしれないな。こちらも図形的に考えてみるといい。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
グラフに書いてみるってことかお。ん、でも、そんな式どうやって図に書けばいいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
具体例を考えるんだよ。まずは、以下の図にあるような適当な重みベクトルがあったとして、その状態でラベルが $y = 1$、つまり暖色系の学習用データを与えた場合にどうなるかを考えてみよう。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron9.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
上の図の場合、重みベクトル $\boldsymbol{w}$ と学習用データ $\boldsymbol{x}^{(1)}$ との成す角は $90^{\circ}$ 以上だから内積が負になって、識別関数 $f_{\boldsymbol{w}}$ は -1 を返すはずだお。そして、与えられた学習用データのラベルは $y = 1$ だから、識別関数による分類に失敗しているお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
パラメータ更新式の $f_{\boldsymbol{w}}(\boldsymbol{x}^{(1)}) \neq y^{(1)}$ という状況だな。つまり、パラメータが更新されるというわけだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なるほど。いま、$y^{(1)} = 1$ だから、更新式は $\boldsymbol{w} + \boldsymbol{x}^{(1)}$ になるわけだお。んー、普通のベクトルの足し算だから、さっきの図に書き足してみるお&hellip;</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron10.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
重みベクトル $\boldsymbol{w}$ が更新されて、最初に引いてた直線が回転したお！</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
新しい重みベクトルの元で識別関数に通すと、今度はちゃんと分類に成功することもわかるだろう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
さっきは、$\boldsymbol{x}^{(1)}$ が直線を挟んで重みベクトルと反対側にあったけど、新しい重みベクトルで見た場合、$\boldsymbol{x}^{(1)}$ は重みベクトルと同じ側にあるから確かに良さそうだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
学習用データのラベルが $y = -1$ の場合は、今度はパラメータ更新がベクトルの引き算になるわけだな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
足し算と引き算の違いはあるけど、同じように新しい重みベクトルに更新されて、その分だけ直線が回転するってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
この繰り返しによってパラメータを更新していくのがパーセプトロンの学習だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
とてもよくわかりましたお。顔画像を準備して、巨乳なら 1、そうじゃないなら -1 ってラベルをつけていけば、未知の女の子をパーセプトロンで分類できそうだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
お前、まだそれ諦めてなかったのかよ&hellip;そんなものはおそらくパーセプトロンでは分類できないだろうな。</p>

<h1 id="線形分離可能">線形分離可能</h1>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
またやる夫のことバカにしてるのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
パーセプトロンによる学習はとてもシンプルでわかりやすいが、その分デメリットも大きい。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
えっ&hellip;。やらない夫は、いつも最後に残念な想いをさせるから、そういうのやめて欲しいお。終わり良ければ全て良しだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
最初に嫌な話をしても、それはそれでその後のモチベーションが下がるだろ。それに、デメリットを知っておくことは大事だぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
そのデメリットによってやる夫の夢は打ち砕かれるのかお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
パーセプトロンの最たるデメリットは線形分離可能な問題しか解けないということだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
線形分離可能？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
たとえば、こんな学習用データがあったとしよう。赤い点が 1 で、青いバツが -1 とするんだ。これを分類するために 1 本だけ直線を引くとしたら、やる夫はどんな風に線を引く？</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/perceptron11.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
いや、こんなのどうやっても直線 1 本じゃ分類できないお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだ、この例のように直線 1 本では分類できないようなものは線形分離可能ではない。つまり、パーセプトロンでは分類できない。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やる夫が作ろうとしている分類器も線形分離可能ではない、って言いたいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
画像を扱う場合、入力がかなりの高次元になるため可視化はできないが、顔の特徴を掴んで分類しようとするタスクはそんなに単純じゃない。確実に線形分離不可能だろうな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
パーセプトロンなんて使い物にならんお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
確かに、パーセプトロンは現場で使われることはほとんど無い。実際に解きたい問題が線形分離可能であることはほぼありえないからな。ほとんどが線形分離不可能な問題だ。でも安心しろ。ちゃんと実用的な分類器を構築するためのアルゴリズムもあるんだぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
一旦落としておいて、その後に希望を見せるそのやり方、クセになるお。解決策があるって素晴らしいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
偉大な先人たちのおかげだな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ところで、やらない夫は大きいのと小さいのはどっちが好きなんだお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
お前、ほんと好きだな、それ&hellip;</p>

<hr />

<p><a href="/blog/2016/06/04/yaruo-machine-learning5/">やる夫で学ぶ機械学習 - ロジスティック回帰 -</a> へ続く。</p>

    </article>
    <footer class="tag">
      <i class="fa fa-tag"></i>
      
        <a class="tag-link" href="tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a>
      
        <a class="tag-link" href="tags/%E3%82%84%E3%82%8B%E5%A4%AB%E3%81%A7%E5%AD%A6%E3%81%B6">やる夫で学ぶ</a>
      
    </footer>
    <ul class="snsb">
      <li>
        <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://tkengo.github.io/blog/2016/06/03/yaruo-machine-learning4/" data-lang="ja">ツイート</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li>
        <div class="fb-like" data-href="http://tkengo.github.io/blog/2016/06/03/yaruo-machine-learning4/" data-layout="button_count" data-action="like" data-show-faces="false" data-share="false"></div>
      </li>
      <li>
        <a href="http://b.hatena.ne.jp/entry/http://tkengo.github.io/blog/2016/06/03/yaruo-machine-learning4/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
      </li>
    </ul>
  </main>
</div>

        
        <div class="container">
  <main>
    <header>
      <p class="day">2016-06-02</p>
      <h1><a href="http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/">やる夫で学ぶ機械学習 - 多項式回帰と重回帰 -</a></h1>
    </header>
    <article>
      

<p>やる夫で学ぶ機械学習シリーズの第 3 回です。多項式回帰と重回帰について見ていきます。</p>

<p>第 2 回はこちら。<a href="/blog/2016/01/04/yaruo-machine-learning2/">やる夫で学ぶ機械学習 - 単回帰問題 -</a></p>

<p>目次はこちら。<a href="/blog/2016/06/06/yaruo-machine-learning0/">やる夫で学ぶ機械学習シリーズ</a></p>

<h1 id="多項式回帰">多項式回帰</h1>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さて、回帰の話にはもう少し続きがあるんだが。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あんまり難しすぎるのは勘弁だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
前回の延長だから、回帰が理解できていれば、そう難しい話ではないだろう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
その前に、ちょっとおしっこいってくるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
お前、そういうのは休憩中にやっとけよ&hellip;</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
わかったお！仕方ないから我慢するお！！</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
意外と根性あるな&hellip;、ではこれから、前回の回帰の話をもう少し発展させていく。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
(あっ、ほんとに話を進めるのかお&hellip;)</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
前回、俺たちは、求めたい関数をこのように定義して、パラメータである $\theta$ を求めた。</p>

<p>$$
f_{\theta}(x) = \theta_0 + \theta_1 x
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
覚えてるお。$f_{\theta}(x)$ を使った目的関数を定義して、最急降下法でパラメータの $\theta$ を求めたんだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
$f_{\theta}(x)$ は一次関数だから、関数の形は当然のことながら直線になる。そこは大丈夫だよな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なんか回りくどいお。前回やったことはちゃんと覚えてるから、復習ならしなくていいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうか。では、本題に入るが、最初に示したプロットは、実は直線より曲線の方がよりフィットするんだ。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/linear-regression3.png">
<img src="/assets/img/yaruo/machine-learning/linear-regression7.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
お、なるほど。確かに、曲線のグラフの方が、よりフィットしているように見えるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
これは、関数 $f_\theta(x)$ を二次式として定義することで実現できる。</p>

<p>$$
f_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
あるいは、それ以上の次数の式として定義すれば、より複雑な曲線にも対応できる。</p>

<p>$$
f_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 + \cdots + \theta_n x^n
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
どういう式にするのかは、勝手に自分で決めていいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな、解きたい問題に対して、どの式が一番フィットするのかを見極めながら、いろいろ試してみる必要はあるがな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
う、なんか面倒くさそうだお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
経験と勘がモノを言う部分だな。どれが最適なのかは、俺にもわからないからな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫にもわからないなら、仕方ないお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さっきの二次式を見返してみると、新しいパラメータ $\theta_2$ が増えているな。このあと、どうすればいいかわかるか？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、前と同じように偏微分して、パラメータ更新していけばいいのかお？こんな風に&hellip;</p>

<p>\begin{eqnarray}
\theta_0 &amp; := &amp; \theta_0 - \eta\sum_{i=1}^n (f_{\theta} (x^{(i)}) - y^{(i)}) \<br />
\theta_1 &amp; := &amp; \theta_1 - \eta\sum_{i=1}^n (f_{\theta} (x^{(i)}) - y^{(i)})x^{(i)} \<br />
\theta_2 &amp; := &amp; \theta_2 - \eta\sum_{i=1}^n (f_{\theta} (x^{(i)}) - y^{(i)}){x^{(i)}}^2
\end{eqnarray}</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。この式を使って $\theta_0, \theta_1, \theta_2$ を更新していけば、よりデータにフィットした $f_{\theta}(x)$ が得られる。当然だが、前にも言ったようにそれぞれの $\theta$ は同時に更新していく必要がある。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
これって、パラメータが $\theta_3, \theta_4, \cdots$ って増えていっても、同じことを繰り返せばいいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだ。こんな風に多項式の次数を増やした関数を使うものを多項式回帰と言うんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
次数が高ければ高いほど、より正確に未知のデータを予測できる関数になるってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
いや、そこはそうとも限らない。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
えっ、なんでだお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
過剰適合、またはオーバーフィッティング (Overfitting) と呼ばれる問題があってな。もちろん、オーバーフィッティングに対する解決策はある。が、それはまたの機会にとっておこう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
もったいぶらないで説明してほしいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
回帰の話にもう少し続きがあるから、先にそっちを説明したいんだ。それに、一度になんでもかんでも詰め込んでも、頭がパンクしてしまうぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
そうかお&hellip;、やらない夫は言いくるめるのが上手だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
言い方に悪意を感じるな、お前&hellip;</p>

<h1 id="重回帰">重回帰</h1>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
おしっこ行ってきたお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
多項式回帰の話の続き、いくぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
よろしくお願いしますお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
今までは学習用データの変数は 1 つしかなった。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
学習用データの変数？攻撃力 $x$ のことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだ。先の例では、攻撃力という 1 つの変数で、ダメージが決まっていた。ただ、実際は変数が 2 つ以上の複雑な問題なことが多い。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
さっきの多項式回帰で $x$ に加えて $x^2$ や $x^3$ を使う場合ってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
いや、そうじゃない。確かに多項式回帰には別々の次数を持った複数の項があるが、実際に俺たちが使った変数は「攻撃力」のみだ。そうだろ？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
「攻撃力」と「攻撃力の 2 乗」と「攻撃力の 3 乗」の 3 つを使った、ってことじゃないのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
まあそうだが、本質的な変数は「攻撃力」のみだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
言ってることがよくわからんお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
今考えている問題設定を少し拡張するぞ。最初は、攻撃力が決まればダメージが決まる、という設定だったが、ダメージを決める要素は「攻撃力」の他に、実は「Lv」と「体力」という要素があるとしよう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あー、なるほど。変数が 2 つ以上って、そういうことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだな。ところで、これまでは「変数」という言い方をしてきたが、機械学習の現場では「素性」と言ったりする。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あぁ、なんか聞いたことがあるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
素性が 1 つの時はグラフにプロットできたが、素性が 3 つとなると可視化することはできない。これからは図は省略することになるが&hellip;</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なんか先が思いやられる展開だお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
まぁ、これまでの話が理解できているのであれば大丈夫なはずだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫の説明力を信じて、ついていくお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
今までは攻撃力を $x$ と置いていたが、これからは攻撃力を $x_1$、Lvを $x_2$、体力を $x_3$ と置くとすると、$f_{\theta}$ はどうなるかわかるか？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、こんなんでいいのかお&hellip;？</p>

<p>$$
f_{\theta}(x_1, x_2, x_3) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
正解だ。この時のパラメータ $\theta_0, \cdots, \theta_3$ を求めるにはどうすればいいかは、もうわかるな？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
目的関数を $\theta_0, \cdots, \theta_3$ についてそれぞれ偏微分して、パラメータを更新していけばいいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。では、実際のパラメータの更新式はどうなる？と聞きたいところだが、その前に式の表記をより簡単にしておこう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、どういうことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
素性が一般に $n$ 個ある場合のことを考えよう。$x_1, \cdots, x_n$ までの素性があるときの $f_{\theta}$ はどうなる？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
こうなるお。</p>

<p>$$
f_{\theta}(x_1, \cdots, x_n) = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
毎回そんな風に $n$ 個の $x$ を書いていくのは大変だから、パラメータ $\theta$ と素性 $x$ をベクトルとみなすんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ベクトルって、大きさと向きがある、矢印で表すアレかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それだな。まあ、今回は矢印というイメージは出てこないが&hellip;、むしろ列ベクトルだな。$\theta$ と $x$ をベクトルとして定義するとは、どういうことだと思う？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
んー、列ベクトルなんだったら、こういうことかお？</p>

<p>$$
\boldsymbol{\theta} = \left[
  \begin{array}{c}
    \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n
  \end{array}
\right] \ \ \
\boldsymbol{x} = \left[
  \begin{array}{c}
    x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array}
\right]
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
おしいな。そのままだと、それぞれで次元が違うだろ。$\boldsymbol{\theta}$ は添字が 0 から始まるから $n+1$ 次元で、$\boldsymbol{x}$ の方は $n$ 次元だ。それだと扱いにくい。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
いや、そんなこと言われても、文字はもうこれだけしかないんだお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
何もベクトルの要素が全て文字じゃなくてもいい。こんな風に書きなおしてみよう。</p>

<p>$$
\boldsymbol{\theta} = \left[
  \begin{array}{c}
    \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n
  \end{array}
\right] \ \ \
\boldsymbol{x} = \left[
  \begin{array}{c}
    1 \\ x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array}
\right]
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
勝手に 1 を足していいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
むしろ、こうやって最初に 1 を置くほうが自然なんだ。$\boldsymbol{\theta}$ の方と合わせるために $x_0 = 1$ として、最初の要素に $x_0$ を置いてもいいな。</p>

<p>$$
\boldsymbol{\theta} = \left[
  \begin{array}{c}
    \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n
  \end{array}
\right] \ \ \
\boldsymbol{x} = \left[
  \begin{array}{c}
    x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array}
\right] \ \ \ (x_0 = 1)
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さて、ここで、$\boldsymbol{\theta}$ を転置したものと $\boldsymbol{x}$ を掛けてやると、どうなると思う？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、ベクトルの各要素を掛けあわせて、それを全部足せばいいんだから、ちょっと面倒くさいけど、えっと&hellip;、こうかお。</p>

<p>$$
\boldsymbol{\theta}^{\rm{T}}\boldsymbol{x} = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あ、$x_0 = 1$ だから、これって $f_{\theta}$ だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだ。つまり多項式で表していた $f_{\theta}$ は、ベクトルを使うと以下のように表せる。</p>

<p>$$
f_{\boldsymbol{\theta}}(\boldsymbol{x}) = \boldsymbol{\theta}^{\rm{T}}\boldsymbol{x}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ずいぶんと簡潔になったお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そして、$\boldsymbol{\theta}$ の $j$ 番目の要素を $\theta_j$ とすると、$E$ を $\theta_j$ で偏微分した時の式は、こんな風に書ける。ベクトルとスカラー値で、太字かそうでないかをちゃんと使い分けてるから、気をつけろよ。</p>

<p>$$
\frac{\partial E}{\partial \theta_j} = \sum_{i=1}^n (f_{\boldsymbol{\theta}} (\boldsymbol{x}^{(i)}) - y^{(i)}){x_j}^{(i)}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なるほど。前から思ってたけど、やっぱりこの偏微分には規則性があったのかお。$j=0$ の時、つまり $\theta_0$ で偏微分する時も、後ろについてる $x_j$ は $x_0 = 1$ なので、結局は最初にやる夫が自分で微分した式と一致するお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
要するにパラメータの更新式は、こうだな。各 $\theta_j$ は同時更新する必要があるのは相変わらずだから気をつけろよ。</p>

<p>$$
\theta_j := \theta_j - \eta\sum_{i=1}^n (f_{\boldsymbol{\theta}} (\boldsymbol{x}^{(i)}) - y^{(i)}){x_j}^{(i)}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ミッション・コンプリートだお！！</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
な、今までの話が理解できていれば、素性が増えたとしても大したことは無いだろう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やる夫が優秀だからだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
突然どこから出てくるんだよ、その自信。こんな風に素性を 2 つ以上使ったものを重回帰と言うんだ。逆に、最初にやった例のように素性が 1 つの場合は、単回帰と言うな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
単回帰に、多項式回帰に、重回帰。覚えたお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
回帰の話はこの辺で終わりだな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
最後のベクトル化から続く怒涛の一般化ラッシュ、気持よかったお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
一般化して考えられるのは、数学のいいところだな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ところで、これまでやってきた最急降下法、パラメータの更新にシグマが含まれてるから、全学習データ分だけループすることになるんだお。学習用データが大量にあった場合、for ループの回数が増えて、時間がかかってしまわないかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さすがプログラミングが得意なだけあって、効率云々を気にするんだな。そうだ、やる夫の言うとおり、計算量が多くて遅いことが最急降下法の欠点の 1 つだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
まあ、性能の良いマシンを買えば解決するかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
金に物を言わせるなよ&hellip;、そこはプログラマっぽくないな、お前。ちゃんと、もっと速いアルゴリズムがあるんだよ。むしろ、最急降下法よりも、そっちの速いアルゴリズムの方が好んで使われるな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
じゃ、最初からそっちの速いアルゴリズムの方を教えてくれればよかったお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
速いアルゴリズムの方は最急降下法をベースにしてるんだよ。先に速いアルゴリズムを説明しても、どうせ最急降下法から説明することになるんだから、結局は同じことだ。それに、こういうことは基礎からちゃんとやっといたほうが、後で応用が聞くんだぞ？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うっ、やっぱりやらない夫には言いくるめられてしまうお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
お前、言い方&hellip;</p>

<h1 id="確率的勾配降下法">確率的勾配降下法</h1>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
最後に確率的勾配降下法というアルゴリズムを見ていこう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
それが、さっき言った速いアルゴリズムのことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。最急降下法には、さっきやる夫がいった計算に時間がかかること以外にも、収束が遅かったり、それから局所解に捕まってしまう、というデメリットもある。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
局所解につかまるって、どういうことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
最急降下法を説明する時に使った関数の例は $g(x) = (x - 1)^2$ という二次関数だったから問題なかったが、もっと複雑な形の関数を考えてみよう。この関数の最小値は、赤い点で示してある位置だな。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/sgd1.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ぐにゃぐにゃしてるグラフだお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
最急降下法で関数の最小値を見つけるにしても、まず最初に初期値を決める必要がある。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あー、確かにやったお。例の時は $x = 3$ からはじめたお。あれは、なんで 3 からスタートさせたのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
俺が適当に選んだ値だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫神に導かれるままについていきますお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうじゃなくて、初期値は適当に決めていいって意味だ。ただ、そのせいで局所解に捕まるという問題が発生するんだ。たとえば、この図の青い点が初期値だったとしたら、どうだ？</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/sgd2.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あー、なんとなくわかってきたお。その青い点が初期値だったら、最急降下法でちゃんと赤い点の最小値が求まるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では、赤い点の最小値が求まらないのは、どういう場合だ？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
たとえば、今度はこっちの青い点が初期値だった場合、赤い点まで移動せずに、たぶんオレンジの点で止まってしまうお。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/sgd3.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それが局所解に捕まるということだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
アルゴリズムがシンプルな分、いろいろな問題があるってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そこで、そういった最急降下法のデメリットを克服したアルゴリズムが、確率的勾配降下法だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ふむ。具体的にはどういうアルゴリズムなんだお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
最急降下法のパラメータ更新の式は覚えてるか？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
覚えてるお。</p>

<p>$$
\theta_j := \theta_j - \eta\sum_{i=1}^n (f_{\boldsymbol{\theta}} (\boldsymbol{x}^{(i)}) - y^{(i)}){x_j}^{(i)}
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
よし。その式では、パラメータの更新に全ての学習用データの誤差を使っているが、確率的勾配降下法ではランダムに学習用データを 1 つ選んで、それをパラメータの更新に使うんだ。式中の $k$ は、パラメータ更新毎にランダムに選ばれたインデックスだ。</p>

<p>$$
\theta_j := \theta_j - \eta(f_{\boldsymbol{\theta}} (\boldsymbol{x}^{(k)}) - y^{(k)}){x_j}^{(k)}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
お、シグマがなくなってるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。最急降下法で 1 回パラメータを更新する間に、確率的勾配降下法では $n$ 回パラメータを更新できる。また、学習用データをランダムに選んで、その時点での勾配でパラメータを更新していくので、目的関数の局所解に捕まりにくい。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
そんなに適当なやり方で、ちゃんとした答えが得られるのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
この方法でも、実際に最適解に収束するということがわかっている。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うーん、なんか不思議だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
もちろん、学習率 $\eta$ を適切に設定することは大事だ。特に、学習する度に $\eta$ を小さくしていく、という手法も存在する。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
そうだ、最急降下法の時も思ってたけど、$\eta$ の適切な値って、どうやって見つければいいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
難しい問題だな。解決のためのアイデアは、興味があれば探してみるといい。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫、実は知らないのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
これ以上続けると、話が長くなりすぎるからな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あっ、はぐらかしたお。読者のことを考えてるふりをするなんて、ずるいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
まあ、そういうところは、実際に実装しはじめて困ったときに探すので十分だ。やり始める前から、なんでもかんでも一気に詰め込むのはよくないぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
はいはいですお。</p>

<hr />

<p><a href="/blog/2016/06/03/yaruo-machine-learning4/">やる夫で学ぶ機械学習 - パーセプトロン -</a> へ続く。</p>

    </article>
    <footer class="tag">
      <i class="fa fa-tag"></i>
      
        <a class="tag-link" href="tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a>
      
        <a class="tag-link" href="tags/%E3%82%84%E3%82%8B%E5%A4%AB%E3%81%A7%E5%AD%A6%E3%81%B6">やる夫で学ぶ</a>
      
    </footer>
    <ul class="snsb">
      <li>
        <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/" data-lang="ja">ツイート</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li>
        <div class="fb-like" data-href="http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/" data-layout="button_count" data-action="like" data-show-faces="false" data-share="false"></div>
      </li>
      <li>
        <a href="http://b.hatena.ne.jp/entry/http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
      </li>
    </ul>
  </main>
</div>

        
        <a href="http://tkengo.github.io/post" class="archives">Archives</a>
      </div>
      <footer class="footer">
  <p class="copyright">copyright&nbsp;&copy;&nbsp;2015&nbsp;<a href="https://twitter.com/tkengo" target="_blank">&copy;tkengo</a>&nbsp;-&nbsp;Powered by <a href="http://gohugo.io" target="_blank">Hugo</a>, Designed by <a href="https://twitter.com/keita_kawamoto" target="_blank">&copy;keita_kawamoto</a></p>
  <ul class="social">
    <li><a href="https://twitter.com/tkengo" target="_blank"><i class="fa fa-twitter"></i></a></li>
    <li><a href="https://github.com/tkengo" target="_blank"><i class="fa fa-github"></i></a></li>
  </ul>
</footer>

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/ja_JP/sdk.js#xfbml=1&version=v2.4&appId=327080140754787";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-35368510-1', 'auto');
    ga('send', 'pageview');
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/languages/go.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

    </div>
  </body>
</html>
