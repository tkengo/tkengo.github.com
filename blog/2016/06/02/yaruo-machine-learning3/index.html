<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width">
  <meta name="author" content="@tkengo">
  <meta http-equiv="X-UA-Compatible" CONTENT="IE=EmulateIE7" />
  
    <title>やる夫で学ぶ機械学習 - 多項式回帰と重回帰 - &middot; けんごのお屋敷</title>
  
  <link rel="stylesheet" href="http://tkengo.github.io/css/common.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="shortcut icon" href="http://tkengo.github.io/images/favicon.ico">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.7/styles/atelier-cave.light.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.7/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  
  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
  <style type="text/css">
    @font-face {
      font-family: mplus-1p-regular;
      src: url('http://mplus-fonts.sourceforge.jp/webfonts/mplus-1p-regular.ttf') format("truetype");
      font-family: mplus-1p-bold;
      src: url('http://mplus-fonts.sourceforge.jp/webfonts/mplus-1p-bold.ttf') format("truetype");
    }
  </style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]] },
      TeX: { extensions: ["color.js"] }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

  <body>
    <div class="container-bg">
      <div class="header">
  <header>
    <h1 class="logo"><a href="http://tkengo.github.io/">けんごのお屋敷</a></h1>
    <ul class="menu">
      <li><a href="/post">Archives</a></li>
    </ul>
  </header>
</div>

      <div class="container">
  <main>
    <header>
      <p class="day">2016-06-02</p>
      <h1><a href="http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/">やる夫で学ぶ機械学習 - 多項式回帰と重回帰 -</a></h1>
    </header>
    <article>
      

<p>やる夫で学ぶ機械学習シリーズの第 3 回です。多項式回帰と重回帰について見ていきます。</p>

<p>第 2 回はこちら。<a href="/blog/2016/01/04/yaruo-machine-learning2/">やる夫で学ぶ機械学習 - 単回帰問題 -</a></p>

<p>目次はこちら。<a href="/blog/2016/06/06/yaruo-machine-learning0/">やる夫で学ぶ機械学習シリーズ</a></p>

<h1 id="多項式回帰">多項式回帰</h1>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さて、回帰の話にはもう少し続きがあるんだが。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あんまり難しすぎるのは勘弁だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
前回の延長だから、回帰が理解できていれば、そう難しい話ではないだろう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
その前に、ちょっとおしっこいってくるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
お前、そういうのは休憩中にやっとけよ&hellip;</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
わかったお！仕方ないから我慢するお！！</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
意外と根性あるな&hellip;、ではこれから、前回の回帰の話をもう少し発展させていく。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
(あっ、ほんとに話を進めるのかお&hellip;)</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
前回、俺たちは、求めたい関数をこのように定義して、パラメータである $\theta$ を求めた。</p>

<p>$$
f_{\theta}(x) = \theta_0 + \theta_1 x
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
覚えてるお。$f_{\theta}(x)$ を使った目的関数を定義して、最急降下法でパラメータの $\theta$ を求めたんだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
$f_{\theta}(x)$ は一次関数だから、関数の形は当然のことながら直線になる。そこは大丈夫だよな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なんか回りくどいお。前回やったことはちゃんと覚えてるから、復習ならしなくていいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうか。では、本題に入るが、最初に示したプロットは、実は直線より曲線の方がよりフィットするんだ。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/linear-regression3.png">
<img src="/assets/img/yaruo/machine-learning/linear-regression7.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
お、なるほど。確かに、曲線のグラフの方が、よりフィットしているように見えるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
これは、関数 $f_\theta(x)$ を二次式として定義することで実現できる。</p>

<p>$$
f_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
あるいは、それ以上の次数の式として定義すれば、より複雑な曲線にも対応できる。</p>

<p>$$
f_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 + \cdots + \theta_n x^n
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
どういう式にするのかは、勝手に自分で決めていいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな、解きたい問題に対して、どの式が一番フィットするのかを見極めながら、いろいろ試してみる必要はあるがな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
う、なんか面倒くさそうだお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
経験と勘がモノを言う部分だな。どれが最適なのかは、俺にもわからないからな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫にもわからないなら、仕方ないお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さっきの二次式を見返してみると、新しいパラメータ $\theta_2$ が増えているな。このあと、どうすればいいかわかるか？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、前と同じように偏微分して、パラメータ更新していけばいいのかお？こんな風に&hellip;</p>

<p>\begin{eqnarray}
\theta_0 &amp; := &amp; \theta_0 - \eta\sum_{i=1}^n (f_{\theta} (x^{(i)}) - y^{(i)}) \<br />
\theta_1 &amp; := &amp; \theta_1 - \eta\sum_{i=1}^n (f_{\theta} (x^{(i)}) - y^{(i)})x^{(i)} \<br />
\theta_2 &amp; := &amp; \theta_2 - \eta\sum_{i=1}^n (f_{\theta} (x^{(i)}) - y^{(i)}){x^{(i)}}^2
\end{eqnarray}</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。この式を使って $\theta_0, \theta_1, \theta_2$ を更新していけば、よりデータにフィットした $f_{\theta}(x)$ が得られる。当然だが、前にも言ったようにそれぞれの $\theta$ は同時に更新していく必要がある。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
これって、パラメータが $\theta_3, \theta_4, \cdots$ って増えていっても、同じことを繰り返せばいいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだ。こんな風に多項式の次数を増やした関数を使うものを多項式回帰と言うんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
次数が高ければ高いほど、より正確に未知のデータを予測できる関数になるってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
いや、そこはそうとも限らない。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
えっ、なんでだお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
過剰適合、またはオーバーフィッティング (Overfitting) と呼ばれる問題があってな。もちろん、オーバーフィッティングに対する解決策はある。が、それはまたの機会にとっておこう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
もったいぶらないで説明してほしいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
回帰の話にもう少し続きがあるから、先にそっちを説明したいんだ。それに、一度になんでもかんでも詰め込んでも、頭がパンクしてしまうぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
そうかお&hellip;、やらない夫は言いくるめるのが上手だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
言い方に悪意を感じるな、お前&hellip;</p>

<h1 id="重回帰">重回帰</h1>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
おしっこ行ってきたお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
多項式回帰の話の続き、いくぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
よろしくお願いしますお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
今までは学習用データの変数は 1 つしかなった。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
学習用データの変数？攻撃力 $x$ のことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだ。先の例では、攻撃力という 1 つの変数で、ダメージが決まっていた。ただ、実際は変数が 2 つ以上の複雑な問題なことが多い。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
さっきの多項式回帰で $x$ に加えて $x^2$ や $x^3$ を使う場合ってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
いや、そうじゃない。確かに多項式回帰には別々の次数を持った複数の項があるが、実際に俺たちが使った変数は「攻撃力」のみだ。そうだろ？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
「攻撃力」と「攻撃力の 2 乗」と「攻撃力の 3 乗」の 3 つを使った、ってことじゃないのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
まあそうだが、本質的な変数は「攻撃力」のみだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
言ってることがよくわからんお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
今考えている問題設定を少し拡張するぞ。最初は、攻撃力が決まればダメージが決まる、という設定だったが、ダメージを決める要素は「攻撃力」の他に、実は「Lv」と「体力」という要素があるとしよう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あー、なるほど。変数が 2 つ以上って、そういうことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだな。ところで、これまでは「変数」という言い方をしてきたが、機械学習の現場では「素性」と言ったりする。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あぁ、なんか聞いたことがあるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
素性が 1 つの時はグラフにプロットできたが、素性が 3 つとなると可視化することはできない。これからは図は省略することになるが&hellip;</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なんか先が思いやられる展開だお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
まぁ、これまでの話が理解できているのであれば大丈夫なはずだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫の説明力を信じて、ついていくお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
今までは攻撃力を $x$ と置いていたが、これからは攻撃力を $x_1$、Lvを $x_2$、体力を $x_3$ と置くとすると、$f_{\theta}$ はどうなるかわかるか？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、こんなんでいいのかお&hellip;？</p>

<p>$$
f_{\theta}(x_1, x_2, x_3) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
正解だ。この時のパラメータ $\theta_0, \cdots, \theta_3$ を求めるにはどうすればいいかは、もうわかるな？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
目的関数を $\theta_0, \cdots, \theta_3$ についてそれぞれ偏微分して、パラメータを更新していけばいいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。では、実際のパラメータの更新式はどうなる？と聞きたいところだが、その前に式の表記をより簡単にしておこう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、どういうことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
素性が一般に $n$ 個ある場合のことを考えよう。$x_1, \cdots, x_n$ までの素性があるときの $f_{\theta}$ はどうなる？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
こうなるお。</p>

<p>$$
f_{\theta}(x_1, \cdots, x_n) = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
毎回そんな風に $n$ 個の $x$ を書いていくのは大変だから、パラメータ $\theta$ と素性 $x$ をベクトルとみなすんだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ベクトルって、大きさと向きがある、矢印で表すアレかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それだな。まあ、今回は矢印というイメージは出てこないが&hellip;、むしろ列ベクトルだな。$\theta$ と $x$ をベクトルとして定義するとは、どういうことだと思う？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
んー、列ベクトルなんだったら、こういうことかお？</p>

<p>$$
\boldsymbol{\theta} = \left[
  \begin{array}{c}
    \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n
  \end{array}
\right] \ \ \
\boldsymbol{x} = \left[
  \begin{array}{c}
    x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array}
\right]
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
おしいな。そのままだと、それぞれで次元が違うだろ。$\boldsymbol{\theta}$ は添字が 0 から始まるから $n+1$ 次元で、$\boldsymbol{x}$ の方は $n$ 次元だ。それだと扱いにくい。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
いや、そんなこと言われても、文字はもうこれだけしかないんだお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
何もベクトルの要素が全て文字じゃなくてもいい。こんな風に書きなおしてみよう。</p>

<p>$$
\boldsymbol{\theta} = \left[
  \begin{array}{c}
    \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n
  \end{array}
\right] \ \ \
\boldsymbol{x} = \left[
  \begin{array}{c}
    1 \\ x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array}
\right]
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
勝手に 1 を足していいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
むしろ、こうやって最初に 1 を置くほうが自然なんだ。$\boldsymbol{\theta}$ の方と合わせるために $x_0 = 1$ として、最初の要素に $x_0$ を置いてもいいな。</p>

<p>$$
\boldsymbol{\theta} = \left[
  \begin{array}{c}
    \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n
  \end{array}
\right] \ \ \
\boldsymbol{x} = \left[
  \begin{array}{c}
    x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array}
\right] \ \ \ (x_0 = 1)
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さて、ここで、$\boldsymbol{\theta}$ を転置したものと $\boldsymbol{x}$ を掛けてやると、どうなると思う？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ん、ベクトルの各要素を掛けあわせて、それを全部足せばいいんだから、ちょっと面倒くさいけど、えっと&hellip;、こうかお。</p>

<p>$$
\boldsymbol{\theta}^{\rm{T}}\boldsymbol{x} = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あ、$x_0 = 1$ だから、これって $f_{\theta}$ だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そういうことだ。つまり多項式で表していた $f_{\theta}$ は、ベクトルを使うと以下のように表せる。</p>

<p>$$
f_{\boldsymbol{\theta}}(\boldsymbol{x}) = \boldsymbol{\theta}^{\rm{T}}\boldsymbol{x}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ずいぶんと簡潔になったお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そして、$\boldsymbol{\theta}$ の $j$ 番目の要素を $\theta_j$ とすると、$E$ を $\theta_j$ で偏微分した時の式は、こんな風に書ける。ベクトルとスカラー値で、太字かそうでないかをちゃんと使い分けてるから、気をつけろよ。</p>

<p>$$
\frac{\partial E}{\partial \theta_j} = \sum_{i=1}^n (f_{\boldsymbol{\theta}} (\boldsymbol{x}^{(i)}) - y^{(i)}){x_j}^{(i)}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
なるほど。前から思ってたけど、やっぱりこの偏微分には規則性があったのかお。$j=0$ の時、つまり $\theta_0$ で偏微分する時も、後ろについてる $x_j$ は $x_0 = 1$ なので、結局は最初にやる夫が自分で微分した式と一致するお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
要するにパラメータの更新式は、こうだな。各 $\theta_j$ は同時更新する必要があるのは相変わらずだから気をつけろよ。</p>

<p>$$
\theta_j := \theta_j - \eta\sum_{i=1}^n (f_{\boldsymbol{\theta}} (\boldsymbol{x}^{(i)}) - y^{(i)}){x_j}^{(i)}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ミッション・コンプリートだお！！</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
な、今までの話が理解できていれば、素性が増えたとしても大したことは無いだろう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やる夫が優秀だからだお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
突然どこから出てくるんだよ、その自信。こんな風に素性を 2 つ以上使ったものを重回帰と言うんだ。逆に、最初にやった例のように素性が 1 つの場合は、単回帰と言うな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
単回帰に、多項式回帰に、重回帰。覚えたお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
回帰の話はこの辺で終わりだな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
最後のベクトル化から続く怒涛の一般化ラッシュ、気持よかったお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
一般化して考えられるのは、数学のいいところだな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ところで、これまでやってきた最急降下法、パラメータの更新にシグマが含まれてるから、全学習データ分だけループすることになるんだお。学習用データが大量にあった場合、for ループの回数が増えて、時間がかかってしまわないかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
さすがプログラミングが得意なだけあって、効率云々を気にするんだな。そうだ、やる夫の言うとおり、計算量が多くて遅いことが最急降下法の欠点の 1 つだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
まあ、性能の良いマシンを買えば解決するかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
金に物を言わせるなよ&hellip;、そこはプログラマっぽくないな、お前。ちゃんと、もっと速いアルゴリズムがあるんだよ。むしろ、最急降下法よりも、そっちの速いアルゴリズムの方が好んで使われるな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
じゃ、最初からそっちの速いアルゴリズムの方を教えてくれればよかったお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
速いアルゴリズムの方は最急降下法をベースにしてるんだよ。先に速いアルゴリズムを説明しても、どうせ最急降下法から説明することになるんだから、結局は同じことだ。それに、こういうことは基礎からちゃんとやっといたほうが、後で応用が聞くんだぞ？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うっ、やっぱりやらない夫には言いくるめられてしまうお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
お前、言い方&hellip;</p>

<h1 id="確率的勾配降下法">確率的勾配降下法</h1>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
最後に確率的勾配降下法というアルゴリズムを見ていこう。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
それが、さっき言った速いアルゴリズムのことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。最急降下法には、さっきやる夫がいった計算に時間がかかること以外にも、収束が遅かったり、それから局所解に捕まってしまう、というデメリットもある。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
局所解につかまるって、どういうことかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
最急降下法を説明する時に使った関数の例は $g(x) = (x - 1)^2$ という二次関数だったから問題なかったが、もっと複雑な形の関数を考えてみよう。この関数の最小値は、赤い点で示してある位置だな。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/sgd1.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ぐにゃぐにゃしてるグラフだお&hellip;</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
最急降下法で関数の最小値を見つけるにしても、まず最初に初期値を決める必要がある。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あー、確かにやったお。例の時は $x = 3$ からはじめたお。あれは、なんで 3 からスタートさせたのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
俺が適当に選んだ値だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫神に導かれるままについていきますお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうじゃなくて、初期値は適当に決めていいって意味だ。ただ、そのせいで局所解に捕まるという問題が発生するんだ。たとえば、この図の青い点が初期値だったとしたら、どうだ？</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/sgd2.png">
</div>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あー、なんとなくわかってきたお。その青い点が初期値だったら、最急降下法でちゃんと赤い点の最小値が求まるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
では、赤い点の最小値が求まらないのは、どういう場合だ？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
たとえば、今度はこっちの青い点が初期値だった場合、赤い点まで移動せずに、たぶんオレンジの点で止まってしまうお。</p>

<div style="text-align:center;">
<img src="/assets/img/yaruo/machine-learning/sgd3.png">
</div>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
それが局所解に捕まるということだ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
アルゴリズムがシンプルな分、いろいろな問題があるってことかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そこで、そういった最急降下法のデメリットを克服したアルゴリズムが、確率的勾配降下法だ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
ふむ。具体的にはどういうアルゴリズムなんだお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
最急降下法のパラメータ更新の式は覚えてるか？</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
覚えてるお。</p>

<p>$$
\theta_j := \theta_j - \eta\sum_{i=1}^n (f_{\boldsymbol{\theta}} (\boldsymbol{x}^{(i)}) - y^{(i)}){x_j}^{(i)}
$$</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
よし。その式では、パラメータの更新に全ての学習用データの誤差を使っているが、確率的勾配降下法ではランダムに学習用データを 1 つ選んで、それをパラメータの更新に使うんだ。式中の $k$ は、パラメータ更新毎にランダムに選ばれたインデックスだ。</p>

<p>$$
\theta_j := \theta_j - \eta(f_{\boldsymbol{\theta}} (\boldsymbol{x}^{(k)}) - y^{(k)}){x_j}^{(k)}
$$</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
お、シグマがなくなってるお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
そうだな。最急降下法で 1 回パラメータを更新する間に、確率的勾配降下法では $n$ 回パラメータを更新できる。また、学習用データをランダムに選んで、その時点での勾配でパラメータを更新していくので、目的関数の局所解に捕まりにくい。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
そんなに適当なやり方で、ちゃんとした答えが得られるのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
この方法でも、実際に最適解に収束するということがわかっている。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
うーん、なんか不思議だお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
もちろん、学習率 $\eta$ を適切に設定することは大事だ。特に、学習する度に $\eta$ を小さくしていく、という手法も存在する。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
そうだ、最急降下法の時も思ってたけど、$\eta$ の適切な値って、どうやって見つければいいのかお？</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
難しい問題だな。解決のためのアイデアは、興味があれば探してみるといい。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
やらない夫、実は知らないのかお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
これ以上続けると、話が長くなりすぎるからな。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
あっ、はぐらかしたお。読者のことを考えてるふりをするなんて、ずるいお。</p>

<p><img src="/assets/img/icon/yaranaio.gif" style="border-width:0;"> <strong>やらない夫</strong><br/>
まあ、そういうところは、実際に実装しはじめて困ったときに探すので十分だ。やり始める前から、なんでもかんでも一気に詰め込むのはよくないぞ。</p>

<p><img src="/assets/img/icon/yaruo.gif" style="border-width:0;"> <strong>やる夫</strong><br/>
はいはいですお。</p>

<hr />

<p><a href="/blog/2016/06/03/yaruo-machine-learning4/">やる夫で学ぶ機械学習 - パーセプトロン -</a> へ続く。</p>

    </article>
    <footer class="tag">
      <i class="fa fa-tag"></i>
      
        <a class="tag-link" href="tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a>
      
        <a class="tag-link" href="tags/%E3%82%84%E3%82%8B%E5%A4%AB%E3%81%A7%E5%AD%A6%E3%81%B6">やる夫で学ぶ</a>
      
    </footer>
    <ul class="snsb">
      <li>
        <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/" data-lang="ja">ツイート</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li>
        <div class="fb-like" data-href="http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/" data-layout="button_count" data-action="like" data-show-faces="false" data-share="false"></div>
      </li>
      <li>
        <a href="http://b.hatena.ne.jp/entry/http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
      </li>
    </ul>
  </main>
</div>

      <footer class="footer">
  <p class="copyright">copyright&nbsp;&copy;&nbsp;2015&nbsp;<a href="https://twitter.com/tkengo" target="_blank">&copy;tkengo</a>&nbsp;-&nbsp;Powered by <a href="http://gohugo.io" target="_blank">Hugo</a>, Designed by <a href="https://twitter.com/keita_kawamoto" target="_blank">&copy;keita_kawamoto</a></p>
  <ul class="social">
    <li><a href="https://twitter.com/tkengo" target="_blank"><i class="fa fa-twitter"></i></a></li>
    <li><a href="https://github.com/tkengo" target="_blank"><i class="fa fa-github"></i></a></li>
  </ul>
</footer>

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/ja_JP/sdk.js#xfbml=1&version=v2.4&appId=327080140754787";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-35368510-1', 'auto');
    ga('send', 'pageview');
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/languages/go.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

  </div>
  </body>
</html>
