<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width">
  <meta name="author" content="@tkengo">
  <meta http-equiv="X-UA-Compatible" CONTENT="IE=EmulateIE7" />
  
    <title>自然言語処理における畳み込みニューラルネットワークを理解する &middot; けんごのお屋敷</title>
  
  <link rel="stylesheet" href="http://tkengo.github.io/css/common.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="shortcut icon" href="http://tkengo.github.io/images/favicon.ico">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.7/styles/atelier-cave.light.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.7/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  
  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
  <style type="text/css">
    @font-face {
      font-family: mplus-1p-regular;
      src: url('http://mplus-fonts.sourceforge.jp/webfonts/mplus-1p-regular.ttf') format("truetype");
      font-family: mplus-1p-bold;
      src: url('http://mplus-fonts.sourceforge.jp/webfonts/mplus-1p-bold.ttf') format("truetype");
    }
  </style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]] },
      TeX: { extensions: ["color.js"] }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

  <body>
    <div class="container-bg">
      <div class="header">
  <header>
    <h1 class="logo"><a href="http://tkengo.github.io/">けんごのお屋敷</a></h1>
    <ul class="menu">
      <li><a href="/post">Archives</a></li>
    </ul>
  </header>
</div>

      <div class="container">
  <main>
    <header>
      <p class="day">2016-03-11</p>
      <h1><a href="http://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/">自然言語処理における畳み込みニューラルネットワークを理解する</a></h1>
    </header>
    <article>
      

<p>最近、畳み込みニューラルネットワークを使ったテキスト分類の実験をしていて、知見が溜まってきたのでそれについて何か記事を書こうと思っていた時に、こんな記事をみつけました。</p>

<p><a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp</a></p>

<p>畳み込みニューラルネットワークを自然言語処理に適用する話なのですが、この記事、個人的にわかりやすいなと思ったので、著者に許可をもらって日本語に翻訳しました。なお、この記事を読むにあたっては、ニューラルネットワークに関する基礎知識程度は必要かと思われます。</p>

<p><em>※日本語としてよりわかりやすく自然になるように、原文を直訳していない箇所もいくつかありますのでご了承ください。翻訳の致命的なミスなどありましたら、Twitterなどで指摘いただければすみやかに修正します。</em></p>

<hr />

<h4 id="以下訳文">以下訳文</h4>

<p>畳み込みニューラルネットワーク (CNN) という言葉を聞いた時、普通はコンピュータービジョンのことを思い浮かべるでしょう。CNN は過去に画像分類の分野においてブレークスルーを引き起こし、今日では Facebook の写真の自動タギングから自動運転車に至るまで、ほとんどのコンピュータービジョンシステムの中核となっています。</p>

<p>そして近年、自然言語処理 (NLP) の領域の問題に対しても CNN が適用されはじめ、いくつか興味深い結果を得ています。この記事では CNN とはいったい何なのかということ、またどのようにして NLP の領域で使われるのか、ということを説明してみたいと思います。コンピュータービジョンでのユースケースを話した方が CNN においてはいくぶん直感的ですので、まずはそこから始めたいと思います。そしてゆっくりと NLP の話題へ移っていきましょう。</p>

<h1 id="畳み込みとは">畳み込みとは？</h1>

<p>私は、畳み込みについては、行列に適用されるスライド窓関数 (sliding window function) として考えるとわかりやすいと思います。言葉でこう書くとちょっと難しいかもしれませんが、視覚的に表してみると非常にわかりやすいです。</p>

<div style="text-align:center;">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/convolution_schematic.gif">
<br/>
<p style="text-align:center;font-size:13px;color:#828282;">
3x3 の畳み込みフィルタ。引用元:<br/>
<a href="http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution">http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution</a>
</p>
</div>

<p>左側にある行列は白黒の画像を表していると考えてください。行列の各要素はそれぞれ画像の 1 つのピクセルに対応しており、0 が黒、1 が白です (一般的には 0 から 255 の間の値を取るグレースケールの画像)。スライド窓はカーネル (kernel) やフィルタ (filter) または特徴検出器 (feature detector) などと呼ばれます。ここでは 3x3 のフィルタを使っており、そのフィルタの値と行列の値を要素毎に掛けあわせ、それらの値を合計します。この操作を、行列全体をカバーするようにフィルタをスライドさせながら各要素に対して行っていき、全体の畳み込みを取得します。</p>

<p>でも結局これで何が出来るの？と不思議に思いませんか。ここで直感的な例を挙げてみましょう。</p>

<h4 id="各ピクセルとその周囲を平均して画像をぼかす">各ピクセルとその周囲を平均して画像をぼかす:</h4>

<div style="text-align:center;">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/convolution-blur.png">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/generic-taj-convmatrix-blur.jpg">
</div>

<p><br/></p>

<h4 id="各ピクセルとその周囲の差分をとってエッジを検出する">各ピクセルとその周囲の差分をとってエッジを検出する:</h4>

<p>(これを直感的に理解するためには、ピクセルの色が周囲の色と同じであるような色の変化がなめらかな部分で、どういうことが起こるのかを考えてみましょう。そういった部分にこのフィルタを適用すると、可算が相殺されて結果的には 0、つまり黒になります。逆に、明度が極端に違うエッジの部分であれば - これはたとえば白から黒へ移り変わっている部分 - においては、差分が大きくなり結果的には白くなります。)</p>

<div style="text-align:center;">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/convolution-edge-detect1.png">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/generic-taj-convmatrix-edge-detect.jpg">
</div>

<p><br/></p>

<p><a href="http://docs.gimp.org/en/plug-in-convmatrix.html">GIMP のマニュアル</a> には、ここで紹介した以外のサンプルがいくつか含まれています。畳み込みについてより詳しく理解したければ <a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/">Chris Olah の記事</a> も読んでみることをオススメします。</p>

<h1 id="畳み込みニューラルネットワークとは">畳み込みニューラルネットワークとは？</h1>

<p>さて、これで畳み込みの正体はわかりました。しかし、CNN とは一体なんでしょう？CNN は、<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> や <a href="https://reference.wolfram.com/language/ref/Tanh.html">tanh</a> のような非線形な活性化関数を通した、いくつかの畳み込みの層のことです。伝統的な順伝搬型ニューラルネットワークでは、それぞれの入力ニューロンは次の層のニューロンにそれぞれ接続されており、これは全結合層やアフィン層とも呼ばれます。しかし CNN ではそのようなことはせずに、ニューロンの出力を計算するのに畳み込みを使います。これによって、入力となるニューロンのある領域が、それぞれ対応する出力のニューロンに接続されているような、局所的な接続ができることになります。各層は別々の異なるフィルタを適用し - これは一般的には 100 〜 1000 程度の数になりますが - それらを結合します。これをプーリング層 (subsampling) と呼びます。これについては後ほど詳しくお話します。CNN の学習フェーズでは、解決したいタスクに適応できるように <strong>フィルタの値を自動的に学習していきます</strong>。たとえば画像分類の話でいうと、CNN は最初の層で生のピクセルデータからエッジを検出するための学習を進め、そのエッジを使って今度は次の層で単純な形状を検出し、さらにより深い層ではその形状を使ってより高レベルな特徴、つまり顔の形状などの特徴を検出するようになります。そして最後の層は、そういった高レベルな特徴を使った分類器となります。</p>

<div style="width:70%;margin:0 auto;">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/convolutional-neural-network-overview.png">
</div>

<p>さて、畳み込み計算には注目に値すべき点が 2 つあります。<strong>位置不変性 (Location Invariance)</strong> と <strong>構成性 (Compositionality)</strong> です。画像の中に象が写っているかどうかを判別したいとしましょう。畳み込み層では画像全体にわたってフィルタをスライドさせていくので、画像中のどこに象が現れるのかを気にしなくてもよいのです。またプーリング層でも、平行移動、回転、スケーリングに対して不変性を得ることができますが、これは後述します。そしてもう 1 つの鍵は構成性です。各フィルタは、低レベルな特徴である画像の一区画から、より高レベルな特徴を表現できるようにしてくれます。これがコンピュータービジョンにおいて CNN が非常に強力である理由です。ピクセルからエッジを、エッジから形状を、そしてその形状からより複雑なオブジェクトを構築する、という流れは直感的に理にかなっています。</p>

<h2 id="これらをどうやって-nlp-へ適用するのか">これらをどうやって NLP へ適用するのか？</h2>

<p>画像分類では入力は画像のピクセル列になりますが、ほとんどの NLP タスクではピクセル列の代わりに、行列で表現された文章または文書が入力となります。行列の各行は 1 つのトークンに対応しており、一般的には単語がトークンになることが多いですが、文字がトークンでもかまいません。すなわち、各行は単語を表現するベクトルです。普通、これらのベクトルは <a href="https://code.google.com/p/word2vec/">word2vec</a> や <a href="http://nlp.stanford.edu/projects/glove/">GloVe</a> のような低次元な単語埋め込み表現 (word embeddings) を使いますが、one-hot ベクトルでもかまいません。100 次元の単語埋め込みを使った 10 単語の文章があった場合、10x100 の行列となります。これが NLP における &ldquo;画像&rdquo; です。</p>

<p>コンピュータービジョンでは、フィルタは画像のある区画上をスライドしていきますが、NLP では一般的に行列の行全体 (つまり単語毎) をスライドするフィルタを使います。つまり、フィルタの幅は入力となる行列の幅と同じにします。高さは様々ですが、一般的には 2-5 くらいの単語くらいでしょうか。これらのことを加味すると NLP の畳み込みニューラルネットワークはこんな感じになります (少し時間をかけてこの図を見て、どんな風に計算されていくのかを理解してみてください。プーリングについては今のところは無視してください、後でちゃんと説明します)。</p>

<div style="width:70%;margin:0 auto;">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/convolutional-neural-network-for-nlp-without-pooling.png">
</div>

<p style="font-size:13px;color:#828282;">
文章分類のための畳み込みニューラルネットワーク (CNN) のアーキテクチャを説明した図。この図には 2、3、4 の高さをもったフィルタが、それぞれ 2 つずつあります。各フィルタは文章の行列上で畳み込みを行い、特徴マップを生成します。それから、各特徴マップに対して最大プーリングをかけていき、各特徴マップの中で一番大きい値を記録していきます。そして、全 6 つの特徴マップから単変量な特徴 (univariate feature) が生成され、それら 6 つの特徴は結合されて、それが最後から 2 番目の層になります。一番最後の softmax 層では先程の特徴を入力として受け取り、文章を分類します。ここでは二値分類を前提としていますので、最終的には 2 つの出力があります。<br/>
引用元: hang, Y., & Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification
</p>

<p>位置不変性と構成性は、画像においては直感的にわかりますが、NLP の場合はそうではありません。NLP ではたぶん、文章内で単語が出現する場所なんかを気にするんじゃないでしょうか。お互いに近くにあるピクセル同士は意味的に関連している、つまりは同じオブジェクトであると言えると思いますが、単語においてそれは常にそうとは限りません。多くの言語において、フレーズの一部は単語によって区切ることができます。こういった文章の組成は明確なものではありません。単語のより高レベルな表現 (たとえば実際の単語の &ldquo;意味&rdquo; など) はピュータービジョンほど明らかではなく、一体この NLP に対する CNN はどうやって動作しているのでしょうか。</p>

<p>こうなると、CNN は NLP のタスクにはうまく適合しないんじゃないかという気がしてきます。<a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">リカレントニューラルネットワーク</a> はもっと直感的ですし、実際に人間が言語を処理するやり方に似ています。左から右に順番に読んでいく方法ですね。とは言っても、これまでの説明は決して CNN がうまく動かないという意味ではありません。<a href="https://en.wikipedia.org/wiki/All_models_are_wrong">完璧なモデルなど無いが、それでも役に立つモデルはある</a> という言葉がありますが、NLP の問題へ適用された CNN は、結果的にはとてもよい性能を発揮します。単純な <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag of Words モデル</a> は誤った仮定のもと単純化しすぎなのは明らかですが、にもかかわらずしばらくは一般的なアプローチであり、人々はそれを使ってより良い結果を得ようとしてきました。</p>

<p>CNN を使う言い分としては、とても速いということです。非常に速いです。畳み込みはコンピューターグラフィックスにおいて重要なものであり GPU 上にハードウェアレベルで実装されています。<a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a> のようなものと比べて CNN は効率的に単語を表現ができます。ボキャブラリーが巨大な場合、3-grams 以上のものは計算量が多すぎてすぐに計算できなくなります。Google でさえ 5-grams を超えるものは提供していません。畳み込みフィルタはボキャブラリー全体を表現する必要なく、勝手に適切な表現を学習してくれます。最初の層にあるたくさんの学習済みのフィルタは n-grams と非常に良く似た特徴を捉えますが、よりコンパクトな表現を得ることができると考えると良いでしょう (しかも計算量が多すぎて計算ができないといった制限もありません)。</p>

<h1 id="cnn-のハイパーパラメータ">CNN のハイパーパラメータ</h1>

<p>CNN を NLP へ適用するやり方を説明する前に、CNN を構築する際に必要となってくるものがいくつかありますのでそれを見ていきましょう。きっと CNN 理解の手助けとなるはずです。</p>

<h2 id="畳み込み幅のサイズ">畳み込み幅のサイズ</h2>

<p>私が最初に畳み込みの説明をした時、フィルタを適用する際の詳細について説明を飛ばしたものがあります。行列の真ん中辺りに 3x3 のフィルタを適用するのは問題ありませんが、それではフチの辺りに適用する場合はどうでしょうか？行列の左側にも上側にも隣接した要素がないような、たとえば行列の最初の要素にはどうやってフィルタを適用すればよいでしょうか？そういった場合には、ゼロパディングが使えます。行列の外側にはみ出してしまう要素は全て 0 で埋めるのです。こうすることで、入力となる行列の全要素にわたってフィルタを適用することができます。ゼロパディングを行うことは wide convolution とも呼ばれ、逆にゼロパディングをしない場合は narrow convolution と呼ばれます。1 次元での例を見てみましょう。</p>

<div style="width:70%;margin:0 auto;">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/narrow_vs_wide_convolution.png">
</div>

<p style="text-align:center;font-size:13px;color:#828282;">
Narrow Convolution と Wid Convolution。フィルタのサイズは 5 で、入力データのサイズは 7。引用元: A Convolutional Neural Network for Modelling Sentences (2014)
</p>

<p>入力データのサイズに対してフィルタサイズが大きい時には wide convolution が有用です。narrow convolution は出力されるサイズが $(7 - 5) + 1 = 3$ になりますし、wide convolutin は $(7 + 2 * 4 - 5) + 1 = 11$ になります。一般化すると、出力サイズは $n_{out}=(n_{in} + 2*n_{padding} - n_{filter}) + 1$ です。</p>

<h2 id="ストライド">ストライド</h2>

<p>ストライドという畳み込みのもう一つのハイパーパラメータがあります。これはフィルタを順に適用していく際に、フィルタをどれくらいシフトするのかという値です。これまでに示してきた例は全てストライド 1 で、フィルタは重複しながら連続的に適用されています。ストライドを大きくするとフィルタの適用回数は少なくなって、出力のサイズも小さくなります。以下のような図が <a href="http://cs231n.github.io/convolutional-networks/">Stanford cs231</a> にありますが、これは 1 次元の入力に対して、ストライドのサイズが 1 または 2 のフィルタを適用している様子です。</p>

<div style="width:70%;margin:0 auto;">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/stride.png">
</div>

<p style="text-align:center;font-size:13px;color:#828282;">
畳み込みのストライドのサイズ。左側のストライドは 1。右側のストライドは 2。引用元: http://cs231n.github.io/convolutional-networks/
</p>

<p>普通、文書においてはストライドのサイズは 1 ですが、ストライドのサイズを大きくすることで、例えばツリーのような <a href="https://en.wikipedia.org/wiki/Recursive_neural_network">再帰型ニューラルネットワーク</a> と似た挙動を示すモデルを作れるかもしれません。</p>

<h2 id="プーリング層">プーリング層</h2>

<p>畳み込みニューラルネットワークの鍵は、畳み込み層の後に適用されるプーリング層です。プーリング層は、入力をサブサンプリングします。最も良く使われるプーリングは、各フィルタの結果の中から最大値を得る操作です。ただ、畳み込み結果の行列全体にわたってプーリングする必要はなく、指定サイズのウィンドウ上でプーリングすることもできます。たとえば、以下の図は 2x2 のサイズのウィンドウ上で最大プーリングを実行した様子です (NLP では一般的に出力全体にわたってプーリングを適用します。つまり各フィルタからは 1 つの数値が出力されることになります)。</p>

<div style="width:70%;margin:0 auto;">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/max-pooling.png">
</div>

<p style="text-align:center;font-size:13px;color:#828282;">
CNN における最大プーリング。引用元: http://cs231n.github.io/convolutional-networks/#pool
</p>

<p>プーリング層をはさむ理由はいくつかあります。プーリングの特徴の 1 つは、出力される行列が固定サイズになるということです。たとえば 1000 個のフィルタがあってそれぞれのフィルタに対して最大プーリングを適用したとすると、入力のサイズやフィルタのサイズがどんなものであっても結果としては 1000 次元の出力が得られますね。これはつまり、文章のサイズやフィルタのサイズが可変だったとしても、最終的に分類器へデータが渡ってくる時点では常に同じ次元になっているということです。</p>

<p>また、プーリングは次元削減も行いますが、単に次元を削減するのではなく必要な情報は維持したまま次元を削減してくれます。フィルタをある特定の特徴を抽出するためのものとして考えるのです。たとえば &ldquo;not amazing&rdquo; などの否定が文章内に含まれているかどうかを検出するためのもの、という感じです。もしこんなフレーズが文章のどこかにでてきた場合、その部分にフィルタを適用すると、出力される計算結果は大きな値になるでしょう。しかし、それ以外の別の部分にフィルタを適用した場合は、出力結果は小さな値になるでしょう。最大プーリングを適用することで、文章中に &ldquo;とある特徴&rdquo; が存在するかどうかという情報は残ったままですが、その特徴が実際にどこに出現するのかといった情報は失われることになります。しかし、このような位置に関する情報は本当に消えても大丈夫なのでしょうか？答えは yes です。n-grams モデルも似たようなものです。位置に関する情報は失いますが、フィルタによって捉えられた局所的な情報 - たとえば &ldquo;not amazing&rdquo; と &ldquo;amazing not&rdquo; の違いなど - は残ったままなのです。</p>

<p>画像認識での話ですが、プーリングは位置と回転に不変性を与えます。ある領域についてプーリングを行うと、画像が数ピクセルだけ移動や回転をしてもその出力はほぼ同じになります。それは最大プーリングが、微妙なピクセルの違いを無視して同じ値を抽出してきてくれるからです。</p>

<h2 id="チャンネル">チャンネル</h2>

<p>最後はチャンネルです。チャンネルとは、入力データを異なる視点から見たものと言えるでしょう。画像認識での例を挙げると、普通は画像は RGB (red, green, blue) の 3 チャンネルを持っています。畳み込みはこれらのチャンネル全体に適用でき、その時のフィルタは各チャンネル毎に別々に用意してもいいですし、同じものを使ってもかまいません。NLP では、異なる単語埋め込み表現 (<a href="https://code.google.com/p/word2vec/">word2vec</a> や <a href="http://nlp.stanford.edu/projects/glove/">GloVe</a> など) でチャンネルを分けたり、同じ文章を異なる言語で表現してみたり、また異なるフレーズで表現してみたり、という風にして複数チャンネルを持たせることができそうですね。</p>

<h1 id="nlp-へ適用された畳み込みニューラルネットワーク">NLP へ適用された畳み込みニューラルネットワーク</h1>

<p>それでは、自然言語処理に対して CNN を使ったアプリケーションの研究結果について見ていきましょう。CNN のアプリケーションについては私が知らないものもたくさんありますが、少なくとも有名な研究結果についてはカバーできていると思います。</p>

<p>CNN が得意なのは、感情分析 (Sentiment Analysis) やスパム検出 (Spam Detection)、カテゴリ分類 (Topic Categorization) などの分類問題です。畳み込みとプーリングの操作を適用すると、単語の局所的な位置情報は失われますので、品詞タグ付け (PoS Tagging) や固有表現抽出 (Entity Extraction) などを目的として、純粋な CNN を使うのはちょっと難しいでしょう (ただ、入力データに位置情報に関する特徴を追加すれば不可能ではないと思います)。</p>

<p>[1] では、主に感情分析とカテゴリ分類から成る様々なデータセット上で CNN のアーキテクチャを評価しています。CNN は全体的にとても良いパフォーマンスを発揮しています。この論文で使われているネットワークは非常にシンプルなのに、とても強力なので驚きです。入力層は <a href="https://code.google.com/p/word2vec/">word2vec</a> による単語埋め込み表現で構成された文章で、その後に複数のフィルタを持つ畳み込み層と最大プーリング層が続き、そして最後に softmax 分類器があります。この論文では、一方は学習中に微修正されていく動的な単語埋め込み表現、もう一方は学習中に変化しない静的な単語埋め込み表現、といった 2 つの異なるチャンネルを持つデータに対する実験もしています。似てはいますが、もう少し複雑なアーキテクチャが [2] で提案されています。[6] では &ldquo;semantic clustering&rdquo; と呼ばれる操作を行う層をネットワークに追加しています。</p>

<div style="width:70%;margin:0 auto;">
<img src="/assets/img/understanding-convolutional-neural-networks-for-nlp/cnn-for-sentence-classification.png">
</div>

<p style="text-align:center;font-size:13px;color:#828282;">
Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification
</p>

<p>[4] では、word2vec や GloVe などを使った単語ベクトルの事前学習はせずに、one-hot なベクトルに対して直接畳み込みを適用して、スクラッチで CNN を学習させています。著者はまた、空間効率の良い (space-efficient) BoW のような表現を提案しており、ネットワークが学習するパラメータ数を減らしています。[5] では、CNN を使ってテキストのある領域のコンテキストを予測する &ldquo;region embedding&rdquo; と呼ばれる教師なしのモデルを拡張しています。これらの論文のアプローチは長いテキスト (たとえば映画のレビューのような) に対してはうまく動作しているように見えますが、短いテキスト (ツイートなど) に対してはそうではありません。短いテキストに対しては、単語埋め込み表現を事前学習しておいた方がより良い結果になりそうですね。</p>

<p>CNN を実装するためには、いろいろなハイパーパラメータを決める必要があります。いくつかは先ほど紹介しましたが、入力データのベクトル表現 (word2vec なのか GloVe なのか one-hot なのか)、その数、畳み込みフィルタのサイズ、プーリングの方法 (最大プーリングなのか平均プーリングなのか)、活性化関数 (ReLU なのか tanh なのか)、などです。[7] では CNN のハイパーパラメータを様々に変化させながら、その時の CNN のパフォーマンスと複数回実行した際の分散を調査し、評価しています。もし、あなたが自分でテキスト分類問題を解くための CNN を実装するつもりであれば、この論文の結果を初期値として使うのが良いでしょう。この論文によると、平均プーリングより最大プーリングの方が毎回いい結果を出しており、理想的なフィルタサイズを考えるのはとても重要ですがそれはタスク毎に異なっています。それから、正則化項を導入しても NLP においては結果が大きく変わることはないようです。1 つ注意点としては、この研究で使っているデータセットはどれもテキストの長さが非常に似ているものばかりですので、テキスト長が明らかに異なるようなデータに対しては同じように考えることは恐らくできないでしょう。</p>

<p>[8] では関係抽出 (Relation Extraction) と関係分類 (Relation Classification) に対して CNN の研究をしています。単語ベクトルに関して、興味のあるエンティティに対する単語の相対位置を畳み込み層への入力として使っています。このモデルは、エンティティの位置はあらかじめ与えられているものとして、各サンプルデータはそれぞれ 1 つの関連を含んでいます。[9] と [10] でも似たようなモデルを使っています。</p>

<p>他の面白そうなユースケースとしては Microsoft Research から発表された [11] と [12] があります。これらの論文は、情報検索システムで使われる文章の &ldquo;意味に関する有用な表現&rdquo; をどうやって学習するのかを説明しています。ユーザーが現在読んでいるテキストをベースとして、ユーザーが潜在的に興味があるであろう文書をオススメする手法が例として含まれています。この文書表現は検索エンジンのログデータを元に学習されます。</p>

<p>ほとんどの CNN アーキテクチャは、学習プロセスの一部として単語及び文章に対しての埋め込み表現 (より低次元な表現) を学習します。すべての論文がこのやり方に沿っているわけではありませんし、有用な埋め込み表現をどのように学習するのかという調査をしているわけでもありません。[13] では、単語や文章に対する埋め込み表現を生成しながら Facebook への投稿のハッシュタグを予測する CNN を紹介しています。こういった学習済みの埋め込み表現は、他のタスクに適用してもうまくいきます。</p>

<h2 id="キャラクターレベル-cnn">キャラクターレベル CNN</h2>

<p>ここまでは、どのモデルも単語をベースにしたものでした。しかし文字に対して直接 CNN を適用する研究も続けられてきました。[14] では文字レベル (character-level) で埋め込み表現を学習し、それらを事前学習した単語埋め込み表現と結合し、品詞タグ付けをするために CNN を使っています。[15] と [16] では、事前学習された埋め込み表現を使わずに、文字から直接学習する CNN を説明しています。とりわけ著者は、全部で 9 個の層を持つ比較的深いネットワークを感情分析やテキスト分類に適用しています。大規模なデータセット (100万件程度のデータ) を使えば、文字レベルの入力を直接学習することで良い結果が得られることがわかっていますが、小規模なデータセット (数千件程度のデータ) しかない場合は単純なモデルにも負けてしまいます。[17] では文字レベルの CNN の出力を LSTM への入力として使うようなアプリケーションに対する説明をしています。これは様々な言語に適用できます。</p>

<p>驚くべきは、これらの論文は全て過去 1-2 年の間で公開された論文だということです。NLP に CNN を適用する以前から、自然言語処理界隈では CNN を使わずに <a href="http://arxiv.org/abs/1103.0398">スクラッチで</a> 素晴らしい成果を出してきましたが、最近の新しい結果や最新のシステムが公開されるスピードは加速し続けていますね。</p>

<h1 id="参考論文">参考論文</h1>

<ul>
<li>[1] <a href="http://arxiv.org/abs/1408.5882">Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 1746–1751.</a></li>
<li>[2] <a href="http://arxiv.org/abs/1404.2188">Kalchbrenner, N., Grefenstette, E., &amp; Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences. Acl, 655–665.</a></li>
<li>[3] <a href="http://www.aclweb.org/anthology/C14-1008">Santos, C. N. dos, &amp; Gatti, M. (2014). Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. In COLING-2014 (pp. 69–78).</a></li>
<li>[4] <a href="http://arxiv.org/abs/1412.1058v1">Johnson, R., &amp; Zhang, T. (2015). Effective Use of Word Order for Text Categorization with Convolutional Neural Networks. To Appear: NAACL-2015, (2011).</a></li>
<li>[5] <a href="http://arxiv.org/abs/1504.01255">Johnson, R., &amp; Zhang, T. (2015). Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding.</a></li>
<li>[6] <a href="http://www.aclweb.org/anthology/P15-2058">Wang, P., Xu, J., Xu, B., Liu, C., Zhang, H., Wang, F., &amp; Hao, H. (2015). Semantic Clustering and Convolutional Neural Network for Short Text Categorization. Proceedings ACL 2015, 352–357.</a></li>
<li>[7] <a href="http://arxiv.org/abs/1510.03820">Zhang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification,</a></li>
<li>[8] <a href="http://www.cs.nyu.edu/~thien/pubs/vector15.pdf">Nguyen, T. H., &amp; Grishman, R. (2015). Relation Extraction: Perspective from Convolutional Neural Networks. Workshop on Vector Modeling for NLP, 39–48.</a></li>
<li>[9] <a href="http://ijcai.org/papers15/Papers/IJCAI15-192.pdf">Sun, Y., Lin, L., Tang, D., Yang, N., Ji, Z., &amp; Wang, X. (2015). Modeling Mention , Context and Entity with Neural Networks for Entity Disambiguation, (Ijcai), 1333–1339.</a></li>
<li>[10] <a href="http://www.aclweb.org/anthology/C14-1220">Zeng, D., Liu, K., Lai, S., Zhou, G., &amp; Zhao, J. (2014). Relation Classification via Convolutional Deep Neural Network. Coling, (2011), 2335–2344.</a></li>
<li>[11] <a href="http://research.microsoft.com/pubs/226584/604_Paper.pdf">Gao, J., Pantel, P., Gamon, M., He, X., &amp; Deng, L. (2014). Modeling Interestingness with Deep Neural Networks.</a></li>
<li>[12] <a href="http://research.microsoft.com/pubs/226585/cikm2014_cdssm_final.pdf">Shen, Y., He, X., Gao, J., Deng, L., &amp; Mesnil, G. (2014). A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval. Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management – CIKM ’14, 101–110. </a></li>
<li>[13] <a href="http://emnlp2014.org/papers/pdf/EMNLP2014194.pdf">Weston, J., &amp; Adams, K. (2014). # T AG S PACE : Semantic Embeddings from Hashtags, 1822–1827.</a></li>
<li>[14] <a href="http://jmlr.org/proceedings/papers/v32/santos14.pdf">Santos, C., &amp; Zadrozny, B. (2014). Learning Character-level Representations for Part-of-Speech Tagging. Proceedings of the 31st International Conference on Machine Learning, ICML-14(2011), 1818–1826. </a></li>
<li>[15] <a href="http://arxiv.org/abs/1509.01626">Zhang, X., Zhao, J., &amp; LeCun, Y. (2015). Character-level Convolutional Networks for Text Classification, 1–9.</a></li>
<li>[16] <a href="http://arxiv.org/abs/1502.01710">Zhang, X., &amp; LeCun, Y. (2015). Text Understanding from Scratch. arXiv E-Prints, 3, 011102.</a></li>
<li>[17] <a href="http://arxiv.org/abs/1508.06615">Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2015). Character-Aware Neural Language Models.</a></li>
</ul>

    </article>
    <footer class="tag">
      <i class="fa fa-tag"></i>
      
        <a class="tag-link" href="tags/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF">ニューラルネットワーク</a>
      
        <a class="tag-link" href="tags/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86">自然言語処理</a>
      
    </footer>
    <ul class="snsb">
      <li>
        <a href="https://twitter.com/share" class="twitter-share-button" data-url="http://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/" data-lang="ja">ツイート</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li>
        <div class="fb-like" data-href="http://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/" data-layout="button_count" data-action="like" data-show-faces="false" data-share="false"></div>
      </li>
      <li>
        <a href="http://b.hatena.ne.jp/entry/http://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
      </li>
    </ul>
  </main>
</div>

      <footer class="footer">
  <p class="copyright">copyright&nbsp;&copy;&nbsp;2015&nbsp;<a href="https://twitter.com/tkengo" target="_blank">&copy;tkengo</a>&nbsp;-&nbsp;Powered by <a href="http://gohugo.io" target="_blank">Hugo</a>, Designed by <a href="https://twitter.com/keita_kawamoto" target="_blank">&copy;keita_kawamoto</a></p>
  <ul class="social">
    <li><a href="https://twitter.com/tkengo" target="_blank"><i class="fa fa-twitter"></i></a></li>
    <li><a href="https://github.com/tkengo" target="_blank"><i class="fa fa-github"></i></a></li>
  </ul>
</footer>

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/ja_JP/sdk.js#xfbml=1&version=v2.4&appId=327080140754787";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-35368510-1', 'auto');
    ga('send', 'pageview');
</script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/languages/go.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

  </div>
  </body>
</html>
