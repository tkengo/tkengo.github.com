<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>機械学習 on けんごのお屋敷</title>
    <link>http://tkengo.github.io/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/</link>
    <description>Recent content in 機械学習 on けんごのお屋敷</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Wed, 06 Sep 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://tkengo.github.io/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>「やさしく学ぶ 機械学習を理解するための数学のきほん」を執筆しました</title>
      <link>http://tkengo.github.io/blog/2017/09/06/ml-math/</link>
      <pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2017/09/06/ml-math/</guid>
      <description>私が執筆した書籍 やさしく学ぶ 機械学習を理解するための数学のきほん がマイナビ出版から Amazon で 2017/9/20 より発売されます。Amazon 上では既に予約可能になっていますので、興味のある方は是非とも手に取ってみてください。
  本書は、以前よりこのブログ内で公開していた「やる夫で学ぶ機械学習シリーズ」というシリーズ物の記事をベースとして、加筆・修正を加えたものになります。ブログの記事がベースになってはいますが、追加で書いた分の方が多く、お金を出して買ってもらえるクオリティにするために、より丁寧な説明を心がけて書きました。元記事は「やる夫」と「やらない夫」というキャラクターを登場人物として、機械学習の基礎を面白おかしく丁寧に解説していくものでしたが、書籍化するに当たって「やる夫」と「やらない夫」をそのまま使うわけにもいかなかったので、プログラマの「アヤノ」とその友達で機械学習に詳しい「ミオ」というキャラクターを新しく作って、彼女らの会話を通して一緒に勉強していく形を取っています。この本は以下のような構成になっていて、Chapter 1 〜 5 が本編で、Appendix で必要な数学知識の補足をしています。
 Chapter 1 - ふたりの旅の始まり Chapter 2 - 回帰について学ぼう Chapter 3 - 分類について学ぼう Chapter 4 - 評価してみよう Chapter 5 - 実装してみよう Appendix (本編に入り切らなかった数学知識の解説)  会話形式なので、全体的な文量もそこまで多くなくスラスラ読んでいけると思います。ただし、本書は機械学習の数学に関する入門書という位置付けなので、プログラミング自体の入門解説だったり、いろいろなアルゴリズムの紹介だったりは、内容にありません。Chapter 2 及び Chapter 3 あたりが、いま公開されているブログ記事ベースになっていますので、いきなり本を買うのが嫌な方や試し読みをしてみたい方などは、まず やる夫で学ぶ機械学習シリーズ を読んでみると、感覚をつかんでいただけるのではないかなと思います。また、マイナビ出版の Facebook には 本書のサンプルページ が掲載されていますので、そちらを見ても雰囲気はつかんでいただけるかなと思います。
執筆にかけた想い 書籍の宣伝はこの辺にして、ここからは個人的な想いを書いたポエムですので、ご了承ください。
- 数学や機械学習を通じて、気軽に話したりお互いに刺激し合える仲間が欲しい -
始まりは、ただ、それだけでした。
私がいわゆる機械学習と言われる領域に足を踏み入れた頃は、いまほど界隈は賑わっていませんでした。元々は大学時代からコンピュータービジョンや 3D に興味があり、それらに関するアルゴリズムや実装を探しているうちに機械学習に興味をもって勉強し始めましたが、もちろん当時は参考にできる Web サイトも書籍も、今よりも少ない状態です。仕事として機械学習をやっているわけでもなく、手取り足取り教えてくれるような詳しい人や、機械学習に興味を持っていそうな人は周りにはおらず、毎日ひとりでアテもなく先の見えない道を進むばかりだったのを覚えています。一人きりで何かを続けていくことほど、モチベーションが長続きしないことはありません。
そんな状況でしたのでちょっとだけ勉強には苦労していたのですが、学生時代から数学が好きだったこともあり (得意かどうかは置いといて) 数学に対する抵抗はまったくありませんでした。むしろ数学をやっている時間は一人でも楽しくて、没頭することができました。そうして勉強しながらしばらく時が経ち、世の中が機械学習やディープラーニングで盛り上がりを見せ始めたのをきっかけに、自分のブログでもぽつぽつと機械学習に関する情報を発信するようになっていきます。また、数学好きが高じて プログラマのための数学勉強会@福岡 というイベントも主催していました。そうやってアウトプットを続けていれば、きっと仲間が集って、いつかみんなでワイワイできるようになるんだ、と信じて。</description>
    </item>
    
    <item>
      <title>機械学習でパラメータ推定 - 最尤推定法 -</title>
      <link>http://tkengo.github.io/blog/2016/08/22/maximum-likelyhood-estimation-by-machine-learning/</link>
      <pubDate>Mon, 22 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/08/22/maximum-likelyhood-estimation-by-machine-learning/</guid>
      <description>最尤推定法 (Maximum Likelyhood や Maximum Likelyhood Estimation と言われ、それぞれ頭文字を取って ML や MLE などとも言われる) は機械学習やコンピュータービジョンなどの分野で良く使われる推定法で、次のような条件付き同時確率を最大化することでパラメータの推定を行います。
$$ \hat{\theta} = \mathop{\mathrm{argmax}}\limits_{\theta} \mathrm{P}(x_1, x_2, \cdots, x_N|\theta) $$
これだけ見て「うん、アレね」と理解できる人はこの記事の対象読者ではなさそうですので、逆にいろいろ教えて下さい。この記事では理論の面から最尤推定法にアタックしてみます。数式成分が多めで、うっとなることもあるかもしれませんが、ゆっくり読んでいきましょう。
※この記事を読むにあたっては、確率論と微分の基礎知識程度は必要です。
尤度 いきなり応用問題から始めると必ず挫折するので、まずは一番簡単な問題設定から始めます。Wikipedia に 最尤推定 のページがありますので、この中で使われている例を参考に話を進めていきましょう。
見た目がまったく同じ 3 枚のコイン A, B, C があります。これらのコインはイカサマで、投げた時に表の出る確率がそれぞれ違います。
 コイン A は 1&amp;frasl;3 の確率で表が出る コイン B は 1&amp;frasl;2 の確率で表が出る コイン C は 2&amp;frasl;3 の確率で表が出る  この 3 枚のコインを箱の中に入れてシャッフルした後に 1 枚引きます。引いたコインを 10 回投げたら、表が 3 回、裏が 7 回出ました。あなたは A, B, C のどのコインを引いたでしょうか？
この問題設定は極めて単純です。単純すぎて最尤推定法を使わなくても解けるくらい簡単ですが、ここでは敢えて最尤推定法を使って解いてみます。最尤推定法は次の条件付き同時確率を最大化するパラメータ $\hat{\theta}$ を求めることでした。</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - 対数尤度関数 -</title>
      <link>http://tkengo.github.io/blog/2016/06/16/yaruo-machine-learning6/</link>
      <pubDate>Thu, 16 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/16/yaruo-machine-learning6/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 6 回です。ロジスティック回帰の目的関数である尤度関数をもう少し詳しくみて、線形分離不可能な問題にどのように適用していくのかを見ていきます。
第 5 回はこちら。やる夫で学ぶ機械学習 - ロジスティック回帰 -
目次はこちら。やる夫で学ぶ機械学習シリーズ
対数尤度関数 やらない夫 尤度関数を微分してパラメータ $\boldsymbol{\theta}$ の更新式を求めてみようか。
やる夫 もう脳みそパンパンですお。
やらない夫 その前に、尤度関数はそのままだと扱いにくいから、少し変形しよう。
やる夫 扱いにくい？どういうことかお？
やらない夫 まず同時確率という点だ。確率なので 1 以下の数の掛け算の連続になることはわかるな？
やる夫 確かに、確率の値としては 0 より大きくて 1 より小さいものだお。
やらない夫 1 より小さい数を何度も掛け算すると、どんどん値が小さくなっていくだろう。コンピュータで計算する場合はそれは致命的な問題だ。
やる夫 あー、オーバーフローの逆かお。アンダーフロー。
やらない夫 次に掛け算という点だ。掛け算は足し算に比べて計算が面倒だ。
やる夫 小数点の計算とかあんまりやりたくないお。
やらない夫 そこで一般的には尤度関数の対数をとったもの、対数尤度関数を使う。
$$ \log L(\boldsymbol{\theta}) = \log \prod_{i=1}^n P(y^{(i)}=1|\boldsymbol{x})^{y^{(i)}} P(y^{(i)}=0|\boldsymbol{x})^{1-y^{(i)}} $$
やる夫 目的関数に対して勝手に対数をとったりして、答え変わらないのかお？
やらない夫 問題ない。対数関数は単調増加な関数だからだ。対数関数のグラフの形を覚えているか？
やる夫 確かこんな感じのグラフだお。
 やらない夫 それで正解だ。グラフがずっと右上がりになってることがわかるだろう。つまり単調増加な関数ってのは $x_1 &amp;lt; x_2$ ならば $f(x_1) &amp;lt; f(x_2)$ となるような関数 $f(x)$ だということだ。</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習シリーズ</title>
      <link>http://tkengo.github.io/blog/2016/06/06/yaruo-machine-learning0/</link>
      <pubDate>Mon, 06 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/06/yaruo-machine-learning0/</guid>
      <description> 本シリーズ記事が書籍になりました。詳しくはこちら。   「やさしく学ぶ 機械学習を理解するための数学のきほん」を執筆しました  これは、機械学習に関する基礎知識をまとめたシリーズ記事の目次となる記事です。まとめることで知識を体系化できて自分自身の為にもなるので、こういうアウトプットをすることは大事だと思っています。ただ、普通にブログ記事を書くのも面白くないので、ちょっといつもとは違う方法でやってみようというのが今回のシリーズ記事。
2 ちゃんねるのキャラクターが登場人物として出てきて、彼らが会話して話が進んでいく「やる夫で学ぶシリーズ」という講義調の形式のものがあります。個人的にはやる夫で学ぶシリーズや 数学ガール のような会話形式で話が進んでいく読み物は読みやすいと思っています。さらに、先日みつけた やる夫で学ぶディジタル信号処理 という資料がとてつもなくわかりやすく、これの真似をして書いてみようと思い至りました。記事中のやる夫とやらない夫のアイコンは http://matsucon.net/material/mona/ こちらのサイトの素材を使わせていただきました。
第 2 回まで半年ほど前に公開していましたが、その後ブログ執筆の熱が冷めて放置されていました。が、実は下書きだけなら乱雑ながら第 5 回までは書いていて、周りにいる機械学習入門中の知り合い数名から下書きあるなら公開して欲しいと言われたので、お蔵入りさせるのも勿体ないし一気に清書して、この目次記事をつけて公開することにしました。今後、このシリーズ記事が増えるかどうかはわかりません。
まえがき このシリーズでは実践的な内容というよりかは、基礎的で理論的な部分をまとめていきます。機械学習をやり始めるにあたってまず最初にやったほうが良いのは、こういった座学のような記事を頑張って読むよりかは、やってみた系の記事を読みながら実際に手を動かしてコーディングしてみることです。いきなり機械学習の本や数式を眺めて理論から理解し始めるのは、数学に自信がある人や素養がある人以外は難しく、挫折してしまう原因となります。
今は良い時代になっていて、フレームワークを基盤として少しのコードを書いて、公開されている無料の学習用データを使えば、それらしいものが出来てしまいます。そういうもので感覚を掴んでから、理論を理解するのでも遅くはないと思っています。
とはいえ、プログラマであれば中身を知らないものをアレコレ触るのは怖さがあるというもの。理論の方に手をだしてみたくなったりもします。座学系の記事は、このブログ以外にもたくさん転がっているのでイロイロ読み比べて知識を自分のものにしていくのが大事です。一晩でなんとかなるものでもないですし、じっくりやっていきましょう。
対象  機械学習って最近よく聞くけど中身を良くしらない人 中身しらないけど機械学習って楽しそうだしなんか勉強してみたい人 数学が好きな人 ドヤ顔で機械学習のことを話したい人  ※上級者向けではありません。どちらかというとむしろ初学者向けです。
目次 記事 1 つ 1 つが長く、分割していくうちに記事数が多くなってきたので、目次を作りました。
 やる夫で学ぶ機械学習 - 序章 - やる夫で学ぶ機械学習 - 単回帰問題 - やる夫で学ぶ機械学習 - 多項式回帰と重回帰 - やる夫で学ぶ機械学習 - パーセプトロン - やる夫で学ぶ機械学習 - ロジスティック回帰 - やる夫で学ぶ機械学習 - 対数尤度関数 -  </description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - ロジスティック回帰 -</title>
      <link>http://tkengo.github.io/blog/2016/06/04/yaruo-machine-learning5/</link>
      <pubDate>Sat, 04 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/04/yaruo-machine-learning5/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 5 回です。分類問題を解くためのロジスティック回帰を見ていきます。
第 4 回はこちら。やる夫で学ぶ機械学習 - パーセプトロン -
目次はこちら。やる夫で学ぶ機械学習シリーズ
ロジスティック回帰 やる夫 分類問題を解くための素晴らしいアルゴリズムがあると聞きましたお。
やらない夫 今日は展開が早いな。
やる夫 回りくどいのは終わりだお。
やらない夫 では、今日はロジスティック回帰の話をしていこう。ロジスティック回帰は、パーセプトロンのように基本的には二値分類の分類器を構築するためのものだ。
やる夫 パーセプトロンよりは使い物になるのかお？
やらない夫 もちろんさ。ロジスティック回帰はいろんなところで使われているし、線形分離不可能な問題も解ける。
やる夫 それはナイスだお。
やらない夫 いつものように最初は簡単な具体例を示して概要を見ていこうか。
やる夫 よろしくお願いしますお。
やらない夫 問題設定はパーセプトロンの時と同じものを使おう。つまり、色を暖色か寒色に分類することを考えるんだ。
やる夫 それ、線形分離可能な問題だお？線形分離不可能な問題やらないのかお？
やらない夫 物事には順序ってものがあるだろう。先に基礎からやるんだよ。ロジスティック回帰も、もちろん線形分離可能な問題は解ける。まずそこから入って、その応用で最後に線形分離不可能な問題を見ていこう。
やる夫 基礎力つけるの面倒くさいけど、わかったお&amp;hellip;
やらない夫 ロジスティック回帰は分類を確率として考えるんだ。
やる夫 確率？暖色である確率が 80%、寒色である確率が 20%、みたいな話ってことかお？
やらない夫 そうだ、いつもとぼけた顔してる割には冴えてるじゃないか。
やる夫 顔は生まれつきだお。文句いうなお。
やらない夫 暖色、寒色のままでは扱いにくいのはパーセプトロンと同じだから、ここでは暖色を $1$、寒色を $0$ と置くとしよう。
やる夫 あれ、寒色は $-1$ じゃないのかお？
やらない夫 クラス毎の値が異なっていれば別になんでもいいんだが、パーセプトロンの時に暖色が $1$ で寒色が $-1$ にしたのは、そうした方が重みの更新式が簡潔に書けるからだ。
やる夫 なるほど。ロジスティック回帰の場合は暖色を $1$ で寒色を $0$ にした方が、重みの更新式が簡潔に書き表せるってことかお？
やらない夫 そういうことだな。話を進めよう。回帰の時に、未知のデータ $\boldsymbol{x}$ に対応する値を求めるためにこういう関数を定義したのを覚えているか？</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - パーセプトロン -</title>
      <link>http://tkengo.github.io/blog/2016/06/03/yaruo-machine-learning4/</link>
      <pubDate>Fri, 03 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/03/yaruo-machine-learning4/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 4 回です。分類問題を解くための基礎、パーセプトロンを図形的な側面から覗いてみます。
第 3 回はこちら。やる夫で学ぶ機械学習 - 多項式回帰と重回帰 -
目次はこちら。やる夫で学ぶ機械学習シリーズ
問題設定 やらない夫 今日は分類問題について詳しく見ていく。
やる夫 分類問題かお。やる夫は、女の子が巨乳か貧乳かを分類して、想像を膨らませたいお。
やらない夫 そんなものは深夜に一人でニヤニヤしながらやるか、いつも引きこもってないで外に出て本物の女の子を見ることだな。
やる夫 冗談きついお、やらない夫。
やらない夫 分類の場合も、回帰の時と同じように具体例を示して、それを元に話を進めていったほうがわかりやすいな。
やる夫 それが良いお。具体例は、やる夫の将来の嫁さんくらい大事だお。
やらない夫 また意味のわからないたとえをありがとう。今回は分類の話なので、そうだな、色を分類することを考えてみよう。
やる夫 おっぱいじゃなくて、色を分類、するのかお？
やらない夫 おっぱいは忘れろ。たとえば、適当に与えられた色が、暖色系なのか寒色系なのかに分類する、という問題はどうだ？
やる夫 二値分類の問題かお。分類は確か教師あり学習だったから、つまりラベル付きの学習用データが要るってことかお？
やらない夫 そうだな、具体的には、色の情報と、その色が暖色なのか寒色なのか、というラベルを学習用データとして用意してやる必要がある。
やる夫 なるほど。
やらない夫 ところで、色と言えば RGB の三色を考えることができるが、最初は簡単な問題の方がいいから、緑のことは考えずに、赤と青だけに注目していこう。緑の要素は 0 に固定しようか。その方が図にもプロットできてわかりやすいしな。
やる夫 簡単になるなら歓迎だお。
やらない夫 たとえば、この色は暖色系、寒色系、どっちに見える？
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; #d80055  やる夫 んー、暖色系だお。
やらない夫 では、これはどうだ？
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; #2c00c8  やる夫 寒色系だお。
やらない夫 ということは、今 2 つの学習用データができた。緑の要素を 0 に固定しているから、#RRGGBB の GG の部分が 00 になっていることに注目だ。
   色 クラス     &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; #d80055 暖色   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; #2c00c8 寒色    やる夫 うん、分類問題の場合の学習用データのイメージつかめたお。</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - 多項式回帰と重回帰 -</title>
      <link>http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/</link>
      <pubDate>Thu, 02 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 3 回です。多項式回帰と重回帰について見ていきます。
第 2 回はこちら。やる夫で学ぶ機械学習 - 単回帰問題 -
目次はこちら。やる夫で学ぶ機械学習シリーズ
多項式回帰 やらない夫 さて、回帰の話にはもう少し続きがあるんだが。
やる夫 あんまり難しすぎるのは勘弁だお。
やらない夫 前回の延長だから、回帰が理解できていれば、そう難しい話ではないだろう。
やる夫 その前に、ちょっとおしっこいってくるお。
やらない夫 お前、そういうのは休憩中にやっとけよ&amp;hellip;
やる夫 わかったお！仕方ないから我慢するお！！
やらない夫 意外と根性あるな&amp;hellip;、ではこれから、前回の回帰の話をもう少し発展させていく。
やる夫 (あっ、ほんとに話を進めるのかお&amp;hellip;)
やらない夫 前回、俺たちは、求めたい関数をこのように定義して、パラメータである $\theta$ を求めた。
$$ f_{\theta}(x) = \theta_0 + \theta_1 x $$
やる夫 覚えてるお。$f_{\theta}(x)$ を使った目的関数を定義して、最急降下法でパラメータの $\theta$ を求めたんだお。
やらない夫 $f_{\theta}(x)$ は一次関数だから、関数の形は当然のことながら直線になる。そこは大丈夫だよな。
やる夫 なんか回りくどいお。前回やったことはちゃんと覚えてるから、復習ならしなくていいお。
やらない夫 そうか。では、本題に入るが、最初に示したプロットは、実は直線より曲線の方がよりフィットするんだ。
 やる夫 お、なるほど。確かに、曲線のグラフの方が、よりフィットしているように見えるお。
やらない夫 これは、関数 $f_\theta(x)$ を二次式として定義することで実現できる。
$$ f_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2 $$</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - 単回帰問題 -</title>
      <link>http://tkengo.github.io/blog/2016/01/04/yaruo-machine-learning2/</link>
      <pubDate>Mon, 04 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/01/04/yaruo-machine-learning2/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 2 回です。回帰について見ていきます。
第 1 回はこちら。やる夫で学ぶ機械学習 - 序章 -
目次はこちら。やる夫で学ぶ機械学習シリーズ
問題設定 やらない夫 今日は回帰について詳しく見ていく。
やる夫 回帰って響きがカッコいいお。
やらない夫 ここからは、より具体的な例を混じえながら話を進めていこう。
やる夫 具体例は、やる夫の明日のお昼ごはんぐらい大事だお。
やらない夫 まったく意味がわからないたとえなんだが&amp;hellip;。そうだな、たとえば、主人公の攻撃力によって、敵キャラに与えるダメージが決まるゲームがあるとしよう。
やる夫 よくある設定だお。
やらない夫 ダメージには揺らぎがあって、常に同じダメージを与えられるとは限らない。さて、実際に何度か敵キャラに攻撃してみて、その時の攻撃力と与えたダメージをグラフにプロットしてみると、こんな風になっていたとしよう。
 やる夫 なるほど。攻撃力が高くなればなるほど、与えるダメージも大きくなっているお。
やらない夫 やる夫はコレを見て、攻撃力が 10 の時にどれくらいダメージを与えられるかわかるか？
やる夫 馬鹿にしてるのかお！そんなの簡単だお。だいたい 60 前後くらいだお？
 やらない夫 そうだな。俺たちはこれから機械学習を使って、今やる夫がやったように、攻撃力からダメージを予測していく。
やる夫 そんなの、このプロットを見れば、だいたい誰にでもわかるお？
やらない夫 それはこの問題設定が単純だからだよ。実際に機械学習を使って解きたい問題は、複雑な問題であることがほとんだ。そういうものを機械にまかせるために、データから学習させるんだよ。それが機械学習だ。後からもっと複雑な問題も見ていくから、その時もまた同じことが言えるかどうかだな。
やる夫 ふーん。
最小二乗法 やる夫 どうやって学習していくんだお？
やらない夫 関数をイメージするんだ。このプロットの各点を通る関数の形がわかれば、攻撃力からダメージがわかるだろ。ただし、ダメージにはノイズが含まれているから、関数がきっちり全ての点を通るわけじゃない。
 やる夫 これは一次関数だお！一次関数は、切片と傾きがわかれば関数の形が決まるから、それを調べることになるのかお？
やらない夫 冴えてるな。俺たちがいまから考える関数は、切片と傾きを $\theta$ を使って表すと、こんな風になる。
$$ y = \theta_0 + \theta_1 x $$
やる夫 うっ、式が出てくると急に数学っぽくなるお&amp;hellip;、$\theta$ ってなんだお。
やらない夫 $\theta$ はこれから俺たちが求めていく未知数だ。パラメータとも言う。文字は別になんでもいいんだが、統計学の世界では、未知数や推定値を $\theta$ で表すことが多いので、今回も $\theta$ を使っただけだ。</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - 序章 -</title>
      <link>http://tkengo.github.io/blog/2016/01/03/yaruo-machine-learning1/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/01/03/yaruo-machine-learning1/</guid>
      <description>やる夫で学ぶ機械学習を書いてみました。
やる夫で学ぶ機械学習シリーズの第 1 回です。記事がとても長くなったので、分割してます。
目次はこちら。やる夫で学ぶ機械学習シリーズ
機械学習 やる夫 機械学習やってみたいけど、そもそもどうすりゃいいかまったくわからんお。Wikipedia を見たけど、何を言ってるのかさっぱりだお&amp;hellip;
やらない夫 お前、Wikipedia でわかるわけがないだろ、常識的に考えて&amp;hellip;
やる夫 そうなのかお&amp;hellip;。しかも、やたら数式が出てきて、日本語でおｋって言いたくなるお。
やらない夫 確かに数式は出てくるが、そもそも数式というのは日本語で言うと長ったらしくなるものを誰にでもわかるように厳密で簡潔に表してるものなんだぞ。というか、やる夫は機械学習で何がしたいんだ？
やる夫 えっ&amp;hellip;、あーえーっと、ほら、アレだお&amp;hellip;なんか、こう、パーッと、いい感じの&amp;hellip;
やらない夫 お前、ただ機械学習って言いたいだけだろ&amp;hellip;
やる夫 そんなことないお！！やる夫は流行に敏感なんだお！！！
やらない夫 まず機械学習で何をやりたいかを考えることは大事なことなんだが&amp;hellip;、じゃぁ質問を変えよう。機械学習はどういうところで使われていると思う？
やる夫 スパムメールの判定とか、文字認識、金融市場の予測なんかに使われてるお。
やらない夫 その通りだ。よく知ってるじゃないか。
やる夫 Wikipedia に書いてあったお。
やらない夫 実際は、それ以外にも幅広い分野で利用されているんだ。
やる夫 万能タイプなんだお。
やらない夫 そんなことはないぞ。機械学習があればなんでもできるようになるわけじゃない。どこに適用できて、どこに適用できないのかを見極めるのも大事なことだ。それに、大抵の場合は大量の学習データを人手で用意する必要もあるんだ。
やる夫 機械学習ってそんなもんなのかお&amp;hellip;、ちょっと面倒くさいお。勉強するモチベーションさがってきたお。
やらない夫 まあそういうな。機械学習のおかげで、今まで出来なかったことが出来るようになった事例はたくさんあるんだ。勉強してみると案外おもしろいもんだぞ。
やる夫 ふーん。そこまで言うなら勉強につきあってやってもいいお。
やらない夫 なんで急に上から目線なんだよ。
やる夫 ここでやる夫が勉強をやめると、話が続かないんだお。
やらない夫 まったく&amp;hellip;。じゃぁ最初は機械学習でどういう問題を扱えるかという話からだ。
回帰、分類、クラスタリング やらない夫 さっき言ったように、機械学習はどんな問題でも扱えるわけじゃない。機械学習が得意とする問題には、主に以下のようなものがある。
 回帰 分類 クラスタリング  やる夫 統計学で同じような単語を聞いたことがあるお。回帰分析とか、統計的分類とか。
やらない夫 やる夫は統計学に明るいのか。
やる夫 えっ、いや、Wikipedia で読んだことあるだけだお&amp;hellip;
やらない夫 確かに回帰や分類は、統計学で言うところのそれと同じだな。どこが違うかって言われると微妙だが、その辺を気にしてもあまり建設的ではないから、話を先に進めよう。
やる夫 やらない夫がはぐらかしたお。
やらない夫 いいんだよ。とりあえず、回帰の話をしようか。英語で Regression とも言う。回帰は、わかりやすく言うと学習データとして &amp;ldquo;連続するデータ&amp;rdquo;、例えば時系列データ、などが与えられた時、未知のデータがどういうものかを予測するものだ。</description>
    </item>
    
    <item>
      <title>「目に見えるパーセプトロン」という資料を作ったので公開します</title>
      <link>http://tkengo.github.io/blog/2015/08/21/visual-perceptron/</link>
      <pubDate>Fri, 21 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2015/08/21/visual-perceptron/</guid>
      <description>先日、社内の勉強会でネタが募集されていたので機械学習、特にパーセプトロンについてしゃべりました。パーセプトロン自体は非常に単純なモデルで、理解も実装も比較的容易で、それゆえに様々なものの基礎になっています。近年、深層学習と呼ばれる学習を何層にも渡って行うような概念が話題になっていますが、そこに出てくるニューラルネットワークは一種の多層パーセプトロンでもあります。ここ数年での機械学習や深層学習の進歩のスピードは素晴らしいものがありますが、そんな中で基礎的な部分を押さえておくことはより大事になってくるのではないでしょうか。
社内勉強会で使ったスライドでは、簡単な機械学習の概要とパーセプトロンの仕組みを解説してみました。インターネット上に公開したので、このブログでも紹介しておきます。

  補足 いくつか資料の補足をしておきたいと思います。
学習について 基本的には対象データから素性を抽出して、それを識別しながら重みが間違っていれば更新していくという流れです。素性という単語は他にも特徴量と呼ばれたりもします。スライドには、とりあえず全ての学習データが無くなるまで繰り返していくと最終的には理想的な(重みベクトルを法線とする)直線が引かれますが、そうならないこともあるのでその場合は何度か学習を繰り返します。
学習の終了判定は一定の判断基準を持って学習の終了とするか、もしくはある一定回数だけ学習を繰り返したら終了とするかのどちらかになると思います。どちらを選ぶかはケースバイケースですが、勉強や練習のためにパーセプトロンを実装するだけであれば、後者の方法の方がはるかに簡単です。
線形分離不可能について パーセプトロンは基本的に線形分離可能な問題にしか適用することができません。それは資料のまとめの部分にも書いてある通りなのですが、あるテクニックを使うと線形分離不可能な問題にも適用することができます。
たとえば分類対象データから抽出した素性が二次元ベクトルでそれらが 2 つのクラスに分類されるんだけど、直線を引いて 2 つに分割できない、すなわち線形分離不可能な問題だった場合、その抽出された素性をより高次元な空間に写像してあげます。たとえばとある二次元の素性 (x, y) があったとすると、これを (x, y, x^2, y^2) などという四次元の空間に写像してあげて、この素性をパーセプトロンに渡して学習を繰り返すと、二次元では線形分離不可能だった問題が四次元で線形分離可能になる場合があります。
これをカーネルトリックと言います。
目に見える スライドのタイトルにもある通り 目に見える というところが推しポイントです。何が目に見えるのかというと学習の過程です。スライドだと二次元の空間において学習していく過程を簡単に説明しました。その部分を読んだ上で今度は三次元空間において学習していく過程を表現したものを作ってみました。それがこれです。
目に見えるパーセプトロン
three.js を使って作っています。まずページを開くと初期状態で以下のようなものが表示されます。各点が三次元の素性ベクトルを空間にプロットしたものです。
ここでスペースキーを押すたびに学習データを読み込んで学習を進めていきます。その際に都度、重みベクトル (ピンク色の矢印) を更新して空間を分割する平面 (資料では二次元なので 線 だったけど、これは三次元なので平面) が更新されていく様子がわかると思います。学習データがなくなるまで学習が進むと、青と緑の点が綺麗に 2 分割されている状態が出来上がると思います。
マウスでグリグリ視点を動かすことも出来るのでいろいろいじってみてください。ちなみにソースコードはこちら。
https://github.com/tkengo/perceptron_visualization
それでは。</description>
    </item>
    
  </channel>
</rss>