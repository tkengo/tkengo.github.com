<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>やる夫で学ぶ on けんごのお屋敷</title>
    <link>http://tkengo.github.io/tags/%E3%82%84%E3%82%8B%E5%A4%AB%E3%81%A7%E5%AD%A6%E3%81%B6/</link>
    <description>Recent content in やる夫で学ぶ on けんごのお屋敷</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Thu, 16 Jun 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://tkengo.github.io/tags/%E3%82%84%E3%82%8B%E5%A4%AB%E3%81%A7%E5%AD%A6%E3%81%B6/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>やる夫で学ぶ機械学習 - 対数尤度関数 -</title>
      <link>http://tkengo.github.io/blog/2016/06/16/yaruo-machine-learning6/</link>
      <pubDate>Thu, 16 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/16/yaruo-machine-learning6/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 6 回です。ロジスティック回帰の目的関数である尤度関数をもう少し詳しくみて、線形分離不可能な問題にどのように適用していくのかを見ていきます。
第 5 回はこちら。やる夫で学ぶ機械学習 - ロジスティック回帰 -
目次はこちら。やる夫で学ぶ機械学習シリーズ
対数尤度関数 やらない夫 尤度関数を微分してパラメータ $\boldsymbol{\theta}$ の更新式を求めてみようか。
やる夫 もう脳みそパンパンですお。
やらない夫 その前に、尤度関数はそのままだと扱いにくいから、少し変形しよう。
やる夫 扱いにくい？どういうことかお？
やらない夫 まず同時確率という点だ。確率なので 1 以下の数の掛け算の連続になることはわかるな？
やる夫 確かに、確率の値としては 0 より大きくて 1 より小さいものだお。
やらない夫 1 より小さい数を何度も掛け算すると、どんどん値が小さくなっていくだろう。コンピュータで計算する場合はそれは致命的な問題だ。
やる夫 あー、オーバーフローの逆かお。アンダーフロー。
やらない夫 次に掛け算という点だ。掛け算は足し算に比べて計算が面倒だ。
やる夫 小数点の計算とかあんまりやりたくないお。
やらない夫 そこで一般的には尤度関数の対数をとったもの、対数尤度関数を使う。
$$ \log L(\boldsymbol{\theta}) = \log \prod_{i=1}^n P(y^{(i)}=1|\boldsymbol{x})^{y^{(i)}} P(y^{(i)}=0|\boldsymbol{x})^{1-y^{(i)}} $$
やる夫 目的関数に対して勝手に対数をとったりして、答え変わらないのかお？
やらない夫 問題ない。対数関数は単調増加な関数だからだ。対数関数のグラフの形を覚えているか？
やる夫 確かこんな感じのグラフだお。
 やらない夫 それで正解だ。グラフがずっと右上がりになってることがわかるだろう。つまり単調増加な関数ってのは $x_1 &amp;lt; x_2$ ならば $f(x_1) &amp;lt; f(x_2)$ となるような関数 $f(x)$ だということだ。</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習シリーズ</title>
      <link>http://tkengo.github.io/blog/2016/06/06/yaruo-machine-learning0/</link>
      <pubDate>Mon, 06 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/06/yaruo-machine-learning0/</guid>
      <description> 本シリーズ記事が書籍になりました。詳しくはこちら。   「やさしく学ぶ 機械学習を理解するための数学のきほん」を執筆しました  これは、機械学習に関する基礎知識をまとめたシリーズ記事の目次となる記事です。まとめることで知識を体系化できて自分自身の為にもなるので、こういうアウトプットをすることは大事だと思っています。ただ、普通にブログ記事を書くのも面白くないので、ちょっといつもとは違う方法でやってみようというのが今回のシリーズ記事。
2 ちゃんねるのキャラクターが登場人物として出てきて、彼らが会話して話が進んでいく「やる夫で学ぶシリーズ」という講義調の形式のものがあります。個人的にはやる夫で学ぶシリーズや 数学ガール のような会話形式で話が進んでいく読み物は読みやすいと思っています。さらに、先日みつけた やる夫で学ぶディジタル信号処理 という資料がとてつもなくわかりやすく、これの真似をして書いてみようと思い至りました。記事中のやる夫とやらない夫のアイコンは http://matsucon.net/material/mona/ こちらのサイトの素材を使わせていただきました。
第 2 回まで半年ほど前に公開していましたが、その後ブログ執筆の熱が冷めて放置されていました。が、実は下書きだけなら乱雑ながら第 5 回までは書いていて、周りにいる機械学習入門中の知り合い数名から下書きあるなら公開して欲しいと言われたので、お蔵入りさせるのも勿体ないし一気に清書して、この目次記事をつけて公開することにしました。今後、このシリーズ記事が増えるかどうかはわかりません。
まえがき このシリーズでは実践的な内容というよりかは、基礎的で理論的な部分をまとめていきます。機械学習をやり始めるにあたってまず最初にやったほうが良いのは、こういった座学のような記事を頑張って読むよりかは、やってみた系の記事を読みながら実際に手を動かしてコーディングしてみることです。いきなり機械学習の本や数式を眺めて理論から理解し始めるのは、数学に自信がある人や素養がある人以外は難しく、挫折してしまう原因となります。
今は良い時代になっていて、フレームワークを基盤として少しのコードを書いて、公開されている無料の学習用データを使えば、それらしいものが出来てしまいます。そういうもので感覚を掴んでから、理論を理解するのでも遅くはないと思っています。
とはいえ、プログラマであれば中身を知らないものをアレコレ触るのは怖さがあるというもの。理論の方に手をだしてみたくなったりもします。座学系の記事は、このブログ以外にもたくさん転がっているのでイロイロ読み比べて知識を自分のものにしていくのが大事です。一晩でなんとかなるものでもないですし、じっくりやっていきましょう。
対象  機械学習って最近よく聞くけど中身を良くしらない人 中身しらないけど機械学習って楽しそうだしなんか勉強してみたい人 数学が好きな人 ドヤ顔で機械学習のことを話したい人  ※上級者向けではありません。どちらかというとむしろ初学者向けです。
目次 記事 1 つ 1 つが長く、分割していくうちに記事数が多くなってきたので、目次を作りました。
 やる夫で学ぶ機械学習 - 序章 - やる夫で学ぶ機械学習 - 単回帰問題 - やる夫で学ぶ機械学習 - 多項式回帰と重回帰 - やる夫で学ぶ機械学習 - パーセプトロン - やる夫で学ぶ機械学習 - ロジスティック回帰 - やる夫で学ぶ機械学習 - 対数尤度関数 -  </description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - ロジスティック回帰 -</title>
      <link>http://tkengo.github.io/blog/2016/06/04/yaruo-machine-learning5/</link>
      <pubDate>Sat, 04 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/04/yaruo-machine-learning5/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 5 回です。分類問題を解くためのロジスティック回帰を見ていきます。
第 4 回はこちら。やる夫で学ぶ機械学習 - パーセプトロン -
目次はこちら。やる夫で学ぶ機械学習シリーズ
ロジスティック回帰 やる夫 分類問題を解くための素晴らしいアルゴリズムがあると聞きましたお。
やらない夫 今日は展開が早いな。
やる夫 回りくどいのは終わりだお。
やらない夫 では、今日はロジスティック回帰の話をしていこう。ロジスティック回帰は、パーセプトロンのように基本的には二値分類の分類器を構築するためのものだ。
やる夫 パーセプトロンよりは使い物になるのかお？
やらない夫 もちろんさ。ロジスティック回帰はいろんなところで使われているし、線形分離不可能な問題も解ける。
やる夫 それはナイスだお。
やらない夫 いつものように最初は簡単な具体例を示して概要を見ていこうか。
やる夫 よろしくお願いしますお。
やらない夫 問題設定はパーセプトロンの時と同じものを使おう。つまり、色を暖色か寒色に分類することを考えるんだ。
やる夫 それ、線形分離可能な問題だお？線形分離不可能な問題やらないのかお？
やらない夫 物事には順序ってものがあるだろう。先に基礎からやるんだよ。ロジスティック回帰も、もちろん線形分離可能な問題は解ける。まずそこから入って、その応用で最後に線形分離不可能な問題を見ていこう。
やる夫 基礎力つけるの面倒くさいけど、わかったお&amp;hellip;
やらない夫 ロジスティック回帰は分類を確率として考えるんだ。
やる夫 確率？暖色である確率が 80%、寒色である確率が 20%、みたいな話ってことかお？
やらない夫 そうだ、いつもとぼけた顔してる割には冴えてるじゃないか。
やる夫 顔は生まれつきだお。文句いうなお。
やらない夫 暖色、寒色のままでは扱いにくいのはパーセプトロンと同じだから、ここでは暖色を $1$、寒色を $0$ と置くとしよう。
やる夫 あれ、寒色は $-1$ じゃないのかお？
やらない夫 クラス毎の値が異なっていれば別になんでもいいんだが、パーセプトロンの時に暖色が $1$ で寒色が $-1$ にしたのは、そうした方が重みの更新式が簡潔に書けるからだ。
やる夫 なるほど。ロジスティック回帰の場合は暖色を $1$ で寒色を $0$ にした方が、重みの更新式が簡潔に書き表せるってことかお？
やらない夫 そういうことだな。話を進めよう。回帰の時に、未知のデータ $\boldsymbol{x}$ に対応する値を求めるためにこういう関数を定義したのを覚えているか？</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - パーセプトロン -</title>
      <link>http://tkengo.github.io/blog/2016/06/03/yaruo-machine-learning4/</link>
      <pubDate>Fri, 03 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/03/yaruo-machine-learning4/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 4 回です。分類問題を解くための基礎、パーセプトロンを図形的な側面から覗いてみます。
第 3 回はこちら。やる夫で学ぶ機械学習 - 多項式回帰と重回帰 -
目次はこちら。やる夫で学ぶ機械学習シリーズ
問題設定 やらない夫 今日は分類問題について詳しく見ていく。
やる夫 分類問題かお。やる夫は、女の子が巨乳か貧乳かを分類して、想像を膨らませたいお。
やらない夫 そんなものは深夜に一人でニヤニヤしながらやるか、いつも引きこもってないで外に出て本物の女の子を見ることだな。
やる夫 冗談きついお、やらない夫。
やらない夫 分類の場合も、回帰の時と同じように具体例を示して、それを元に話を進めていったほうがわかりやすいな。
やる夫 それが良いお。具体例は、やる夫の将来の嫁さんくらい大事だお。
やらない夫 また意味のわからないたとえをありがとう。今回は分類の話なので、そうだな、色を分類することを考えてみよう。
やる夫 おっぱいじゃなくて、色を分類、するのかお？
やらない夫 おっぱいは忘れろ。たとえば、適当に与えられた色が、暖色系なのか寒色系なのかに分類する、という問題はどうだ？
やる夫 二値分類の問題かお。分類は確か教師あり学習だったから、つまりラベル付きの学習用データが要るってことかお？
やらない夫 そうだな、具体的には、色の情報と、その色が暖色なのか寒色なのか、というラベルを学習用データとして用意してやる必要がある。
やる夫 なるほど。
やらない夫 ところで、色と言えば RGB の三色を考えることができるが、最初は簡単な問題の方がいいから、緑のことは考えずに、赤と青だけに注目していこう。緑の要素は 0 に固定しようか。その方が図にもプロットできてわかりやすいしな。
やる夫 簡単になるなら歓迎だお。
やらない夫 たとえば、この色は暖色系、寒色系、どっちに見える？
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; #d80055  やる夫 んー、暖色系だお。
やらない夫 では、これはどうだ？
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; #2c00c8  やる夫 寒色系だお。
やらない夫 ということは、今 2 つの学習用データができた。緑の要素を 0 に固定しているから、#RRGGBB の GG の部分が 00 になっていることに注目だ。
   色 クラス     &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; #d80055 暖色   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; #2c00c8 寒色    やる夫 うん、分類問題の場合の学習用データのイメージつかめたお。</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - 多項式回帰と重回帰 -</title>
      <link>http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/</link>
      <pubDate>Thu, 02 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/06/02/yaruo-machine-learning3/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 3 回です。多項式回帰と重回帰について見ていきます。
第 2 回はこちら。やる夫で学ぶ機械学習 - 単回帰問題 -
目次はこちら。やる夫で学ぶ機械学習シリーズ
多項式回帰 やらない夫 さて、回帰の話にはもう少し続きがあるんだが。
やる夫 あんまり難しすぎるのは勘弁だお。
やらない夫 前回の延長だから、回帰が理解できていれば、そう難しい話ではないだろう。
やる夫 その前に、ちょっとおしっこいってくるお。
やらない夫 お前、そういうのは休憩中にやっとけよ&amp;hellip;
やる夫 わかったお！仕方ないから我慢するお！！
やらない夫 意外と根性あるな&amp;hellip;、ではこれから、前回の回帰の話をもう少し発展させていく。
やる夫 (あっ、ほんとに話を進めるのかお&amp;hellip;)
やらない夫 前回、俺たちは、求めたい関数をこのように定義して、パラメータである $\theta$ を求めた。
$$ f_{\theta}(x) = \theta_0 + \theta_1 x $$
やる夫 覚えてるお。$f_{\theta}(x)$ を使った目的関数を定義して、最急降下法でパラメータの $\theta$ を求めたんだお。
やらない夫 $f_{\theta}(x)$ は一次関数だから、関数の形は当然のことながら直線になる。そこは大丈夫だよな。
やる夫 なんか回りくどいお。前回やったことはちゃんと覚えてるから、復習ならしなくていいお。
やらない夫 そうか。では、本題に入るが、最初に示したプロットは、実は直線より曲線の方がよりフィットするんだ。
 やる夫 お、なるほど。確かに、曲線のグラフの方が、よりフィットしているように見えるお。
やらない夫 これは、関数 $f_\theta(x)$ を二次式として定義することで実現できる。
$$ f_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2 $$</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - 単回帰問題 -</title>
      <link>http://tkengo.github.io/blog/2016/01/04/yaruo-machine-learning2/</link>
      <pubDate>Mon, 04 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/01/04/yaruo-machine-learning2/</guid>
      <description>やる夫で学ぶ機械学習シリーズの第 2 回です。回帰について見ていきます。
第 1 回はこちら。やる夫で学ぶ機械学習 - 序章 -
目次はこちら。やる夫で学ぶ機械学習シリーズ
問題設定 やらない夫 今日は回帰について詳しく見ていく。
やる夫 回帰って響きがカッコいいお。
やらない夫 ここからは、より具体的な例を混じえながら話を進めていこう。
やる夫 具体例は、やる夫の明日のお昼ごはんぐらい大事だお。
やらない夫 まったく意味がわからないたとえなんだが&amp;hellip;。そうだな、たとえば、主人公の攻撃力によって、敵キャラに与えるダメージが決まるゲームがあるとしよう。
やる夫 よくある設定だお。
やらない夫 ダメージには揺らぎがあって、常に同じダメージを与えられるとは限らない。さて、実際に何度か敵キャラに攻撃してみて、その時の攻撃力と与えたダメージをグラフにプロットしてみると、こんな風になっていたとしよう。
 やる夫 なるほど。攻撃力が高くなればなるほど、与えるダメージも大きくなっているお。
やらない夫 やる夫はコレを見て、攻撃力が 10 の時にどれくらいダメージを与えられるかわかるか？
やる夫 馬鹿にしてるのかお！そんなの簡単だお。だいたい 60 前後くらいだお？
 やらない夫 そうだな。俺たちはこれから機械学習を使って、今やる夫がやったように、攻撃力からダメージを予測していく。
やる夫 そんなの、このプロットを見れば、だいたい誰にでもわかるお？
やらない夫 それはこの問題設定が単純だからだよ。実際に機械学習を使って解きたい問題は、複雑な問題であることがほとんだ。そういうものを機械にまかせるために、データから学習させるんだよ。それが機械学習だ。後からもっと複雑な問題も見ていくから、その時もまた同じことが言えるかどうかだな。
やる夫 ふーん。
最小二乗法 やる夫 どうやって学習していくんだお？
やらない夫 関数をイメージするんだ。このプロットの各点を通る関数の形がわかれば、攻撃力からダメージがわかるだろ。ただし、ダメージにはノイズが含まれているから、関数がきっちり全ての点を通るわけじゃない。
 やる夫 これは一次関数だお！一次関数は、切片と傾きがわかれば関数の形が決まるから、それを調べることになるのかお？
やらない夫 冴えてるな。俺たちがいまから考える関数は、切片と傾きを $\theta$ を使って表すと、こんな風になる。
$$ y = \theta_0 + \theta_1 x $$
やる夫 うっ、式が出てくると急に数学っぽくなるお&amp;hellip;、$\theta$ ってなんだお。
やらない夫 $\theta$ はこれから俺たちが求めていく未知数だ。パラメータとも言う。文字は別になんでもいいんだが、統計学の世界では、未知数や推定値を $\theta$ で表すことが多いので、今回も $\theta$ を使っただけだ。</description>
    </item>
    
    <item>
      <title>やる夫で学ぶ機械学習 - 序章 -</title>
      <link>http://tkengo.github.io/blog/2016/01/03/yaruo-machine-learning1/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/01/03/yaruo-machine-learning1/</guid>
      <description>やる夫で学ぶ機械学習を書いてみました。
やる夫で学ぶ機械学習シリーズの第 1 回です。記事がとても長くなったので、分割してます。
目次はこちら。やる夫で学ぶ機械学習シリーズ
機械学習 やる夫 機械学習やってみたいけど、そもそもどうすりゃいいかまったくわからんお。Wikipedia を見たけど、何を言ってるのかさっぱりだお&amp;hellip;
やらない夫 お前、Wikipedia でわかるわけがないだろ、常識的に考えて&amp;hellip;
やる夫 そうなのかお&amp;hellip;。しかも、やたら数式が出てきて、日本語でおｋって言いたくなるお。
やらない夫 確かに数式は出てくるが、そもそも数式というのは日本語で言うと長ったらしくなるものを誰にでもわかるように厳密で簡潔に表してるものなんだぞ。というか、やる夫は機械学習で何がしたいんだ？
やる夫 えっ&amp;hellip;、あーえーっと、ほら、アレだお&amp;hellip;なんか、こう、パーッと、いい感じの&amp;hellip;
やらない夫 お前、ただ機械学習って言いたいだけだろ&amp;hellip;
やる夫 そんなことないお！！やる夫は流行に敏感なんだお！！！
やらない夫 まず機械学習で何をやりたいかを考えることは大事なことなんだが&amp;hellip;、じゃぁ質問を変えよう。機械学習はどういうところで使われていると思う？
やる夫 スパムメールの判定とか、文字認識、金融市場の予測なんかに使われてるお。
やらない夫 その通りだ。よく知ってるじゃないか。
やる夫 Wikipedia に書いてあったお。
やらない夫 実際は、それ以外にも幅広い分野で利用されているんだ。
やる夫 万能タイプなんだお。
やらない夫 そんなことはないぞ。機械学習があればなんでもできるようになるわけじゃない。どこに適用できて、どこに適用できないのかを見極めるのも大事なことだ。それに、大抵の場合は大量の学習データを人手で用意する必要もあるんだ。
やる夫 機械学習ってそんなもんなのかお&amp;hellip;、ちょっと面倒くさいお。勉強するモチベーションさがってきたお。
やらない夫 まあそういうな。機械学習のおかげで、今まで出来なかったことが出来るようになった事例はたくさんあるんだ。勉強してみると案外おもしろいもんだぞ。
やる夫 ふーん。そこまで言うなら勉強につきあってやってもいいお。
やらない夫 なんで急に上から目線なんだよ。
やる夫 ここでやる夫が勉強をやめると、話が続かないんだお。
やらない夫 まったく&amp;hellip;。じゃぁ最初は機械学習でどういう問題を扱えるかという話からだ。
回帰、分類、クラスタリング やらない夫 さっき言ったように、機械学習はどんな問題でも扱えるわけじゃない。機械学習が得意とする問題には、主に以下のようなものがある。
 回帰 分類 クラスタリング  やる夫 統計学で同じような単語を聞いたことがあるお。回帰分析とか、統計的分類とか。
やらない夫 やる夫は統計学に明るいのか。
やる夫 えっ、いや、Wikipedia で読んだことあるだけだお&amp;hellip;
やらない夫 確かに回帰や分類は、統計学で言うところのそれと同じだな。どこが違うかって言われると微妙だが、その辺を気にしてもあまり建設的ではないから、話を先に進めよう。
やる夫 やらない夫がはぐらかしたお。
やらない夫 いいんだよ。とりあえず、回帰の話をしようか。英語で Regression とも言う。回帰は、わかりやすく言うと学習データとして &amp;ldquo;連続するデータ&amp;rdquo;、例えば時系列データ、などが与えられた時、未知のデータがどういうものかを予測するものだ。</description>
    </item>
    
  </channel>
</rss>