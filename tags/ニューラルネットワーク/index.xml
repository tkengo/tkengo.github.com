<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ニューラルネットワーク on けんごのお屋敷</title>
    <link>http://tkengo.github.io/tags/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF/</link>
    <description>Recent content in ニューラルネットワーク on けんごのお屋敷</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Mon, 09 May 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://tkengo.github.io/tags/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Word2Vec のニューラルネットワーク学習過程を理解する</title>
      <link>http://tkengo.github.io/blog/2016/05/09/understand-how-to-learn-word2vec/</link>
      <pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/05/09/understand-how-to-learn-word2vec/</guid>
      <description>Word2Vec というと、文字通り単語をベクトルとして表現することで単語の意味をとらえることができる手法として有名なものですが、最近だと Word2Vec を協調フィルタリングに応用する研究 (Item2Vec と呼ばれる) などもあるようで、この Word2Vec というツールは自然言語処理の分野の壁を超えて活躍しています。
実は Item2Vec を実装してみたくて Word2Vec の仕組みを理解しようとしていたのですが、Word2Vec の内部の詳細に踏み込んで解説した日本語記事を見かけることがなかったので、今更感はありますが自分の知識の整理のためにもブログに残しておきます。なお、この記事は Word2Vec のソースコードといくつかのペーパーを読んで自力で理解した内容になります。間違いが含まれている可能性もありますのでご了承ください。もし間違いを見つけた場合は指摘してもらえると修正します。
※この記事を読むにあたっては、ニューラルネットワーク及び確率的勾配降下法に関する基礎知識程度は必要です。
Word2Vec Word2Vec の正体は、隠れ層と出力層の 2 層からなる単純なニューラルネットワークです。このニューラルネットワークに次々に単語を読み込ませて重みを学習させていくのですが、Word2Vec で獲得できる単語のベクトル表現というのは実はネットワークの重みそのものです。入力データに対する &amp;ldquo;良い表現&amp;rdquo; を獲得するという意味では自己符号化器 (オートエンコーダ) に近いものがあると思います。
Word2Vec のネットワークのアーキテクチャとしては CBoW 及び Skip-gram と呼ばれる 2 種類があり、また学習を高速化するテクニックとして Hierarchical Softmax 及び Negative Sampling の 2 種類が提案されています。Word2Vec 論文の著者が公開しているオリジナルの C のソースコードでは両方のアーキテクチャ及び高速化テクニックが実装されていますが、この記事では Skip-gram と Negative Sampling について書いていきます。CBoW 及び Hierarchical Softmax については機会があれば別途記事を書きます。
Skip-gram でモデル化する Skip-gram とは、ある単語が与えられた時、その周辺の単語を予測するためのモデルです。たとえば以下のような単語の集合があったとしましょう。このようなものはボキャブラリと呼ばれます。
{ I, am, a, programmer, like, dog, cat }  これらの単語を使って文章を作ってください、と言われたら I am a programmer や I like a dog などの文章がすぐに思い浮かぶでしょう。ここで作った文章中の I という単語に注目すると、その次には am や like が現れて、さらにその次には a があって、それに続いて programmer や dog があります。しかし、I の次にいきなり programmer や dog といった単語はあまり現れないでしょう。つまり I という単語の周辺にはどういった単語が出現しやすいか、という確率を考えることができます。</description>
    </item>
    
    <item>
      <title>畳み込みニューラルネットワークによるテキスト分類を TensorFlow で実装する</title>
      <link>http://tkengo.github.io/blog/2016/03/14/text-classification-by-cnn/</link>
      <pubDate>Mon, 14 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/03/14/text-classification-by-cnn/</guid>
      <description>先日、九工大や東工大などの学生さんが LINE Fukuoka に遊びにきてくれました。せっかく学生さんが遊びに来てくれるので LINE Fukuoka の社員と学生さんとで LT 大会をやろうという運びになって、学生さんは普段やっている研究内容を、LINE Fukuoka 側はなんでも良いので適当な話を、それぞれやりました。当日は私を含む LINE Fukuoka の社員 3 人と、学生さん 2 人の合計 5 人が LT をしました。詳細は LINE Fukuoka 公式ブログに書かれていますので、興味のある方は御覧ください。
[社外活動/報告] 学生を招いてのエンジニア技術交流会を開催しました。
LT に使った資料は公開してもいいよ、とのことだったので、せっかくなので公開。当日はテキスト分類のデモをやったのですが、残念ながらデモ環境までは公開できませんでした。ただ、ソースコードは github で公開していますので見ることができますので後ほどリンクを貼っておきます。
  この LT では時間が 15 分しかなくて実装の話は一切できなかったので、せっかくなのでこの記事では実装の話を書こうと思います。実装にあたっては Google 製の機械学習フレームワークの TensorFlow を利用しています。実際は、この実装の元になった実装があって、本当はそれをそのまま使いたかったのですが、自分が適用したいタスクに対してはいくつか問題点があったのと、TensorFlow の感覚を掴みたかったので、自分でスクラッチで実装しました。 以下ソースコードです。
tkengo/tf/cnn_text_classification
また、もし時間があれば、この記事を読む前に以下のリンク先も合わせて読んでおくとより理解が深まると思います。
 Convolutional Neural Networks for Sentence Classification 実装の元になっている論文。 Implementing a CNN for Text Classification in TensorFlow 私の実装の元になったオリジナル実装についての記事。英語。 自然言語処理における畳み込みニューラルネットワークを理解する 上記ブログと同じ作者が執筆した別記事を私が日本語に翻訳したもの。  なお、本記事では実装の詳しい内容を書いていますので、以下のような方を対象読者と想定しています。
 ニューラルネットワークに対する簡単な基礎知識はある TensorFlow の MNIST For ML Beginners チュートリアルは完了して基本的な概念と使い方の知識はある  完全に初心者だという方は少し難しい内容かもしれませんが、別に当てはまってなければ読んではダメというものでもないので、興味のある方は是非読み進めてみてください。</description>
    </item>
    
    <item>
      <title>自然言語処理における畳み込みニューラルネットワークを理解する</title>
      <link>http://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/</link>
      <pubDate>Fri, 11 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/</guid>
      <description>最近、畳み込みニューラルネットワークを使ったテキスト分類の実験をしていて、知見が溜まってきたのでそれについて何か記事を書こうと思っていた時に、こんな記事をみつけました。
http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp
畳み込みニューラルネットワークを自然言語処理に適用する話なのですが、この記事、個人的にわかりやすいなと思ったので、著者に許可をもらって日本語に翻訳しました。なお、この記事を読むにあたっては、ニューラルネットワークに関する基礎知識程度は必要かと思われます。
※日本語としてよりわかりやすく自然になるように、原文を直訳していない箇所もいくつかありますのでご了承ください。翻訳の致命的なミスなどありましたら、Twitterなどで指摘いただければすみやかに修正します。
以下訳文 畳み込みニューラルネットワーク (CNN) という言葉を聞いた時、普通はコンピュータービジョンのことを思い浮かべるでしょう。CNN は過去に画像分類の分野においてブレークスルーを引き起こし、今日では Facebook の写真の自動タギングから自動運転車に至るまで、ほとんどのコンピュータービジョンシステムの中核となっています。
そして近年、自然言語処理 (NLP) の領域の問題に対しても CNN が適用されはじめ、いくつか興味深い結果を得ています。この記事では CNN とはいったい何なのかということ、またどのようにして NLP の領域で使われるのか、ということを説明してみたいと思います。コンピュータービジョンでのユースケースを話した方が CNN においてはいくぶん直感的ですので、まずはそこから始めたいと思います。そしてゆっくりと NLP の話題へ移っていきましょう。
畳み込みとは？ 私は、畳み込みについては、行列に適用されるスライド窓関数 (sliding window function) として考えるとわかりやすいと思います。言葉でこう書くとちょっと難しいかもしれませんが、視覚的に表してみると非常にわかりやすいです。
 3x3 の畳み込みフィルタ。引用元: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution  左側にある行列は白黒の画像を表していると考えてください。行列の各要素はそれぞれ画像の 1 つのピクセルに対応しており、0 が黒、1 が白です (一般的には 0 から 255 の間の値を取るグレースケールの画像)。スライド窓はカーネル (kernel) やフィルタ (filter) または特徴検出器 (feature detector) などと呼ばれます。ここでは 3x3 のフィルタを使っており、そのフィルタの値と行列の値を要素毎に掛けあわせ、それらの値を合計します。この操作を、行列全体をカバーするようにフィルタをスライドさせながら各要素に対して行っていき、全体の畳み込みを取得します。
でも結局これで何が出来るの？と不思議に思いませんか。ここで直感的な例を挙げてみましょう。
各ピクセルとその周囲を平均して画像をぼかす:  
各ピクセルとその周囲の差分をとってエッジを検出する: (これを直感的に理解するためには、ピクセルの色が周囲の色と同じであるような色の変化がなめらかな部分で、どういうことが起こるのかを考えてみましょう。そういった部分にこのフィルタを適用すると、可算が相殺されて結果的には 0、つまり黒になります。逆に、明度が極端に違うエッジの部分であれば - これはたとえば白から黒へ移り変わっている部分 - においては、差分が大きくなり結果的には白くなります。)
 
GIMP のマニュアル には、ここで紹介した以外のサンプルがいくつか含まれています。畳み込みについてより詳しく理解したければ Chris Olah の記事 も読んでみることをオススメします。</description>
    </item>
    
  </channel>
</rss>